name: ESG Fund Data Scraper

on:
  schedule:
    # 매주 금요일 오전 9시 (KST) = 목요일 오후 12시 (UTC)
    - cron: '0 0 * * 5'
  workflow_dispatch:  # 수동 실행도 가능하도록 설정

jobs:
  scrape-and-update:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'
    
    - name: Cache pip packages
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements_esg.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        echo "Current directory contents:"
        ls -la
        echo "Installing numpy first to avoid compatibility issues..."
        pip install numpy==1.24.3
        echo "Installing playwright..."
        pip install playwright==1.40.0
        echo "Installing other requirements from requirements_esg.txt..."
        if [ -f requirements_esg.txt ]; then
          pip install -r requirements_esg.txt
        else
          echo "requirements_esg.txt not found, installing packages individually"
          pip install requests==2.31.0
          pip install pandas==2.0.3
          pip install gspread==5.10.0
          pip install google-auth==2.22.0
          pip install google-auth-oauthlib==1.0.0
          pip install google-auth-httplib2==0.1.0
          pip install openpyxl==3.1.2
        fi
        echo "Verifying installations..."
        pip show numpy
        pip show pandas
        pip show playwright
        echo "All installed packages:"
        pip list
    
    - name: Install Playwright browsers
      run: |
        echo "Installing Playwright browsers..."
        python -m playwright install chromium
        echo "Installing system dependencies..."
        # Ubuntu 버전에 맞는 의존성 설치
        sudo apt-get update
        sudo apt-get install -y \
          libnss3 \
          libnspr4 \
          libatk1.0-0 \
          libatk-bridge2.0-0 \
          libcups2 \
          libdrm2 \
          libdbus-1-3 \
          libatspi2.0-0 \
          libx11-6 \
          libxcomposite1 \
          libxdamage1 \
          libxext6 \
          libxfixes3 \
          libxrandr2 \
          libgbm1 \
          libxcb1 \
          libxkbcommon0 \
          libpango-1.0-0 \
          libcairo2 \
          libasound2t64 || true
    
    - name: Run ESG Fund Scraper
      env:
        GOOGLE_SERVICE: ${{ secrets.GOOGLE_SERVICE }}
        KRFUND_SPREADSHEET_ID: ${{ secrets.KRFUND_SPREADSHEET_ID }}
        TELCO_NEWS_TOKEN: ${{ secrets.TELCO_NEWS_TOKEN }}
        TELCO_NEWS_TESTER: ${{ secrets.TELCO_NEWS_TESTER }}
      run: |
        python esg_krfund_scraper.py
    
    - name: Commit and push backup data
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add data_backup/
        git diff --staged --quiet || git commit -m "Update ESG fund data backup - $(date +'%Y-%m-%d')"
        git push
      continue-on-error: true  # 백업 실패해도 워크플로우는 성공으로 처리
    
    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      with:
        name: esg-fund-data-${{ github.run_number }}
        path: data_backup/
        retention-days: 30  # 30일간 보관
    
    - name: Send notification on failure
      if: failure()
      env:
        TELCO_NEWS_TOKEN: ${{ secrets.TELCO_NEWS_TOKEN }}
        TELCO_NEWS_TESTER: ${{ secrets.TELCO_NEWS_TESTER }}
      run: |
        python -c "import requests, os; bot_token = os.environ.get('TELCO_NEWS_TOKEN'); chat_id = os.environ.get('TELCO_NEWS_TESTER'); requests.post(f'https://api.telegram.org/bot{bot_token}/sendMessage', data={'chat_id': chat_id, 'text': '❌ ESG Fund 데이터 수집 워크플로우 실패!'}) if bot_token and chat_id else None"
      continue-on-error: true
