name: MSIT Telco Service Monitoring

on:
  schedule:
    - cron: '40 23 1-31/2 * *'  # UTC 23:40 (KST 08:40) 홀수날마다 실행
  workflow_dispatch:
    inputs:
      days_range:
        description: '몇 일 전까지의 게시물을 확인할지 설정'
        required: false
        default: '4'
        type: string
      check_sheets:
        description: 'Google Sheets 업데이트 여부'
        required: false
        default: 'true'
        type: boolean
      spreadsheet_name:
        description: 'Google Sheets 스프레드시트 이름'
        required: false
        default: 'MSIT 통신 통계'
        type: string

permissions:
  contents: write

jobs:
  monitor:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        cache: 'pip'

    - name: Install Chrome
      run: |
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable
        google-chrome --version  # 디버깅용 버전 출력

    - name: Install Xvfb
      run: sudo apt-get install -y xvfb

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then
          pip install -r requirements.txt
        else
          pip install selenium beautifulsoup4 python-telegram-bot requests pandas gspread oauth2client lxml html5lib selenium-stealth webdriver-manager
        fi

    - name: Create directories
      run: |
        mkdir -p downloads
        mkdir -p screenshots
        mkdir -p html_content  # HTML 내용을 저장할 디렉토리 추가

    - name: Fix indentation issues
      run: |
        cat > fix_indentation.py << 'EOF'
#!/usr/bin/env python

def convert_tabs_to_spaces():
    with open('msit_monitor.py', 'r') as file:
        content = file.read()

    # Convert all tabs to 4 spaces
    fixed_content = content.replace('\t', '    ')

    with open('msit_monitor.py', 'w') as file:
        file.write(fixed_content)

    print("All tabs converted to spaces in the file")

if __name__ == "__main__":
    convert_tabs_to_spaces()
EOF
    
        python fix_indentation.py

    - name: Create document viewer script
      run: |
        cat > document_capture.py << 'EOF'
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import time
import sys
import os
import random
import json
import re
from bs4 import BeautifulSoup
import pandas as pd

def capture_document_content(url, file_id, order_id):
    options = Options()
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument("--headless")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option("useAutomationExtension", False)
    
    # 중요: 고유한 사용자 데이터 디렉터리 사용
    unique_dir = f'/tmp/chrome-user-data-{int(time.time())}-{random.randint(1000, 9999)}'
    options.add_argument(f'--user-data-dir={unique_dir}')
    print(f"Chrome 사용자 데이터 디렉토리: {unique_dir}")
    
    print("ChromeDriver 설치 시작...")
    driver_path = ChromeDriverManager().install()
    print(f"ChromeDriver 설치 완료: {driver_path}")
    
    driver = webdriver.Chrome(service=Service(driver_path), options=options)
    
    try:
        print(f"문서 뷰어 URL 접속: {url}")
        driver.get(url)
        print("문서 로딩 대기 중...")
        time.sleep(10)  # 문서 뷰어 로딩 대기
        
        driver.save_screenshot(f"html_content/document_{file_id}_{order_id}_screenshot.png")
        print(f"문서 뷰어 스크린샷 저장 완료")
        
        # HTML 내용 저장
        html_content = driver.page_source
        with open(f"html_content/document_{file_id}_{order_id}_content.html", "w", encoding="utf-8") as f:
            f.write(html_content)
        print(f"HTML 내용 저장 완료")
        
        # 시트 탭 확인
        sheet_tabs = driver.find_elements(By.CSS_SELECTOR, ".sheet-tab")
        sheet_names = []
        if sheet_tabs:
            print(f"{len(sheet_tabs)}개 시트 탭 발견")
            for idx, tab in enumerate(sheet_tabs):
                sheet_name = tab.get_attribute("textContent") or f"Sheet{idx+1}"
                sheet_names.append(sheet_name.strip())
                print(f"시트 {idx+1}: {sheet_name.strip()}")
                
                # 각 시트 탭 클릭하고 내용 저장
                try:
                    tab.click()
                    time.sleep(3)  # 탭 전환 대기
                    
                    driver.save_screenshot(f"html_content/document_{file_id}_{order_id}_sheet_{idx+1}.png")
                    
                    sheet_html = driver.page_source
                    with open(f"html_content/document_{file_id}_{order_id}_sheet_{idx+1}.html", "w", encoding="utf-8") as f:
                        f.write(sheet_html)
                    print(f"시트 {idx+1} HTML 내용 저장 완료")
                    
                    # JavaScript를 통해 테이블 내용 추출 시도
                    try:
                        tables_data = driver.execute_script('''
                            const tables = document.querySelectorAll('table');
                            let result = [];
                            
                            tables.forEach((table, tableIndex) => {
                                let tableData = {
                                    id: tableIndex,
                                    rows: [],
                                    dimensions: { rows: 0, cols: 0 }
                                };
                                
                                const rows = table.querySelectorAll('tr');
                                tableData.dimensions.rows = rows.length;
                                
                                rows.forEach((row, rowIndex) => {
                                    const cells = row.querySelectorAll('td, th');
                                    if (rowIndex === 0) {
                                        tableData.dimensions.cols = cells.length;
                                    }
                                    
                                    let rowData = [];
                                    cells.forEach(cell => {
                                        rowData.push(cell.textContent.trim());
                                    });
                                    
                                    tableData.rows.push(rowData);
                                });
                                
                                result.push(tableData);
                            });
                            
                            return result;
                        ''')
                        
                        if tables_data:
                            with open(f"html_content/document_{file_id}_{order_id}_sheet_{idx+1}_tables.json", "w", encoding="utf-8") as f:
                                json.dump(tables_data, f, ensure_ascii=False, indent=2)
                            print(f"시트 {idx+1}에서 {len(tables_data)}개 테이블 데이터 추출 완료")
                        else:
                            print(f"시트 {idx+1}에서 테이블 데이터 추출 실패")
                    except Exception as js_error:
                        print(f"테이블 데이터 추출 JavaScript 오류: {str(js_error)}")
                        
                except Exception as click_error:
                    print(f"시트 탭 클릭 오류: {str(click_error)}")
                    
            # 시트 정보 저장
            with open(f"html_content/document_{file_id}_{order_id}_sheets.json", "w", encoding="utf-8") as f:
                json.dump(sheet_names, f, ensure_ascii=False, indent=2)
        else:
            print("시트 탭을 찾을 수 없음")
            
        # iframe 내용 확인
        iframes = driver.find_elements(By.TAG_NAME, "iframe")
        if iframes:
            print(f"{len(iframes)}개 iframe 발견")
            for idx, iframe in enumerate(iframes):
                try:
                    driver.switch_to.frame(iframe)
                    iframe_html = driver.page_source
                    with open(f"html_content/document_{file_id}_{order_id}_iframe_{idx+1}.html", "w", encoding="utf-8") as f:
                        f.write(iframe_html)
                    print(f"iframe {idx+1} 내용 저장 완료")
                    driver.switch_to.default_content()  # 기본 컨텍스트로 복귀
                except Exception as iframe_error:
                    print(f"iframe {idx+1} 처리 오류: {str(iframe_error)}")
                    driver.switch_to.default_content()
            
        return True
            
    except Exception as e:
        print(f'문서 내용 캡처 오류: {str(e)}')
        return False
    finally:
        print("WebDriver 종료")
        driver.quit()

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("사용법: python document_capture.py <문서뷰어URL> <파일ID> <순서ID>")
        sys.exit(1)
        
    url = sys.argv[1]
    file_id = sys.argv[2] if len(sys.argv) > 2 else "unknown"
    order_id = sys.argv[3] if len(sys.argv) > 3 else "1"
    
    success = capture_document_content(url, file_id, order_id)
    if success:
        print("문서 내용 캡처 성공")
    else:
        print("문서 내용 캡처 실패")
        sys.exit(1)
EOF

    - name: Create HTML processor script
      run: |
        cat > process_html.py << 'EOF'
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import json
import glob
import pandas as pd
from bs4 import BeautifulSoup
import sys

def extract_tables_from_html(html_file):
    """HTML 파일에서 테이블 추출"""
    print(f"HTML 파일 분석 중: {html_file}")
    
    try:
        with open(html_file, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        soup = BeautifulSoup(html_content, 'html.parser')
        tables = soup.find_all('table')
        
        if not tables:
            print(f"테이블을 찾을 수 없음: {html_file}")
            return None
        
        print(f"{len(tables)}개 테이블 발견")
        extracted_tables = []
        
        for i, table in enumerate(tables):
            # 테이블 행 추출
            rows = []
            for tr in table.find_all('tr'):
                row = []
                for cell in tr.find_all(['td', 'th']):
                    cell_text = cell.get_text(strip=True)
                    row.append(cell_text)
                if row:  # 빈 행 건너뛰기
                    rows.append(row)
            
            if not rows:
                print(f"테이블 {i+1}에 행이 없음")
                continue
            
            # 데이터프레임 변환 시도
            try:
                # 열 수가 동일한지 확인
                col_counts = [len(row) for row in rows]
                if len(set(col_counts)) > 1:
                    # 열 수가 다른 경우, 최대 열 수로 맞춤
                    max_cols = max(col_counts)
                    rows = [row + [''] * (max_cols - len(row)) for row in rows]
                    print(f"테이블 {i+1}: 열 수 불일치 보정 ({col_counts} -> {max_cols})")
                
                # 첫 번째 행을 헤더로 간주
                headers = rows[0]
                data = rows[1:]
                
                # 모든 값이 빈 열 확인
                is_empty_col = [all(row[j] == '' for row in data) for j in range(len(headers))]
                
                # 빈 열이 있는 경우 필터링
                if any(is_empty_col):
                    filtered_headers = [h for j, h in enumerate(headers) if not is_empty_col[j]]
                    filtered_data = [[row[j] for j, is_empty in enumerate(is_empty_col) if not is_empty] for row in data]
                    
                    headers = filtered_headers
                    data = filtered_data
                    print(f"테이블 {i+1}: 빈 열 {is_empty_col.count(True)}개 제거")
                
                # 데이터프레임 생성
                df = pd.DataFrame(data, columns=headers)
                
                # CSV 파일로 저장
                basename = os.path.basename(html_file)
                table_name = f"{basename.split('.')[0]}_table_{i+1}"
                csv_path = f"html_content/{table_name}.csv"
                df.to_csv(csv_path, index=False, encoding='utf-8-sig')
                
                print(f"테이블 {i+1} CSV 저장 완료: {csv_path}")
                extracted_tables.append({
                    'table_index': i,
                    'csv_path': csv_path,
                    'rows': len(df),
                    'columns': len(df.columns)
                })
            
            except Exception as df_error:
                print(f"테이블 {i+1} 데이터프레임 변환 오류: {str(df_error)}")
        
        return extracted_tables
    
    except Exception as e:
        print(f"HTML 파일 처리 오류: {str(e)}")
        return None

def process_html_files():
    """HTML 디렉토리의 모든 HTML 파일 처리"""
    html_dir = "html_content"
    if not os.path.exists(html_dir):
        print(f"HTML 디렉토리가 없음: {html_dir}")
        return False
    
    # HTML 파일 목록 가져오기 (iframe, sheet 파일 우선)
    html_files = []
    html_files.extend(glob.glob(f"{html_dir}/*_sheet_*.html"))
    html_files.extend(glob.glob(f"{html_dir}/*_iframe_*.html"))
    html_files.extend(glob.glob(f"{html_dir}/*_content.html"))
    
    if not html_files:
        print("처리할 HTML 파일이 없음")
        return False
    
    print(f"{len(html_files)}개 HTML 파일 처리 중...")
    
    all_tables = []
    for html_file in html_files:
        tables = extract_tables_from_html(html_file)
        if tables:
            all_tables.extend(tables)
    
    if all_tables:
        print(f"총 {len(all_tables)}개 테이블 추출 완료")
        
        # 추출 결과 JSON 저장
        with open(f"{html_dir}/extracted_tables_info.json", "w", encoding="utf-8") as f:
            json.dump(all_tables, f, ensure_ascii=False, indent=2)
        
        # Excel 파일 생성
        try:
            excel_path = f"{html_dir}/all_extracted_tables.xlsx"
            with pd.ExcelWriter(excel_path) as writer:
                for table in all_tables:
                    try:
                        csv_path = table['csv_path']
                        df = pd.read_csv(csv_path, encoding='utf-8-sig')
                        
                        # 시트 이름 생성 (31자 제한)
                        sheet_name = os.path.basename(csv_path).split('.')[0]
                        if len(sheet_name) > 31:
                            sheet_name = sheet_name[:28] + "..."
                        
                        df.to_excel(writer, sheet_name=sheet_name, index=False)
                        print(f"테이블 추가 완료: {sheet_name}")
                        
                    except Exception as e:
                        print(f"테이블 Excel 추가 오류: {str(e)}")
            
            print(f"모든 테이블이 Excel 파일로 결합됨: {excel_path}")
            return True
        
        except Exception as excel_error:
            print(f"Excel 파일 생성 오류: {str(excel_error)}")
    
    return len(all_tables) > 0

if __name__ == "__main__":
    success = process_html_files()
    if not success:
        print("HTML 파일 처리 실패")
        sys.exit(1)
EOF

    - name: Run monitoring script
      env:
        TELCO_NEWS_TOKEN: ${{ secrets.TELCO_NEWS_TOKEN }}
        TELCO_NEWS_TESTER: ${{ secrets.TELCO_NEWS_TESTER }}
        MSIT_GSPREAD_ref: ${{ secrets.MSIT_GSPREAD_ref }}
        MSIT_SPREADSHEET_ID: ${{ secrets.MSIT_SPREADSHEET_ID }}
        DAYS_RANGE: ${{ github.event.inputs.days_range || '4' }}
        CHECK_SHEETS: ${{ github.event.inputs.check_sheets || 'true' }}
        SPREADSHEET_NAME: ${{ github.event.inputs.spreadsheet_name || 'MSIT 통신 통계' }}
        PYTHONIOENCODING: utf-8
      run: |
        export DISPLAY=:99
        Xvfb :99 -screen 0 1920x1080x24 > /dev/null 2>&1 &
        sleep 2
        
        # 모니터링 스크립트 실행
        echo "모니터링 스크립트 실행..."
        python msit_monitor.py || { echo "모니터링 스크립트 실행 실패"; exit 1; }

    - name: Capture document content (if document view URL exists)
      if: always()
      run: |
        export DISPLAY=:99
        
        # 로그 파일에서 바로보기 URL 추출
        DOCUMENT_VIEW_URL=$(grep -o "바로보기 URL: [^ ]*" *.log 2>/dev/null | head -1 | sed 's/바로보기 URL: //')
        
        if [ -n "$DOCUMENT_VIEW_URL" ]; then
          echo "바로보기 URL 발견: $DOCUMENT_VIEW_URL"
          
          # URL에서 파일 ID와 순서 ID 추출
          FILE_ID=$(echo $DOCUMENT_VIEW_URL | grep -o "atchFileNo=[0-9]*" | sed 's/atchFileNo=//')
          ORDER_ID=$(echo $DOCUMENT_VIEW_URL | grep -o "fileOrdr=[0-9]*" | sed 's/fileOrdr=//')
          
          echo "문서 캡처 스크립트 실행..."
          python document_capture.py "$DOCUMENT_VIEW_URL" "$FILE_ID" "$ORDER_ID"
          
          # 캡처된 HTML 파일 확인
          echo "캡처된 HTML 파일 목록:"
          ls -la html_content/
        else
          echo "바로보기 URL을 찾을 수 없음"
        fi

    - name: Process captured HTML files
      if: always()
      run: |
        # HTML 파일이 존재하는지 확인
        if [ -d "html_content" ] && [ "$(ls -A html_content/*.html 2>/dev/null)" ]; then
          echo "HTML 파일 처리 중..."
          python process_html.py
        else
          echo "HTML 파일이 없음"
        fi

    - name: Archive artifacts
      if: always()
      run: |
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        mkdir -p artifacts
        cp -r *.log *.png *.html downloads/* screenshots/* html_content/* *.bak artifacts/ 2>/dev/null || true
        echo "실행 정보:" > artifacts/run_info.txt
        echo "실행 ID: ${{ github.run_id }}" >> artifacts/run_info.txt
        echo "실행 번호: ${{ github.run_number }}" >> artifacts/run_info.txt
        echo "타임스탬프: ${TIMESTAMP}" >> artifacts/run_info.txt
        echo "실행자: ${{ github.actor }}" >> artifacts/run_info.txt
        echo "워크플로우: ${{ github.workflow }}" >> artifacts/run_info.txt
        tar -czf monitoring-artifacts-${TIMESTAMP}.tar.gz artifacts/
        echo "ARTIFACT_PATH=monitoring-artifacts-${TIMESTAMP}.tar.gz" >> $GITHUB_ENV
        echo "ARTIFACT_NAME=monitoring-artifacts-${TIMESTAMP}" >> $GITHUB_ENV
        echo "TIMESTAMP=${TIMESTAMP}" >> $GITHUB_ENV

    - name: Create Release
      if: always()
      uses: softprops/action-gh-release@v1
      with:
        tag_name: monitoring-${{ env.TIMESTAMP }}
        name: 모니터링 실행 ${{ github.run_number }} (${{ env.TIMESTAMP }})
        files: ${{ env.ARTIFACT_PATH }}
        body: |
          ## MSIT 통신 통계 모니터링 실행 결과
          - **실행 ID**: ${{ github.run_id }}
          - **실행 번호**: ${{ github.run_number }}
          - **실행 시간**: ${{ env.TIMESTAMP }}
          - **실행 유형**: ${{ github.event_name }}
          이 릴리스에는 모니터링 스크립트 실행 결과와 관련 로그 및 스크린샷이 포함되어 있습니다.
        token: ${{ secrets.GITHUB_TOKEN }}
