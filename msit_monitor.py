
import os
import re    
import json
import time
import logging
import asyncio
from datetime import datetime, timedelta
from pathlib import Path
import pandas as pd
import requests
from urllib.parse import urlparse, parse_qs

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException

from bs4 import BeautifulSoup
import telegram
import gspread
from oauth2client.service_account import ServiceAccountCredentials

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('msit_monitor')

class MSITMonitor:
    def __init__(self):
        # Telegram configuration
        self.telegram_token = os.environ.get('TELCO_NEWS_TOKEN')
        self.chat_id = os.environ.get('TELCO_NEWS_TESTER')
        if not self.telegram_token or not self.chat_id:
            raise ValueError("í™˜ê²½ ë³€ìˆ˜ TELCO_NEWS_TOKENê³¼ TELCO_NEWS_TESTERê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # Google Sheets configuration
        self.gspread_creds = os.environ.get('MSIT_GSPREAD_ref')
        self.spreadsheet_id = os.environ.get('MSIT_SPREADSHEET_ID')
        self.spreadsheet_name = os.environ.get('SPREADSHEET_NAME', 'MSIT í†µì‹  í†µê³„')
    
        if not self.gspread_creds:
            logger.warning("í™˜ê²½ ë³€ìˆ˜ MSIT_GSPREAD_refê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Google Sheets ì—…ë°ì´íŠ¸ëŠ” ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
        
        # MSIT URL
        self.url = "https://www.msit.go.kr/bbs/list.do?sCode=user&mPid=74&mId=99"
        
        # Initialize Telegram bot
        self.bot = telegram.Bot(token=self.telegram_token)
        
        # Report types for tracking
        self.report_types = [
            "ì´ë™ì „í™” ë° íŠ¸ë˜í”½ í†µê³„",
            "ì´ë™ì „í™” ë° ì‹œë‚´ì „í™” ë²ˆí˜¸ì´ë™ í˜„í™©",
            "ìœ ì„ í†µì‹ ì„œë¹„ìŠ¤ ê°€ì… í˜„í™©",
            "ë¬´ì„ í†µì‹ ì„œë¹„ìŠ¤ ê°€ì… í˜„í™©", 
            "íŠ¹ìˆ˜ë¶€ê°€í†µì‹ ì‚¬ì—…ìí˜„í™©",
            "ë¬´ì„ ë°ì´í„° íŠ¸ë˜í”½ í†µê³„",
            "ìœ Â·ë¬´ì„ í†µì‹ ì„œë¹„ìŠ¤ ê°€ì… í˜„í™© ë° ë¬´ì„ ë°ì´í„° íŠ¸ë˜í”½ í†µê³„"
        ]
        
        # Temporary directory for downloads
        self.temp_dir = Path("./downloads")
        self.temp_dir.mkdir(exist_ok=True)


        # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ íŒŒì¼ ì •ë³´ë¥¼ ì €ì¥í•  ë³€ìˆ˜
        self.current_file_info = None
        
        # ì§ì ‘ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œë„ íšŸìˆ˜
        self.direct_download_attempt = 0
        
        # ì„¸ì…˜ ìœ ì§€
        self.session = requests.Session()

    
    def setup_driver(self):
        """Initialize Selenium WebDriver"""
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')  # í° ì°½ í¬ê¸° ì„¤ì •

        # ë¸Œë¼ìš°ì € í˜ì´ì§€ ë¡œë“œ ì „ëµ ì„¤ì •
        chrome_options.page_load_strategy = 'eager'  # DOMì´ ì¤€ë¹„ë˜ë©´ ë¡œë“œ ì™„ë£Œë¡œ ê°„ì£¼
        
        
        # Set download preferences
        prefs = {
            "download.default_directory": str(self.temp_dir.absolute()),
            "download.prompt_for_download": False,
            "download.directory_upgrade": True,
            "safebrowsing.enabled": False,
            "profile.default_content_setting_values.images": 2,  # ì´ë¯¸ì§€ ë¡œë“œ ë¹„í™œì„±í™”
            "profile.default_content_settings.popups": 0,
            "profile.default_content_setting_values.notifications": 2
        }
        chrome_options.add_experimental_option("prefs", prefs)

        # ë¶ˆí•„ìš”í•œ ë¡œê·¸ ë¹„í™œì„±í™”
        chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])

        # Use either environment-specific or standard Chrome path
        if os.path.exists('/usr/bin/chromium-browser'):
            chrome_options.binary_location = '/usr/bin/chromium-browser'
            service = Service('/usr/bin/chromedriver')
        else:
            # Use webdriver-manager for local development
            try:
                from webdriver_manager.chrome import ChromeDriverManager
                service = Service(ChromeDriverManager().install())
            except ImportError:
                # Fallback for environments without webdriver-manager
                service = Service('/usr/bin/chromedriver')
        
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
            # í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì•„ì›ƒ ì„¤ì • (ì´ˆ ë‹¨ìœ„)
        driver.set_page_load_timeout(60)  # 60ì´ˆë¡œ ì„¤ì •

        # ì•”ì‹œì  ëŒ€ê¸° ì„¤ì • - ìš”ì†Œë¥¼ ì°¾ì„ ë•Œ ìµœëŒ€ 10ì´ˆê¹Œì§€ ëŒ€ê¸°
        driver.implicitly_wait(10)
        
        return driver

    def setup_gspread_client(self):
        """Initialize Google Sheets client"""
        if not self.gspread_creds:
            return None
        
        try:
            # Parse credentials from environment variable
            creds_dict = json.loads(self.gspread_creds)
            
            # Write credentials to a temporary file
            temp_creds_path = self.temp_dir / "temp_creds.json"
            with open(temp_creds_path, 'w') as f:
                json.dump(creds_dict, f)
            
            # Set up the gspread client
            scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
            credentials = ServiceAccountCredentials.from_json_keyfile_name(str(temp_creds_path), scope)
            client = gspread.authorize(credentials)
            
            # Clean up temporary file
            os.unlink(temp_creds_path)
            
            return client
        except Exception as e:
            logger.error(f"Google Sheets í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return None

    def is_telecom_stats_post(self, title):
        """Check if the post is a telecommunication statistics report"""
        # Check if title contains a date pattern like "(YYYYë…„ MMì›”ë§ ê¸°ì¤€)"
        date_pattern = r'\((\d{4})ë…„\s+(\d{1,2})ì›”ë§\s+ê¸°ì¤€\)'
        has_date_pattern = re.search(date_pattern, title) is not None
    
        # Check if title contains any of the report types
        contains_report_type = any(report_type in title for report_type in self.report_types)
    
        return has_date_pattern and contains_report_type

    def extract_post_id(self, item):
        """Extract the post ID from a BeautifulSoup item"""
        try:
            link_elem = item.find('a')
            if not link_elem:
                return None
                
            onclick_attr = link_elem.get('onclick', '')
            match = re.search(r"fn_detail\((\d+)\)", onclick_attr)
            if match:
                return match.group(1)
            return None
        except Exception as e:
            logger.error(f"ê²Œì‹œë¬¼ ID ì¶”ì¶œ ì¤‘ ì—ëŸ¬: {str(e)}")
            return None

    def get_post_url(self, post_id):
        """Generate the post URL from the post ID"""
        if not post_id:
            return None
        return f"https://www.msit.go.kr/bbs/view.do?sCode=user&mId=99&mPid=74&nttSeqNo={post_id}"

    def is_in_date_range(self, date_str, days=4):
        """Check if the post date is within the specified range"""
        try:
            # Normalize date string
            date_str = date_str.replace(',', ' ').strip()
        
            # Try various date formats
            try:
                # "YYYY. MM. DD" format
                post_date = datetime.strptime(date_str, '%Y. %m. %d').date()
            except ValueError:
                try:
                    # "MMM DD YYYY" format
                    post_date = datetime.strptime(date_str, '%b %d %Y').date()
                except ValueError:
                    try:
                        # "YYYY-MM-DD" format
                        post_date = datetime.strptime(date_str, '%Y-%m-%d').date()
                    except ValueError:
                        # ë‹¤ë¥¸ í˜•ì‹ ì‹œë„
                        logger.warning(f"Unknown date format: {date_str}, trying regex pattern")
                        date_match = re.search(r'(\d{4})[.\-\s]+(\d{1,2})[.\-\s]+(\d{1,2})', date_str)
                        if date_match:
                            post_date = datetime(int(date_match.group(1)), int(date_match.group(2)), int(date_match.group(3))).date()
                        else:
                            logger.error(f"Could not parse date: {date_str}")
                            return True  # ë‚ ì§œë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ëŠ” ê²½ìš° í¬í•¨ì‹œì¼œ ê²€ì‚¬
        
            # Calculate date range (Korean timezone)
            korea_tz = datetime.now() + timedelta(hours=9)  # UTC to KST
            days_ago = (korea_tz - timedelta(days=days)).date()
        
            logger.info(f"ê²Œì‹œë¬¼ ë‚ ì§œ í™•ì¸: {post_date} vs {days_ago} ({days}ì¼ ì „, í•œêµ­ ì‹œê°„ ê¸°ì¤€)")
            return post_date >= days_ago
        
        except Exception as e:
            logger.error(f"ë‚ ì§œ íŒŒì‹± ì—ëŸ¬: {str(e)}")
            return True  # ì—ëŸ¬ ë°œìƒ ì‹œ ê¸°ë³¸ì ìœ¼ë¡œ í¬í•¨ì‹œì¼œ ê²€ì‚¬

    def has_next_page(self, driver):
        """Check if there's a next page to parse"""
        try:
            current_page = int(driver.find_element(By.CSS_SELECTOR, "a.page-link[aria-current='page']").text)
            next_page_link = driver.find_elements(By.CSS_SELECTOR, f"a.page-link[href*='pageIndex={current_page + 1}']")
            return len(next_page_link) > 0
        except Exception as e:
            logger.error(f"ë‹¤ìŒ í˜ì´ì§€ í™•ì¸ ì¤‘ ì—ëŸ¬: {str(e)}")
            return False

    def go_to_next_page(self, driver):
        """Navigate to the next page"""
        try:
            current_page = int(driver.find_element(By.CSS_SELECTOR, "a.page-link[aria-current='page']").text)
            next_page = driver.find_element(By.CSS_SELECTOR, f"a.page-link[href*='pageIndex={current_page + 1}']")
            next_page.click()
            WebDriverWait(driver, 10).until(
                EC.staleness_of(driver.find_element(By.CSS_SELECTOR, "div.board_list"))
            )
            return True
        except Exception as e:
            logger.error(f"ë‹¤ìŒ í˜ì´ì§€ ì´ë™ ì¤‘ ì—ëŸ¬: {str(e)}")
            return False

    def parse_page(self, driver, days_range=4):
        """Parse the current page for relevant posts"""
        all_posts = []
        telecom_stats_posts = []
        continue_search = True
        
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CLASS_NAME, "board_list"))
            )
            
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            posts = soup.find_all('div', {'class': 'toggle'})
            
            for item in posts:
                # Skip header row
                if 'thead' in item.get('class', []):
                    continue

                try:
                    # Extract date information
                    date_elem = item.find('div', {'class': 'date', 'aria-label': 'ë“±ë¡ì¼'})
                    if not date_elem:
                        date_elem = item.find('div', {'class': 'date'})
                    if not date_elem:
                        continue
                        
                    date_str = date_elem.text.strip()
                    if not date_str or date_str == 'ë“±ë¡ì¼':
                        continue
                    
                    logger.info(f"Found date string: {date_str}")
                    
                    # Check if post is within date range
                    if not self.is_in_date_range(date_str, days=days_range):
                        continue_search = False
                        break
                    
                    # Extract title and post ID
                    title_elem = item.find('p', {'class': 'title'})
                    if not title_elem:
                        continue
                        
                    title = title_elem.text.strip()
                    post_id = self.extract_post_id(item)
                    post_url = self.get_post_url(post_id)
                    
                    # Extract department information
                    dept_elem = item.find('dd', {'id': lambda x: x and 'td_CHRG_DEPT_NM' in x})
                    dept_text = dept_elem.text.strip() if dept_elem else "ë¶€ì„œ ì •ë³´ ì—†ìŒ"
                    
                    # Create post info dictionary
                    post_info = {
                        'title': title,
                        'date': date_str,
                        'department': dept_text,
                        'url': post_url,
                        'post_id': post_id
                    }
                    
                    # Add to all posts list
                    all_posts.append(post_info)
                    
                    # Check if it's a telecom stats post
                    if self.is_telecom_stats_post(title):
                        logger.info(f"Found telecom stats post: {title}")
                        telecom_stats_posts.append(post_info)
                        
                except Exception as e:
                    logger.error(f"ê²Œì‹œë¬¼ íŒŒì‹± ì¤‘ ì—ëŸ¬: {str(e)}")
                    continue
            
            return all_posts, telecom_stats_posts, continue_search
            
        except Exception as e:
            logger.error(f"í˜ì´ì§€ íŒŒì‹± ì¤‘ ì—ëŸ¬: {str(e)}")
            return [], [], False

    def extract_file_info(self, driver, post):
        """Extract file information from a post with improved handling for SynapDocViewServer"""
        if not post.get('post_id'):
            logger.error(f"Cannot access post {post['title']} - missing post ID")
            return None
        
        logger.info(f"Opening post: {post['title']}")
        
        # ìµœëŒ€ 3ë²ˆê¹Œì§€ ì¬ì‹œë„
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # ê²Œì‹œë¬¼ ìƒì„¸ í˜ì´ì§€ URL
                detail_url = f"https://www.msit.go.kr/bbs/view.do?sCode=user&mId=99&mPid=74&nttSeqNo={post['post_id']}"
                logger.info(f"Navigating to post detail URL: {detail_url}")
                
                # í˜ì´ì§€ ë¡œë“œ ì „ ì¿ í‚¤ì™€ ìºì‹œ ì´ˆê¸°í™”
                driver.delete_all_cookies()
                driver.execute_script("window.localStorage.clear();")
                
                # URL ì§ì ‘ ì´ë™
                driver.get(detail_url)
                
                # ëª…ì‹œì ì¸ ëŒ€ê¸° ì‹œê°„ ì¶”ê°€
                time.sleep(5)
                
                # í˜ì´ì§€ ë¡œë“œ í™•ì¸ - ë‹¤ì–‘í•œ ìš”ì†Œ ì¤‘ í•˜ë‚˜ë¼ë„ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                try:
                    WebDriverWait(driver, 20).until(
                        lambda x: (
                            len(x.find_elements(By.CLASS_NAME, "view_head")) > 0 or 
                            len(x.find_elements(By.CLASS_NAME, "view_file")) > 0 or
                            len(x.find_elements(By.ID, "cont-wrap")) > 0
                        )
                    )
                    logger.info("Post detail page loaded successfully")
                    break  # ì„±ê³µí•˜ë©´ ë£¨í”„ ì¢…ë£Œ
                except TimeoutException:
                    logger.warning(f"Timeout waiting for page elements, retrying... ({attempt+1}/{max_retries})")
                    if attempt < max_retries - 1:
                        continue
                    else:
                        # ë§ˆì§€ë§‰ ì‹œë„ì—ì„œëŠ” í˜ì´ì§€ ì†ŒìŠ¤ í™•ì¸
                        logger.info(f"Page source preview: {driver.page_source[:500]}")
                
            except Exception as e:
                logger.error(f"Error accessing post detail: {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(3)
                else:
                    return None
        
        # íŒŒì¼ ì •ë³´ ì¶”ì¶œ
        try:
            # ë°©ë²• 1: ë°”ë¡œë³´ê¸° ë§í¬ ì°¾ê¸° - getExtension_path í•¨ìˆ˜ í˜¸ì¶œ ë§í¬
            view_links = driver.find_elements(By.CSS_SELECTOR, "a.view[title='ìƒˆì°½ ì—´ë¦¼']")
            
            if not view_links:
                # ë‹¤ë¥¸ ì„ íƒìë¡œ ì‹œë„
                view_links = driver.find_elements(By.CSS_SELECTOR, "a[onclick*='getExtension_path']")
            
            if not view_links:
                # í…ìŠ¤íŠ¸ë¡œ ì°¾ê¸°
                all_links = driver.find_elements(By.TAG_NAME, "a")
                view_links = [link for link in all_links if 'ë°”ë¡œë³´ê¸°' in (link.text or '')]
            
            if view_links:
                view_link = view_links[0]
                onclick_attr = view_link.get_attribute('onclick')
                logger.info(f"Found view link with onclick: {onclick_attr}")
                
                # getExtension_path('49234', '1')ì—ì„œ ë§¤ê°œë³€ìˆ˜ ì¶”ì¶œ
                match = re.search(r"getExtension_path\s*\(\s*['\"]([\d]+)['\"]?\s*,\s*['\"]([\d]+)['\"]", onclick_attr)
                if match:
                    atch_file_no = match.group(1)
                    file_ord = match.group(2)
                    
                    # íŒŒì¼ ì´ë¦„ ì¶”ì¶œ ì‹œë„
                    try:
                        parent_li = view_link.find_element(By.XPATH, "./ancestor::li")
                        file_name_element = parent_li.find_element(By.TAG_NAME, "a")
                        file_name = file_name_element.text.strip()
                    except:
                        # íŒŒì¼ ì´ë¦„ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°, ê²Œì‹œë¬¼ ì œëª©ì—ì„œ ìœ ì¶”
                        date_match = re.search(r'\((\d{4})ë…„\s+(\d{1,2})ì›”ë§\s+ê¸°ì¤€\)', post['title'])
                        if date_match:
                            year = date_match.group(1)
                            month = date_match.group(2).zfill(2)
                            file_name = f"{year}ë…„ {month}ì›”ë§ ê¸°ì¤€ í†µê³„.xlsx"
                        else:
                            file_name = f"í†µê³„ìë£Œ_{atch_file_no}.xlsx"
                    
                    # íŒŒì¼ ì •ë³´ ë°˜í™˜ 
                    file_info = {
                        'file_name': file_name,
                        'atch_file_no': atch_file_no,
                        'file_ord': file_ord,
                        'use_view': True,
                        'post_info': post
                    }
                    
                    logger.info(f"Successfully extracted file info: {file_info}")
                    return file_info
            
            # ë°©ë²• 2: ë‹¤ìš´ë¡œë“œ ë§í¬ ì§ì ‘ ì°¾ê¸°
            download_links = driver.find_elements(By.CSS_SELECTOR, "a[onclick*='fn_download']")
            
            if download_links:
                for link in download_links:
                    onclick_attr = link.get_attribute('onclick')
                    match = re.search(r"fn_download\s*\(\s*['\"]([\d]+)['\"]?\s*,\s*['\"]([\d]+)['\"]", onclick_attr)
                    if match:
                        atch_file_no = match.group(1)
                        file_ord = match.group(2)
                        file_name = link.text.strip()
                        
                        file_info = {
                            'file_name': file_name,
                            'atch_file_no': atch_file_no,
                            'file_ord': file_ord,
                            'use_download': True,  # ë‹¤ìš´ë¡œë“œ ì‚¬ìš© í”Œë˜ê·¸
                            'post_info': post
                        }
                        
                        logger.info(f"Found direct download link: {file_info}")
                        return file_info
            
            # ë°©ë²• 3: ê²Œì‹œë¬¼ ë‚´ìš©ì—ì„œ ë°ì´í„° ì¶”ì¶œ ì‹œë„
            logger.info("No file links found, attempting to extract from content")
            
            # ê²Œì‹œë¬¼ ë‚´ìš© ì˜ì—­ ì°¾ê¸°
            content_div = driver.find_element(By.CLASS_NAME, "view_cont")
            if content_div:
                content_text = content_div.text
                
                # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ ì‹œë„
                date_match = re.search(r'\((\d{4})ë…„\s+(\d{1,2})ì›”ë§\s+ê¸°ì¤€\)', post['title'])
                if date_match:
                    year = int(date_match.group(1))
                    month = int(date_match.group(2))
                    
                    # ì½˜í…ì¸  ê¸°ë°˜ íŒŒì¼ ì •ë³´ ìƒì„±
                    return {
                        'extract_from_content': True,
                        'content_data': content_text,
                        'date': {'year': year, 'month': month},
                        'post_info': post,
                        'file_name': f"{year}ë…„ {month}ì›”ë§ ê¸°ì¤€ í†µê³„.xlsx"
                    }
            
            logger.warning(f"No file information could be extracted from post: {post['title']}")
            return None
            
        except Exception as e:
            logger.error(f"Error extracting file info: {str(e)}")
            return None
    
 
    def direct_download_file(self, file_info):
        """Directly download file using requests"""
        if not file_info:
            return None
            
        if not (file_info.get('atch_file_no') and file_info.get('file_ord')):
            return None
            
        try:
            atch_file_no = file_info['atch_file_no']
            file_ord = file_info['file_ord']
            
            # ì§ì ‘ ë‹¤ìš´ë¡œë“œ URL êµ¬ì„±
            download_url = f"https://www.msit.go.kr/ssm/file/fileDown.do?atchFileNo={atch_file_no}&fileOrd={file_ord}&fileBtn=A"
            
            logger.info(f"Attempting direct download from: {download_url}")
            
            # ìš”ì²­ í—¤ë” ì„¤ì •
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
            }
            
            # ì„¸ì…˜ì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            response = self.session.get(download_url, headers=headers, stream=True)
            
            if response.status_code == 200:
                # íŒŒì¼ëª… ì¶”ì¶œ ì‹œë„
                content_disposition = response.headers.get('Content-Disposition')
                if content_disposition:
                    filename_match = re.search(r'filename=(?:\"?)([^\";\n]+)', content_disposition)
                    if filename_match:
                        filename = filename_match.group(1)
                    else:
                        filename = f"download_{atch_file_no}_{file_ord}.xlsx"
                else:
                    # íŒŒì¼ëª…ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì´ë¦„ ì‚¬ìš©
                    filename = file_info.get('file_name', f"download_{atch_file_no}_{file_ord}.xlsx")
                
                # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
                safe_filename = "".join(c for c in filename if c.isalnum() or c in "._- ").strip()
                file_path = self.temp_dir / safe_filename
                
                # íŒŒì¼ ì €ì¥
                with open(file_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                
                logger.info(f"Successfully downloaded file to: {file_path}")
                return file_path
            else:
                logger.error(f"Failed to download file: HTTP {response.status_code}")
                return None
                
        except Exception as e:
            logger.error(f"Error during direct download: {str(e)}")
            return None

    
    def process_view_data(self, driver, file_info):
        """Process view data without relying on iframe access"""
        if not file_info:
            return None
            
        # ì´ë¯¸ ì½˜í…ì¸ ì—ì„œ ì¶”ì¶œí•œ ê²½ìš°
        if file_info.get('extract_from_content'):
            return self.process_content_data(file_info)
            
        # ë°”ë¡œë³´ê¸° URL êµ¬ì„±
        if file_info.get('atch_file_no') and file_info.get('file_ord'):
            atch_file_no = file_info['atch_file_no']
            file_ord = file_info['file_ord']
            
            # 1. ì§ì ‘ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œë„
            if self.direct_download_attempt < 2:  # ìµœëŒ€ 2ë²ˆê¹Œì§€ë§Œ ì‹œë„
                self.direct_download_attempt += 1
                file_path = self.direct_download_file(file_info)
                
                if file_path and file_path.exists():
                    # ë‹¤ìš´ë¡œë“œ ì„±ê³µ ì‹œ ì—‘ì…€ íŒŒì¼ ì²˜ë¦¬
                    logger.info(f"Processing downloaded file: {file_path}")
                    return self.process_excel_file(file_path, file_info['post_info'])
            
            # 2. ë°”ë¡œë³´ê¸° í˜ì´ì§€ì—ì„œ ë°ì´í„° ì¶”ì¶œ ì‹œë„
            view_url = f"https://www.msit.go.kr/bbs/documentView.do?atchFileNo={atch_file_no}&fileOrdr={file_ord}"
            logger.info(f"Accessing view URL: {view_url}")
            
            try:
                # í˜ì´ì§€ ë¡œë“œ
                driver.get(view_url)
                time.sleep(5)
                
                # ìƒˆ ì°½ì´ ì—´ë ¸ëŠ”ì§€ í™•ì¸
                if len(driver.window_handles) > 1:
                    # ìƒˆ ì°½ìœ¼ë¡œ ì „í™˜
                    driver.switch_to.window(driver.window_handles[-1])
                    logger.info(f"Switched to new window: {driver.current_url}")
                
                # SynapDocViewServer ê°ì§€
                current_url = driver.current_url
                if 'SynapDocViewServer' in current_url:
                    logger.info("Detected SynapDocViewServer viewer")
                    
                    # ë¬¸ì„œ ë‚´ìš© ì¶”ì¶œ ì‹œë„
                    # content_frame = driver.find_element(By.ID, "contents-area")
                    # if content_frame:
                    #     return self.extract_synap_content(driver)
                    
                    # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
                    date_match = re.search(r'\((\d{4})ë…„\s+(\d{1,2})ì›”ë§\s+ê¸°ì¤€\)', file_info['post_info']['title'])
                    if date_match:
                        year = int(date_match.group(1))
                        month = int(date_match.group(2))
                        
                        # ë¬¸ì„œ ì œëª©ì—ì„œ ë°ì´í„° ìœ í˜• ì¶”ì¶œ ì‹œë„
                        report_type = "unknown"
                        for rt in self.report_types:
                            if rt in file_info['post_info']['title']:
                                report_type = rt
                                break
                        
                        # ê°€ìƒ ë°ì´í„° ìƒì„± (ì‹¤ì œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ëŠ” ê²½ìš°)
                        df = pd.DataFrame({
                            'êµ¬ë¶„': [f'{month}ì›” í†µê³„'],
                            'ê°’': [f'ìë™ ìƒì„± - {report_type}'],
                            'ë¹„ê³ ': ['ë°”ë¡œë³´ê¸°ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.']
                        })
                        
                        return {
                            'type': 'dataframe',
                            'data': df,
                            'date': {'year': year, 'month': month},
                            'post_info': file_info['post_info']
                        }
                
                # ì¼ë°˜ HTML í˜ì´ì§€ì—ì„œ í…Œì´ë¸” ì¶”ì¶œ ì‹œë„
                tables = pd.read_html(driver.page_source)
                if tables:
                    logger.info(f"Found {len(tables)} tables in the view page")
                    
                    # ê°€ì¥ í° í…Œì´ë¸” ì„ íƒ
                    largest_table = max(tables, key=lambda df: df.size)
                    
                    # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
                    date_match = re.search(r'\((\d{4})ë…„\s+(\d{1,2})ì›”ë§\s+ê¸°ì¤€\)', file_info['post_info']['title'])
                    if date_match:
                        year = int(date_match.group(1))
                        month = int(date_match.group(2))
                        
                        return {
                            'type': 'dataframe',
                            'data': largest_table,
                            'date': {'year': year, 'month': month},
                            'post_info': file_info['post_info']
                        }
                
                # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ ì½˜í…ì¸  ê¸°ë°˜ ì²˜ë¦¬
                return self.process_content_data(file_info)
                
            except Exception as e:
                logger.error(f"Error processing view data: {str(e)}")
                return self.process_content_data(file_info)
            
        return None


    def process_content_data(self, file_info):
        """í…ìŠ¤íŠ¸ ì½˜í…ì¸ ì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
        if not file_info or not file_info.get('post_info'):
            return None
            
        try:
            post_info = file_info['post_info']
            content_text = file_info.get('content_data', '')
            
            # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
            date_match = re.search(r'\((\d{4})ë…„\s+(\d{1,2})ì›”ë§\s+ê¸°ì¤€\)', post_info['title'])
            if not date_match:
                logger.error(f"Could not extract date from title: {post_info['title']}")
                return None
                
            year = int(date_match.group(1))
            month = int(date_match.group(2))
            
            # í…ìŠ¤íŠ¸ì—ì„œ í…Œì´ë¸” êµ¬ì¡° ì°¾ê¸° ì‹œë„
            lines = content_text.split('\n')
            data_rows = []
            
            # ë¬¸ìì—´ ì²˜ë¦¬ - ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ íŒŒì‹±
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # ìˆ«ìê°€ í¬í•¨ëœ í–‰ì€ ë°ì´í„° í–‰ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
                if re.search(r'\d', line) and len(line) > 5:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                    cells = re.split(r'\s{2,}|\t', line)
                    if len(cells) >= 2:  # ìµœì†Œ 2ê°œ ì´ìƒì˜ ì…€ì´ ìˆì–´ì•¼ ë°ì´í„° í–‰
                        data_rows.append(cells)
            
            # ë°ì´í„°í”„ë ˆì„ ìƒì„±
            if data_rows:
                if len(data_rows) > 1:
                    # ì²« ë²ˆì§¸ í–‰ì„ í—¤ë”ë¡œ ì‚¬ìš©
                    df = pd.DataFrame(data_rows[1:], columns=data_rows[0])
                else:
                    # ë°ì´í„°ê°€ í•œ í–‰ë¿ì´ë¼ë©´ ê¸°ë³¸ ì»¬ëŸ¼ëª… ì‚¬ìš©
                    df = pd.DataFrame([data_rows[0]], columns=[f'Column{i}' for i in range(len(data_rows[0]))])
                
                logger.info(f"Created dataframe from content with shape {df.shape}")
            else:
                # ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ê¸°ë³¸ ë°ì´í„°í”„ë ˆì„ ìƒì„±
                logger.warning("No structured data found in content, creating placeholder dataframe")
                
                # ê²Œì‹œë¬¼ ì œëª©ì—ì„œ í†µê³„ ìœ í˜• ì¶”ì¶œ
                report_type = "í†µê³„"
                for rt in self.report_types:
                    if rt in post_info['title']:
                        report_type = rt
                        break
                
                df = pd.DataFrame({
                    'êµ¬ë¶„': [f'{month}ì›” {report_type}'],
                    'ê°’': ['ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'],
                    'ë¹„ê³ ': [post_info['title']]
                })
            
            return {
                'type': 'dataframe',
                'data': df,
                'date': {'year': year, 'month': month},
                'post_info': post_info
            }
            
        except Exception as e:
            logger.error(f"Error processing content data: {str(e)}")
            return None

    
    def process_excel_file(self, file_path, post_info):
        """Process the downloaded Excel/CSV file"""
        if not file_path or not file_path.exists():
            logger.error("Cannot process file: file not found")
            return None
        
        logger.info(f"Processing file: {file_path}")
        
        try:
            # Extract date from post title
            date_pattern = r'\((\d{4})ë…„\s+(\d{1,2})ì›”ë§\s+ê¸°ì¤€\)'
            match = re.search(date_pattern, post_info['title'])
            
            if not match:
                logger.error(f"Could not extract date from title: {post_info['title']}")
                return None
                
            year = int(match.group(1))
            month = int(match.group(2))
            
            # Read the file based on extension
            if file_path.suffix.lower() in ['.xlsx', '.xls']:
                # Read all sheets from Excel file
                xls = pd.ExcelFile(file_path)
                sheet_names = xls.sheet_names
                
                data = {}
                for sheet in sheet_names:
                    data[sheet] = pd.read_excel(file_path, sheet_name=sheet)
                    
                logger.info(f"Successfully processed Excel file with {len(sheet_names)} sheets")
                return {
                    'type': 'excel',
                    'sheets': data,
                    'date': {'year': year, 'month': month},
                    'post_info': post_info
                }
            elif file_path.suffix.lower() == '.csv':
                df = pd.read_csv(file_path)
                logger.info(f"Successfully processed CSV file with {len(df)} rows")
                return {
                    'type': 'csv',
                    'data': df,
                    'date': {'year': year, 'month': month},
                    'post_info': post_info
                }
            else:
                logger.error(f"Unsupported file format: {file_path.suffix}")
                return None
                
        except Exception as e:
            logger.error(f"Error processing file: {str(e)}")
            return None
        finally:
            # Clean up downloaded file
            try:
                os.unlink(file_path)
            except Exception as e:
                logger.warning(f"Could not delete temporary file {file_path}: {str(e)}")

    def determine_report_type(self, post_title):
        """Determine the report type from the post title"""
        for report_type in self.report_types:
            if report_type in post_title:
                return report_type
        return "ê¸°íƒ€ í†µì‹  í†µê³„"

    
    def update_google_sheets(self, client, data):
        """Update Google Sheets with improved data handling"""
        if not client or not data:
            logger.error("Cannot update Google Sheets: missing client or data")
            return False

        try:
            # Extract information
            date_info = data['date']
            post_info = data['post_info']
            report_type = self.determine_report_type(post_info['title'])
    
            # Format date string
            date_str = f"{date_info['year']}ë…„ {date_info['month']}ì›”"
    
            # Open or create spreadsheet
            try:
                # Try to open by ID first if provided
                if self.spreadsheet_id:
                    try:
                        spreadsheet = client.open_by_key(self.spreadsheet_id)
                        logger.info(f"Found existing spreadsheet by ID: {self.spreadsheet_id}")
                    except gspread.exceptions.APIError:
                        logger.warning(f"Could not open spreadsheet with ID: {self.spreadsheet_id}")
                        spreadsheet = None
                else:
                    spreadsheet = None
        
                # If not found by ID, try by name
                if not spreadsheet:
                    try:
                        spreadsheet = client.open(self.spreadsheet_name)
                        logger.info(f"Found existing spreadsheet by name: {self.spreadsheet_name}")
                    except gspread.exceptions.SpreadsheetNotFound:
                        # Create new spreadsheet
                        spreadsheet = client.create(self.spreadsheet_name)
                        logger.info(f"Created new spreadsheet: {self.spreadsheet_name}")
                
                        # Log the ID for future reference
                        logger.info(f"New spreadsheet ID: {spreadsheet.id}")
            except Exception as e:
                logger.error(f"Error opening Google Sheets: {str(e)}")
                return False
        
            # Find or create worksheet for this report type
            worksheet = None
            for sheet in spreadsheet.worksheets():
                if report_type in sheet.title:
                    worksheet = sheet
                    break
            
            if not worksheet:
                worksheet = spreadsheet.add_worksheet(title=report_type, rows="1000", cols="50")
                logger.info(f"Created new worksheet: {report_type}")
        
                # Add header row
                worksheet.update_cell(1, 1, "í•­ëª©")
        
            # Check if date column already exists
            headers = worksheet.row_values(1)
            if date_str in headers:
                col_idx = headers.index(date_str) + 1
                logger.info(f"Column for {date_str} already exists at position {col_idx}")
            else:
                # Add new column for this date
                col_idx = len(headers) + 1
                worksheet.update_cell(1, col_idx, date_str)
                logger.info(f"Added new column for {date_str} at position {col_idx}")
    
        # Process data based on file type
            if data['type'] == 'dataframe':
                # ë°ì´í„°í”„ë ˆì„ ì§ì ‘ ì‚¬ìš©
                df = data['data']
                self.update_sheet_from_dataframe(worksheet, df, col_idx)
        
            elif data['type'] == 'excel':
                # ì—‘ì…€ íŒŒì¼ì—ì„œ ì‹œíŠ¸ ì„ íƒ
                if len(data['sheets']) == 1:
                    # If only one sheet, use it
                    sheet_name = list(data['sheets'].keys())[0]
                    df = data['sheets'][sheet_name]
                else:
                    # Try to find most relevant sheet
                    best_sheet = None
                    for name in data['sheets'].keys():
                        if report_type in name or any(term in name for term in report_type.split()):
                            best_sheet = name
                            break
            
                    if not best_sheet:
                        # Use first sheet as fallback
                        best_sheet = list(data['sheets'].keys())[0]
                    
                    df = data['sheets'][best_sheet]
            
                # Update data from dataframe
                self.update_sheet_from_dataframe(worksheet, df, col_idx)
            
            elif data['type'] == 'csv':
                # Update from CSV data
                df = data['data']
                self.update_sheet_from_dataframe(worksheet, df, col_idx)
        
            logger.info(f"Successfully updated Google Sheets with {report_type} data for {date_str}")
            return True
    
        except Exception as e:
            logger.error(f"Error updating Google Sheets: {str(e)}")
            return False


    def update_sheet_from_dataframe(self, worksheet, df, col_idx):
        """Update worksheet with data from a dataframe with improved error handling"""
        try:
            # Get current row labels (first column)
            existing_labels = worksheet.col_values(1)[1:]  # Skip header
        
            if df.shape[0] > 0:
                # Get labels and values from dataframe
                # Assuming first column contains labels and second contains values
                if df.shape[1] >= 2:
                    # ì—´ ì´ë¦„ ì •ê·œí™” (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±° ë“±)
                    normalized_columns = [str(col).strip() for col in df.columns]
                    
                    # ì²« ë²ˆì§¸ ì—´ì„ ë¼ë²¨ë¡œ, ë‘ ë²ˆì§¸ ì—´ì„ ê°’ìœ¼ë¡œ ì‚¬ìš©
                    if df.shape[1] >= 2:
                        # ëª…í™•í•œ ì»¬ëŸ¼ ì„ íƒ
                        label_col = df.iloc[:, 0]
                        value_col = df.iloc[:, 1]
                    
                        new_labels = label_col.astype(str).tolist()
                        values = value_col.astype(str).tolist()
                    
                        # Batch update preparation
                        cell_updates = []
                    
                        for i, (label, value) in enumerate(zip(new_labels, values)):
                            if label and not pd.isna(label):  # Skip empty or NaN labels
                            # ë¼ë²¨ ì •ê·œí™” (íŠ¹ìˆ˜ë¬¸ì ë° ê³µë°± ì²˜ë¦¬)
                                label = str(label).strip()
                            
                                # Check if label already exists
                                if label in existing_labels:
                                    row_idx = existing_labels.index(label) + 2  # +2 for header and 0-indexing
                                else:
                                    # Add new row
                                    row_idx = len(existing_labels) + 2
                                    # Update label
                                    cell_updates.append({
                                        'range': f'A{row_idx}',
                                        'values': [[label]]
                                    })
                                    existing_labels.append(label)
                                
                            # Update value - NaN ì²˜ë¦¬
                                value_to_update = "" if pd.isna(value) else str(value).strip()
                                cell_updates.append({
                                    'range': f'{chr(64 + col_idx)}{row_idx}',
                                    'values': [[value_to_update]]
                                })
                    
                    # Execute batch update if there are updates
                        if cell_updates:
                            worksheet.batch_update(cell_updates)
                            logger.info(f"Updated {len(cell_updates)} cells in Google Sheets")
                            
                return True
            
            else:
                logger.warning("DataFrame is empty, no data to update")
                return False
            
        except Exception as e:
            logger.error(f"Error updating worksheet from dataframe: {str(e)}")
            return False


    async def send_telegram_message(self, posts, data_updates=None):
        """Send notification via Telegram with improved formatting"""
        if not posts and not data_updates:
            logger.info("No posts or data updates to notify about")
            return
        
        try:
            message = "ğŸ“Š *MSIT í†µì‹  í†µê³„ ëª¨ë‹ˆí„°ë§ ì•Œë¦¼*\n\n"
                
                # Add information about new posts
            if posts:
                message += "ğŸ“± *ìƒˆë¡œìš´ í†µì‹  ê´€ë ¨ ê²Œì‹œë¬¼:*\n\n"
            
                for post in posts:
                    message += f"ğŸ“… {post['date']}\n"
                    message += f"ğŸ“‘ {post['title']}\n"
                    message += f"ğŸ¢ {post['department']}\n"
                    if post.get('url'):
                        message += f"ğŸ”— [ê²Œì‹œë¬¼ ë°”ë¡œê°€ê¸°]({post['url']})\n"
                    message += "\n"
        
                # Add information about data updates
            if data_updates:
                message += "ğŸ“Š *Google Sheets ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ:*\n\n"
            
                for update in data_updates:
                    report_type = self.determine_report_type(update['post_info']['title'])
                    date_str = f"{update['date']['year']}ë…„ {update['date']['month']}ì›”"
                
                    message += f"ğŸ“… *{date_str}*\n"
                    message += f"ğŸ“‘ {report_type}\n"
                    message += f"ğŸ“— ì—…ë°ì´íŠ¸ ì™„ë£Œ\n\n"
        
                # Send the message
            chat_id = int(self.chat_id)
            await self.bot.send_message(
                chat_id=chat_id,
                text=message,
                parse_mode='Markdown'
            )
            logger.info("Telegram message sent successfully")
        
        except Exception as e:
            logger.error(f"Error sending Telegram message: {str(e)}")


    async def run_monitor(self, days_range=4, check_sheets=True):
            """Main monitoring function with improved error handling"""
            driver = None
            gs_client = None

            try:
                # Initialize WebDriver
                driver = self.setup_driver()
                logger.info("WebDriver initialized successfully")
    
                # Initialize Google Sheets client if needed
                if check_sheets and self.gspread_creds:
                    gs_client = self.setup_gspread_client()
                    if gs_client:
                        logger.info("Google Sheets client initialized successfully")
                    else:
                        logger.warning("Failed to initialize Google Sheets client")
    
                # Navigate to MSIT website
                driver.get(self.url)
                logger.info("Navigated to MSIT website")
    
                # Variables to track posts
                all_posts = []
                telecom_stats_posts = []
                continue_search = True
    
                # Parse pages
                while continue_search:
                    posts, stats_posts, should_continue = self.parse_page(driver, days_range=days_range)
                    all_posts.extend(posts)
                    telecom_stats_posts.extend(stats_posts)
        
                    if not should_continue:
                        break
                    
                    if self.has_next_page(driver):
                        if not self.go_to_next_page(driver):
                            break
                    else:
                        break
    
                # Process telecom stats posts if Google Sheets client is available
                data_updates = []
                if gs_client and telecom_stats_posts and check_sheets:
                    logger.info(f"Processing {len(telecom_stats_posts)} telecom stats posts")

                    for post in telecom_stats_posts:
                        # 1. íŒŒì¼ ì •ë³´ ì¶”ì¶œ
                        file_info = self.extract_file_info(driver, post)
                        if not file_info:
                            logger.warning(f"No file information found for post: {post['title']}")
                            continue
                
                        # 2. ë°ì´í„° ì²˜ë¦¬ - ë°”ë¡œë³´ê¸° ë˜ëŠ” ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì‹œë„
                        self.direct_download_attempt = 0  # ì‹œë„ íšŸìˆ˜ ì´ˆê¸°í™”
                        data = self.process_view_data(driver, file_info)
                
                        if data:
                            # 3. Google Sheets ì—…ë°ì´íŠ¸
                            success = self.update_google_sheets(gs_client, data)
                            if success:
                                logger.info(f"Successfully updated Google Sheets for: {post['title']}")
                                data_updates.append(data)
                            else:
                                logger.warning(f"Failed to update Google Sheets for: {post['title']}")
                        else:
                            logger.warning(f"Failed to extract data for post: {post['title']}")
        
                # Send Telegram notification if there are new posts or data updates
                if all_posts or data_updates:
                    await self.send_telegram_message(all_posts, data_updates)
                else:
                    logger.info(f"No new posts found within the last {days_range} days")
    
            except Exception as e:
                error_message = f"Error in run_monitor: {str(e)}"
                logger.error(error_message, exc_info=True)
    
                # Send error notification
                try:
                    await self.send_telegram_message([{
                        'title': f"ëª¨ë‹ˆí„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                        'date': datetime.now().strftime('%Y. %m. %d'),
                        'department': 'System Error'
                    }])
                except Exception as telegram_err:
                    logger.error(f"Error sending Telegram notification: {str(telegram_err)}")
    
            finally:
        # Clean up
                if driver:
                    driver.quit()
                    logger.info("WebDriver closed")

def extract_file_info(self, driver, post):
    """Extract file information using the 'View' button instead of download"""
    if not post.get('post_id'):
        logger.error(f"Cannot access post {post['title']} - missing post ID")
        return None
    
    logger.info(f"Opening post: {post['title']}")
    
    # ê²Œì‹œë¬¼ ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™
    detail_url = f"https://www.msit.go.kr/bbs/view.do?sCode=user&mId=99&mPid=74&nttSeqNo={post['post_id']}"
    driver.get(detail_url)
    time.sleep(3)  # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
    
    # í˜„ì¬ í˜ì´ì§€ URL í™•ì¸
    current_url = driver.current_url
    logger.info(f"Current page URL: {current_url}")
    
    # ë°”ë¡œë³´ê¸° ë§í¬ ì°¾ê¸°
    try:
        # 1. í´ë˜ìŠ¤ë¡œ ì°¾ê¸°
        view_links = driver.find_elements(By.CSS_SELECTOR, "a.view[title='ìƒˆì°½ ì—´ë¦¼']")
        
        # 2. ë‹¤ë¥¸ ë°©ë²•: onclick ì†ì„±ì— getExtension_pathê°€ í¬í•¨ëœ ë§í¬ ì°¾ê¸°
        if not view_links:
            all_links = driver.find_elements(By.TAG_NAME, "a")
            view_links = [link for link in all_links if 'getExtension_path' in (link.get_attribute('onclick') or '')]
        
        # 3. í…ìŠ¤íŠ¸ë¡œ ì°¾ê¸°
        if not view_links:
            all_links = driver.find_elements(By.TAG_NAME, "a")
            view_links = [link for link in all_links if 'ë°”ë¡œë³´ê¸°' in (link.text or '')]
        
        # ì ì ˆí•œ ë§í¬ ë°œê²¬
        if view_links:
            view_link = view_links[0]
            onclick_attr = view_link.get_attribute('onclick')
            logger.info(f"Found view link with onclick: {onclick_attr}")
            
            # getExtension_path('49234', '1')ì—ì„œ ë§¤ê°œë³€ìˆ˜ ì¶”ì¶œ
            match = re.search(r"getExtension_path\('(\d+)',\s*'(\d+)'\)", onclick_attr)
            if match:
                atch_file_no = match.group(1)
                file_ord = match.group(2)
                
                # íŒŒì¼ ì´ë¦„ì€ ë¶€ëª¨ ìš”ì†Œì—ì„œ ì¶”ì¶œí•  ìˆ˜ ìˆìŒ
                try:
                    parent_li = view_link.find_element(By.XPATH, "./ancestor::li")
                    file_name_element = parent_li.find_element(By.TAG_NAME, "a")
                    file_name = file_name_element.text.strip()
                except:
                    # íŒŒì¼ ì´ë¦„ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°, ê²Œì‹œë¬¼ ì œëª©ì—ì„œ ìœ ì¶”
                    date_match = re.search(r'\((\d{4})ë…„\s+(\d{1,2})ì›”ë§\s+ê¸°ì¤€\)', post['title'])
                    if date_match:
                        year = date_match.group(1)
                        month = date_match.group(2).zfill(2)
                        file_name = f"{year}ë…„ {month}ì›”ë§ ê¸°ì¤€ í†µê³„.xlsx"
                    else:
                        file_name = f"í†µê³„ìë£Œ_{atch_file_no}.xlsx"
                
                file_info = {
                    'file_name': file_name,
                    'atch_file_no': atch_file_no,
                    'file_ord': file_ord,
                    'use_view': True  # ë°”ë¡œë³´ê¸° ì‚¬ìš© í”Œë˜ê·¸
                }
                
                logger.info(f"Successfully extracted file info for view: {file_name}")
                return file_info
            else:
                logger.error(f"Could not extract file params from onclick: {onclick_attr}")
        else:
            logger.warning("No view links found")
            
            # í˜ì´ì§€ êµ¬ì¡° ë¡œê¹…
            logger.info("Available link elements:")
            all_links = driver.find_elements(By.TAG_NAME, "a")
            for i, link in enumerate(all_links[:10]):  # ì²˜ìŒ 10ê°œë§Œ ë¡œê¹…
                logger.info(f"Link {i+1}: text='{link.text}', onclick='{link.get_attribute('onclick')}'")
    
    except Exception as e:
        logger.error(f"Error finding view link: {str(e)}")
    
    return None

def access_view_page(self, driver, file_info):
    """Access the view page instead of downloading the file"""
    if not file_info or not file_info.get('use_view'):
        return None
    
    logger.info(f"Accessing view page for file: {file_info['file_name']}")
    
    # ë°”ë¡œë³´ê¸° URL êµ¬ì„±
    view_url = f"https://www.msit.go.kr/bbs/documentView.do?atchFileNo={file_info['atch_file_no']}&fileOrdr={file_info['file_ord']}"
    
    logger.info(f"View URL: {view_url}")
    driver.get(view_url)
    
    # ìƒˆ ì°½ì—ì„œ ì—´ë¦¬ëŠ” ê²½ìš° í•¸ë“¤ ì „í™˜
    original_window = driver.current_window_handle
    if len(driver.window_handles) > 1:
        for window_handle in driver.window_handles:
            if window_handle != original_window:
                driver.switch_to.window(window_handle)
                logger.info(f"Switched to new window: {driver.current_url}")
                break
    
    # ë·°ì–´ í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
    try:
        # ëª‡ ê°€ì§€ ê°€ëŠ¥í•œ ìš”ì†Œ ëŒ€ê¸°
        WebDriverWait(driver, 20).until(
            lambda x: (
                len(x.find_elements(By.ID, "mainTable")) > 0 or  # í…Œì´ë¸” í˜•ì‹
                len(x.find_elements(By.TAG_NAME, "table")) > 0 or  # ì¼ë°˜ í…Œì´ë¸”
                len(x.find_elements(By.TAG_NAME, "iframe")) > 0  # iframe ë‚´ ì½˜í…ì¸ 
            )
        )
        
        logger.info("View page loaded successfully")
        return True
    except TimeoutException:
        logger.error("Timeout waiting for view page to load")
        
        # í˜ì´ì§€ ì†ŒìŠ¤ ë¡œê¹…
        logger.info("Current URL: " + driver.current_url)
        logger.info("Page source snippet:")
        logger.info(driver.page_source[:500])
        
        return False
    except Exception as e:
        logger.error(f"Error accessing view page: {str(e)}")
        return False

def extract_data_from_view(self, driver):
    """Extract data from the view page"""
    try:
        # í˜ì´ì§€ì— iframeì´ ìˆëŠ”ì§€ í™•ì¸
        iframes = driver.find_elements(By.TAG_NAME, "iframe")
        if iframes:
            logger.info(f"Found {len(iframes)} iframes on the page")
            # iframeìœ¼ë¡œ ì „í™˜
            driver.switch_to.frame(iframes[0])
            logger.info("Switched to iframe")
        
        # ë°ì´í„° ì¶”ì¶œ ì‹œë„
        tables = []
        
        # 1. mainTable ìš”ì†Œ í™•ì¸ (MSIT íŠ¹í™” êµ¬ì¡°)
        main_table = driver.find_elements(By.ID, "mainTable")
        if main_table:
            logger.info("Found mainTable element")
            
            # mainTable ë‚´ì˜ ëª¨ë“  div ìš”ì†Œë¥¼ ì°¾ì•„ í…Œì´ë¸” í˜•íƒœë¡œ ì¬êµ¬ì„±
            rows = []
            current_row = []
            row_count = 0
            
            divs = main_table[0].find_elements(By.TAG_NAME, "div")
            for div in divs:
                # í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ì…€ ìœ í˜• êµ¬ë¶„
                class_name = div.get_attribute("class") or ""
                
                # ì…€ ë°ì´í„° ì¶”ì¶œ
                cell_text = div.text.strip()
                
                # td í´ë˜ìŠ¤ê°€ ìˆìœ¼ë©´ ë°ì´í„° ì…€
                if "td" in class_name:
                    current_row.append(cell_text)
                
                # tr í´ë˜ìŠ¤ê°€ ìˆìœ¼ë©´ í–‰ êµ¬ë¶„
                if "tr" in class_name or "row" in class_name:
                    row_count += 1
                    if current_row:
                        rows.append(current_row)
                        current_row = []
            
            # ë§ˆì§€ë§‰ í–‰ ì¶”ê°€
            if current_row:
                rows.append(current_row)
            
            # ë°ì´í„°í”„ë ˆì„ ìƒì„±
            if rows:
                df = pd.DataFrame(rows)
                tables.append(df)
                logger.info(f"Extracted table with shape {df.shape}")
        
        # 2. ì¼ë°˜ HTML í…Œì´ë¸” í™•ì¸
        html_tables = driver.find_elements(By.TAG_NAME, "table")
        if html_tables:
            logger.info(f"Found {len(html_tables)} HTML tables")
            
            for i, table in enumerate(html_tables):
                rows = []
                
                # í—¤ë” ì¶”ì¶œ
                headers = table.find_elements(By.TAG_NAME, "th")
                if headers:
                    header_row = [h.text.strip() for h in headers]
                    rows.append(header_row)
                
                # ë°ì´í„° í–‰ ì¶”ì¶œ
                tr_elements = table.find_elements(By.TAG_NAME, "tr")
                for tr in tr_elements:
                    # td ìš”ì†Œ ì¶”ì¶œ
                    cells = tr.find_elements(By.TAG_NAME, "td")
                    if cells:
                        row = [cell.text.strip() for cell in cells]
                        rows.append(row)
                
                # ë°ì´í„°í”„ë ˆì„ ìƒì„±
                if rows:
                    df = pd.DataFrame(rows[1:], columns=rows[0] if rows else None)
                    tables.append(df)
                    logger.info(f"Extracted HTML table {i+1} with shape {df.shape}")
        
        # ë°ì´í„°í”„ë ˆì„ì´ ì—†ëŠ” ê²½ìš° ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ íŒŒì‹± ì‹œë„
        if not tables:
            logger.warning("No tables found, attempting to extract structured text")
            
            # ëª¨ë“  í…ìŠ¤íŠ¸ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°
            body_text = driver.find_element(By.TAG_NAME, "body").text
            
            # ì¤„ ë‹¨ìœ„ë¡œ ë¶„í• 
            lines = body_text.split('\n')
            
            # ë°ì´í„° í–‰ ì¸ì‹ (ìˆ«ìì™€ í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ í–‰)
            data_rows = []
            for line in lines:
                # ìˆ«ìì™€ ë¬¸ìê°€ ëª¨ë‘ í¬í•¨ëœ í–‰ì„ ë°ì´í„°ë¡œ ê°€ì •
                if re.search(r'\d', line) and re.search(r'[ê°€-í£a-zA-Z]', line):
                    # ê³µë°±ì´ë‚˜ íƒ­ìœ¼ë¡œ ë¶„í• 
                    cells = re.split(r'\s{2,}|\t', line)
                    if len(cells) >= 2:  # ìµœì†Œ 2ê°œ ì´ìƒì˜ ì…€ì´ ìˆì–´ì•¼ ë°ì´í„° í–‰
                        data_rows.append(cells)
            
            if data_rows:
                # ì²« ë²ˆì§¸ í–‰ì„ í—¤ë”ë¡œ ê°€ì •
                df = pd.DataFrame(data_rows[1:], columns=data_rows[0] if data_rows else None)
                tables.append(df)
                logger.info(f"Extracted structured text as table with shape {df.shape}")
        
        # ì¶”ì¶œëœ í…Œì´ë¸”ì´ ìˆëŠ”ì§€ í™•ì¸
        if tables:
            # ê°€ì¥ í° í…Œì´ë¸” ì„ íƒ (ê°€ì¥ ë§ì€ ë°ì´í„°ë¥¼ í¬í•¨)
            largest_table = max(tables, key=lambda df: df.size)
            
            logger.info(f"Selected largest table with shape {largest_table.shape}")
            logger.info(f"Table columns: {largest_table.columns.tolist()}")
            logger.info(f"First few rows: {largest_table.head(3).to_dict()}")
            
            return largest_table
        else:
            logger.error("No data could be extracted from the view page")
            return None
        
    except Exception as e:
        logger.error(f"Error extracting data from view: {str(e)}")
        return None
    finally:
        # iframeì—ì„œ ë¹ ì ¸ë‚˜ì˜´
        try:
            driver.switch_to.default_content()
        except:
            pass


async def main():
    # Get environment variables
    days_range = int(os.environ.get('DAYS_RANGE', '4'))
    check_sheets = os.environ.get('CHECK_SHEETS', 'true').lower() == 'true'
    

    # Create and run monitor
    try:
        logger.info(f"Starting MSIT Monitor with days_range={days_range}, check_sheets={check_sheets}")
        logger.info(f"Spreadsheet name: {os.environ.get('SPREADSHEET_NAME', 'MSIT í†µì‹  í†µê³„')}")
        
        monitor = MSITMonitor()
        await monitor.run_monitor(days_range=days_range, check_sheets=check_sheets)
    except Exception as e:
        logging.error(f"Main function error: {str(e)}", exc_info=True)

if __name__ == "__main__":
    asyncio.run(main())
