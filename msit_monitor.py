import os
import re
import json
import time
import logging
import asyncio
from datetime import datetime, timedelta
from pathlib import Path
import pandas as pd
import requests
from urllib.parse import urlparse, parse_qs

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

from bs4 import BeautifulSoup
import telegram
import gspread
from oauth2client.service_account import ServiceAccountCredentials

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('msit_monitor')

class MSITMonitor:
    def __init__(self):
        # ê¸°ì¡´ ì´ˆê¸°í™” ì½”ë“œ ìœ ì§€...
        self.telegram_token = os.environ.get('TELCO_NEWS_TOKEN')
        self.chat_id = os.environ.get('TELCO_NEWS_TESTER')
        
        self.gspread_creds = os.environ.get('MSIT_GSPREAD_ref')
        self.spreadsheet_id = os.environ.get('MSIT_SPREADSHEET_ID')
        self.spreadsheet_name = os.environ.get('SPREADSHEET_NAME', 'MSIT í†µì‹  í†µê³„')
        
        self.url = "https://www.msit.go.kr/bbs/list.do?sCode=user&mPid=74&mId=99"
        self.bot = telegram.Bot(token=self.telegram_token)
        
        self.report_types = [
            "ì´ë™ì „í™” ë° íŠ¸ë˜í”½ í†µê³„",
            "ì´ë™ì „í™” ë° ì‹œë‚´ì „í™” ë²ˆí˜¸ì´ë™ í˜„í™©",
            "ìœ ì„ í†µì‹ ì„œë¹„ìŠ¤ ê°€ì… í˜„í™©",
            "ë¬´ì„ í†µì‹ ì„œë¹„ìŠ¤ ê°€ì… í˜„í™©", 
            "íŠ¹ìˆ˜ë¶€ê°€í†µì‹ ì‚¬ì—…ìí˜„í™©",
            "ë¬´ì„ ë°ì´í„° íŠ¸ë˜í”½ í†µê³„",
            "ìœ Â·ë¬´ì„ í†µì‹ ì„œë¹„ìŠ¤ ê°€ì… í˜„í™© ë° ë¬´ì„ ë°ì´í„° íŠ¸ë˜í”½ í†µê³„"
        ]
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬
        self.temp_dir = Path("./downloads")
        self.temp_dir.mkdir(exist_ok=True)

    def setup_driver(self):
        """Selenium WebDriver ì„¤ì •"""
        options = Options()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        
        # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™”ë¡œ ì„±ëŠ¥ í–¥ìƒ
        prefs = {
            "profile.default_content_setting_values.images": 2
        }
        options.add_experimental_option("prefs", prefs)
        
        # ì„œë¹„ìŠ¤ ì„¤ì •
        if os.path.exists('/usr/bin/chromium-browser'):
            options.binary_location = '/usr/bin/chromium-browser'
            service = Service('/usr/bin/chromedriver')
        else:
            try:
                from webdriver_manager.chrome import ChromeDriverManager
                service = Service(ChromeDriverManager().install())
            except ImportError:
                service = Service('/usr/bin/chromedriver')
        
        driver = webdriver.Chrome(service=service, options=options)
        driver.set_page_load_timeout(60)
        return driver

    def is_telecom_stats_post(self, title):
        """í†µì‹  í†µê³„ ê²Œì‹œë¬¼ì¸ì§€ í™•ì¸"""
        date_pattern = r'\((\d{4})ë…„\s+(\d{1,2})ì›”ë§\s+ê¸°ì¤€\)'
        has_date_pattern = re.search(date_pattern, title) is not None
        contains_report_type = any(report_type in title for report_type in self.report_types)
        return has_date_pattern and contains_report_type

    def parse_board(self, driver, days_range=4):
        """ê²Œì‹œíŒ íŒŒì‹± ë° ê´€ë ¨ ê²Œì‹œë¬¼ ì¶”ì¶œ"""
        logger.info("ê³¼ê¸°ì •í†µë¶€ í†µì‹  í†µê³„ ê²Œì‹œíŒ ì ‘ê·¼ ì¤‘...")
        driver.get(self.url)
        
        try:
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.CLASS_NAME, 'board_list'))
            )
        except TimeoutException:
            # ì‹œìŠ¤í…œ ì ê²€ í˜ì´ì§€ í™•ì¸
            if "ì‹œìŠ¤í…œ ì ê²€ ì•ˆë‚´" in driver.page_source:
                logger.warning("ì‹œìŠ¤í…œ ì ê²€ ì¤‘ì…ë‹ˆë‹¤.")
                return []
            logger.error("ê²Œì‹œíŒ ë¡œë”© íƒ€ì„ì•„ì›ƒ")
            return []
            
        all_posts = []
        telecom_posts = []
        
        # í˜ì´ì§€ íŒŒì‹±
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        items = soup.select('div.toggle')
        
        for item in items:
            title_tag = item.find('p', class_='title')
            date_tag = item.find('div', class_='date')
            dept_tag = item.find('dd', {'id': lambda x: x and 'td_CHRG_DEPT_NM' in x})
            
            if not title_tag or not date_tag:
                continue
                
            title = title_tag.text.strip()
            date_str = date_tag.text.strip()
            dept = dept_tag.text.strip() if dept_tag else "ë¶€ì„œ ì •ë³´ ì—†ìŒ"
            
            # ë‚ ì§œ í™•ì¸
            try:
                post_date = self.parse_date(date_str)
                cutoff_date = (datetime.now() - timedelta(days=days_range)).date()
                if post_date < cutoff_date:
                    continue
            except ValueError:
                # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ í¬í•¨ (ì •í™•í•œ í•„í„°ë§ì„ ìœ„í•´)
                logger.warning(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_str}")
            
            # ê²Œì‹œë¬¼ ID ì¶”ì¶œ
            onclick = title_tag.find('a').get('onclick', '')
            match = re.search(r"fn_detail\((\d+)\)", onclick)
            if not match:
                continue
                
            post_id = match.group(1)
            post_url = f"https://www.msit.go.kr/bbs/view.do?sCode=user&mId=99&mPid=74&nttSeqNo={post_id}"
            
            post_info = {
                'title': title,
                'date': date_str,
                'department': dept,
                'post_id': post_id,
                'url': post_url
            }
            
            all_posts.append(post_info)
            
            # í†µì‹  í†µê³„ ê²Œì‹œë¬¼ ë¶„ë¥˜
            if self.is_telecom_stats_post(title):
                telecom_posts.append(post_info)
                logger.info(f"í†µì‹  í†µê³„ ê²Œì‹œë¬¼ ë°œê²¬: {title}")
        
        return all_posts, telecom_posts

    def parse_date(self, date_str):
        """ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹±"""
        date_str = date_str.replace(',', ' ').strip()
        
        # ë‹¤ì–‘í•œ í˜•ì‹ ì‹œë„
        formats = [
            '%Y. %m. %d',  # 2025. 4. 10
            '%b %d %Y',    # Apr 10 2025
            '%Y-%m-%d'     # 2025-04-10
        ]
        
        for fmt in formats:
            try:
                return datetime.strptime(date_str, fmt).date()
            except ValueError:
                continue
                
        # ì •ê·œì‹ìœ¼ë¡œ ì‹œë„
        match = re.search(r'(\d{4})[.\-\s]+(\d{1,2})[.\-\s]+(\d{1,2})', date_str)
        if match:
            year, month, day = map(int, match.groups())
            return datetime(year, month, day).date()
            
        raise ValueError(f"ë‚ ì§œ í˜•ì‹ì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŒ: {date_str}")

    def extract_file_info(self, driver, post):
        """ê²Œì‹œë¬¼ì—ì„œ íŒŒì¼ ì •ë³´ ì¶”ì¶œ"""
        if not post.get('post_id'):
            return None
            
        logger.info(f"ê²Œì‹œë¬¼ ì—´ê¸°: {post['title']}")
        
        detail_url = f"https://www.msit.go.kr/bbs/view.do?sCode=user&mId=99&mPid=74&nttSeqNo={post['post_id']}"
        driver.get(detail_url)
        
        try:
            # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
            WebDriverWait(driver, 15).until(
                lambda x: (
                    len(x.find_elements(By.CLASS_NAME, "view_head")) > 0 or 
                    len(x.find_elements(By.CLASS_NAME, "view_file")) > 0
                )
            )
        except TimeoutException:
            # ì‹œìŠ¤í…œ ì ê²€ í˜ì´ì§€ í™•ì¸
            if "ì‹œìŠ¤í…œ ì ê²€ ì•ˆë‚´" in driver.page_source:
                logger.warning("ì‹œìŠ¤í…œ ì ê²€ ì¤‘ì…ë‹ˆë‹¤.")
                return {'post_info': post, 'system_maintenance': True}
            logger.error("ìƒì„¸ í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ")
            return None
            
        # ë°”ë¡œë³´ê¸° ë§í¬ ì°¾ê¸°
        view_links = driver.find_elements(By.CSS_SELECTOR, "a.view[title='ìƒˆì°½ ì—´ë¦¼']")
        if not view_links:
            view_links = driver.find_elements(By.CSS_SELECTOR, "a[onclick*='getExtension_path']")
        
        if view_links:
            view_link = view_links[0]
            onclick_attr = view_link.get_attribute('onclick')
            
            # getExtension_path('49234', '1') í˜•ì‹ì—ì„œ ë§¤ê°œë³€ìˆ˜ ì¶”ì¶œ
            match = re.search(r"getExtension_path\s*\(\s*['\"]([\d]+)['\"]?\s*,\s*['\"]([\d]+)['\"]", onclick_attr)
            if match:
                atch_file_no = match.group(1)
                file_ord = match.group(2)
                
                return {
                    'atch_file_no': atch_file_no,
                    'file_ord': file_ord,
                    'post_info': post
                }
        
        # ë°”ë¡œë³´ê¸° ë§í¬ê°€ ì—†ìœ¼ë©´ ê²Œì‹œë¬¼ ë‚´ìš© ì¶”ì¶œ
        content = driver.find_element(By.CLASS_NAME, "view_cont").text
        return {
            'content': content,
            'post_info': post
        }

    def access_document_viewer(self, driver, atch_file_no, file_ord):
        """SynapDocViewServer ë¬¸ì„œ ë·°ì–´ ì ‘ê·¼"""
        view_url = f"https://www.msit.go.kr/bbs/documentView.do?atchFileNo={atch_file_no}&fileOrdr={file_ord}"
        logger.info(f"ë¬¸ì„œ ë·°ì–´ ì ‘ê·¼: {view_url}")
        
        driver.get(view_url)
        time.sleep(5)  # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
        
        # SynapDocViewServer ë·°ì–´ ê°ì§€
        current_url = driver.current_url
        if 'SynapDocViewServer' not in current_url:
            logger.warning("SynapDocViewServer ë·°ì–´ê°€ ì•„ë‹™ë‹ˆë‹¤.")
            return None
            
        # ì‹œíŠ¸ ì •ë³´ ì¶”ì¶œ (íƒ­ ëª©ë¡)
        sheet_list = driver.find_elements(By.CSS_SELECTOR, ".sheet-list__sheet-tab")
        sheets = [sheet.text for sheet in sheet_list]
        logger.info(f"ë°œê²¬ëœ ì‹œíŠ¸: {sheets}")
        
        data_frames = {}
        
        # ê° ì‹œíŠ¸ ì ‘ê·¼ ë° ë°ì´í„° ì¶”ì¶œ
        for i, sheet_name in enumerate(sheets):
            try:
                # ê° ì‹œíŠ¸ë¡œ ì „í™˜
                if i > 0:  # ì²« ë²ˆì§¸ ì‹œíŠ¸ëŠ” ì´ë¯¸ í™œì„±í™”ë¨
                    sheet_tab = driver.find_element(By.ID, f"sheet{i}")
                    sheet_tab.click()
                    time.sleep(2)  # ì‹œíŠ¸ ì „í™˜ ëŒ€ê¸°
                
                # iframe ì ‘ê·¼
                iframe = driver.find_element(By.ID, "innerWrap")
                driver.switch_to.frame(iframe)
                
                # í…Œì´ë¸” ì¶”ì¶œ
                iframe_content = driver.page_source
                df = self.extract_table_from_html(iframe_content)
                
                if df is not None:
                    data_frames[sheet_name] = df
                    logger.info(f"ì‹œíŠ¸ '{sheet_name}'ì—ì„œ ë°ì´í„° ì¶”ì¶œ ì„±ê³µ")
                
                # ê¸°ë³¸ í”„ë ˆì„ìœ¼ë¡œ ë³µê·€
                driver.switch_to.default_content()
                
            except Exception as e:
                logger.error(f"ì‹œíŠ¸ '{sheet_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                driver.switch_to.default_content()  # ì˜¤ë¥˜ ë°œìƒí•´ë„ ê¸°ë³¸ í”„ë ˆì„ìœ¼ë¡œ ë³µê·€
        
        return data_frames

    def extract_table_from_html(self, html_content):
        """HTMLì—ì„œ í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # í…Œì´ë¸” ìš”ì†Œ ì°¾ê¸°
        tables = soup.find_all('table')
        if not tables:
            logger.warning("í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return None
            
        # ê°€ì¥ í° í…Œì´ë¸” ì„ íƒ (ì¼ë°˜ì ìœ¼ë¡œ ì£¼ìš” ë°ì´í„° í…Œì´ë¸”)
        largest_table = max(tables, key=lambda t: len(t.find_all('tr')))
        
        # í…Œì´ë¸”ì—ì„œ í–‰ ì¶”ì¶œ
        rows = []
        for tr in largest_table.find_all('tr'):
            row = [td.get_text(strip=True) for td in tr.find_all(['td', 'th'])]
            if row and any(row):  # ë¹ˆ í–‰ ì œì™¸
                rows.append(row)
        
        # í–‰ì´ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ì‹¤íŒ¨
        if len(rows) < 2:
            logger.warning("ì¶©ë¶„í•œ ë°ì´í„° í–‰ì´ ì—†ìŒ")
            return None
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„± - ì²« í–‰ì„ í—¤ë”ë¡œ
        df = pd.DataFrame(rows[1:], columns=rows[0])
        return df

    def update_google_sheets(self, client, data):
        """Google Sheets ì—…ë°ì´íŠ¸"""
        if not client or not data:
            return False
            
        try:
            post_info = data['post_info']
            
            # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
            date_match = re.search(r'\((\d{4})ë…„\s+(\d{1,2})ì›”ë§\s+ê¸°ì¤€\)', post_info['title'])
            if not date_match:
                logger.error(f"ì œëª©ì—ì„œ ë‚ ì§œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ: {post_info['title']}")
                return False
                
            year = int(date_match.group(1))
            month = int(date_match.group(2))
            date_str = f"{year}ë…„ {month}ì›”"
            
            # ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì—´ê¸°
            if self.spreadsheet_id:
                spreadsheet = client.open_by_key(self.spreadsheet_id)
            else:
                spreadsheet = client.open(self.spreadsheet_name)
            
            # ê° ì‹œíŠ¸ ì²˜ë¦¬
            updated = False
            
            if 'sheets' in data:  # ì—¬ëŸ¬ ì‹œíŠ¸ ë°ì´í„°
                for sheet_name, df in data['sheets'].items():
                    # ì›Œí¬ì‹œíŠ¸ ì°¾ê¸° ë˜ëŠ” ìƒì„±
                    try:
                        worksheet = spreadsheet.worksheet(sheet_name)
                    except gspread.exceptions.WorksheetNotFound:
                        worksheet = spreadsheet.add_worksheet(title=sheet_name, rows="1000", cols="50")
                        worksheet.update_cell(1, 1, "í•­ëª©")
                    
                    # ë‚ ì§œ ì—´ í™•ì¸
                    headers = worksheet.row_values(1)
                    if date_str in headers:
                        col_idx = headers.index(date_str) + 1
                    else:
                        col_idx = len(headers) + 1
                        worksheet.update_cell(1, col_idx, date_str)
                    
                    # ë°ì´í„° ì—…ë°ì´íŠ¸
                    self.update_worksheet_with_df(worksheet, df, col_idx)
                    updated = True
                    
            elif 'dataframe' in data:  # ë‹¨ì¼ ë°ì´í„°í”„ë ˆì„
                # ì ì ˆí•œ ì›Œí¬ì‹œíŠ¸ ì°¾ê¸°
                report_type = next((rt for rt in self.report_types if rt in post_info['title']), "ê¸°íƒ€ í†µê³„")
                
                try:
                    worksheet = spreadsheet.worksheet(report_type)
                except gspread.exceptions.WorksheetNotFound:
                    worksheet = spreadsheet.add_worksheet(title=report_type, rows="1000", cols="50")
                    worksheet.update_cell(1, 1, "í•­ëª©")
                
                # ë‚ ì§œ ì—´ í™•ì¸
                headers = worksheet.row_values(1)
                if date_str in headers:
                    col_idx = headers.index(date_str) + 1
                else:
                    col_idx = len(headers) + 1
                    worksheet.update_cell(1, col_idx, date_str)
                
                # ë°ì´í„° ì—…ë°ì´íŠ¸
                self.update_worksheet_with_df(worksheet, data['dataframe'], col_idx)
                updated = True
            
            return updated
            
        except Exception as e:
            logger.error(f"Google Sheets ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False

    def update_worksheet_with_df(self, worksheet, df, col_idx):
        """ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì›Œí¬ì‹œíŠ¸ ì—…ë°ì´íŠ¸"""
        try:
            # ê¸°ì¡´ í•­ëª© (ì²« ë²ˆì§¸ ì—´) ê°€ì ¸ì˜¤ê¸°
            existing_items = worksheet.col_values(1)[1:]  # í—¤ë” ì œì™¸
            
            # ë°ì´í„°í”„ë ˆì„ì˜ í•­ëª© ë° ê°’ ì¶”ì¶œ
            if df.shape[1] >= 2:
                items = df.iloc[:, 0].astype(str).tolist()
                values = df.iloc[:, 1].astype(str).tolist()
                
                # ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì¤€ë¹„
                updates = []
                
                for i, (item, value) in enumerate(zip(items, values)):
                    if not item or pd.isna(item):
                        continue
                        
                    # í•­ëª©ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                    if item in existing_items:
                        row_idx = existing_items.index(item) + 2  # í—¤ë”(1) + 0-ì¸ë±ìŠ¤ ë³´ì •(1)
                    else:
                        # ìƒˆ í•­ëª© ì¶”ê°€
                        row_idx = len(existing_items) + 2
                        updates.append({
                            'range': f'A{row_idx}',
                            'values': [[item]]
                        })
                        existing_items.append(item)
                    
                    # ê°’ ì—…ë°ì´íŠ¸
                    value_to_update = "" if pd.isna(value) else value
                    updates.append({
                        'range': f'{chr(64 + col_idx)}{row_idx}',
                        'values': [[value_to_update]]
                    })
                
                # ì¼ê´„ ì—…ë°ì´íŠ¸ ì‹¤í–‰
                if updates:
                    worksheet.batch_update(updates)
                    
            return True
        
        except Exception as e:
            logger.error(f"ì›Œí¬ì‹œíŠ¸ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False

    async def send_telegram_message(self, posts, data_updates=None):
        """í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì „ì†¡"""
        if not posts and not data_updates:
            return
            
        message = "ğŸ“Š *MSIT í†µì‹  í†µê³„ ëª¨ë‹ˆí„°ë§*\n\n"
        
        # ìƒˆ ê²Œì‹œë¬¼ ì •ë³´ ì¶”ê°€
        if posts:
            message += "ğŸ“± *ìƒˆë¡œìš´ í†µì‹  ê´€ë ¨ ê²Œì‹œë¬¼*\n\n"
            
            for post in posts:
                message += f"ğŸ“… {post['date']}\n"
                message += f"ğŸ“‘ {post['title']}\n"
                message += f"ğŸ¢ {post['department']}\n"
                if post.get('url'):
                    message += f"ğŸ”— [ê²Œì‹œë¬¼ ë§í¬]({post['url']})\n"
                message += "\n"
        
        # ë°ì´í„° ì—…ë°ì´íŠ¸ ì •ë³´ ì¶”ê°€
        if data_updates:
            message += "ğŸ“Š *Google Sheets ë°ì´í„° ì—…ë°ì´íŠ¸*\n\n"
            
            for update in data_updates:
                post_info = update['post_info']
                date_match = re.search(r'\((\d{4})ë…„\s+(\d{1,2})ì›”ë§\s+ê¸°ì¤€\)', post_info['title'])
                if date_match:
                    year = date_match.group(1)
                    month = date_match.group(2)
                    date_str = f"{year}ë…„ {month}ì›”"
                else:
                    date_str = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
                
                message += f"ğŸ“… *{date_str}*\n"
                message += f"ğŸ“‘ {post_info['title']}\n"
                message += f"ğŸ“— ì—…ë°ì´íŠ¸ ì™„ë£Œ\n\n"
        
        await self.bot.send_message(
            chat_id=int(self.chat_id),
            text=message,
            parse_mode='Markdown'
        )

    async def run_monitor(self, days_range=4):
    
        """ëª¨ë‹ˆí„°ë§ ì‹¤í–‰"""
        driver = None
        gs_client = None
    
        try:
            # WebDriver ì´ˆê¸°í™”
            driver = self.setup_driver()
            logger.info("WebDriver ì´ˆê¸°í™” ì™„ë£Œ")
        
        # Google Sheets í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            if self.gspread_creds:
                gs_client = self.setup_gspread_client()
                if gs_client:
                    logger.info("Google Sheets í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
                else:
                    logger.warning("Google Sheets í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
        
        # MSIT ì›¹ì‚¬ì´íŠ¸ë¡œ ì´ë™
            driver.get(self.url)
            logger.info("MSIT ì›¹ì‚¬ì´íŠ¸ ì ‘ê·¼ ì™„ë£Œ")
        
        # ëª¨ë“  ê²Œì‹œë¬¼ ë° í†µì‹  í†µê³„ ê²Œì‹œë¬¼ ì¶”ì 
            all_posts = []
            telecom_posts = []
            continue_search = True
        
        # í˜ì´ì§€ íŒŒì‹±
            while continue_search:
            # í˜„ì¬ í˜ì´ì§€ ê²Œì‹œë¬¼ íŒŒì‹±
                try:
                    WebDriverWait(driver, 15).until(
                        EC.presence_of_element_located((By.CLASS_NAME, 'board_list'))
                    )
                
                    soup = BeautifulSoup(driver.page_source, 'html.parser')
                    posts = soup.find_all('div', {'class': 'toggle'})
                
                    for item in posts:
                        # ì œëª© ì¶”ì¶œ
                        title_tag = item.find('p', {'class': 'title'})
                        if not title_tag:
                            continue
                    
                    # ë‚ ì§œ ì¶”ì¶œ
                        date_tag = item.find('div', {'class': 'date'})
                        if not date_tag:
                            continue
                    
                        date_str = date_tag.text.strip()
                    
                    # ë‚ ì§œê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ ê²€ìƒ‰ ì¤‘ë‹¨
                        if not self.is_in_date_range(date_str, days=days_range):
                            continue_search = False
                            break
                    
                        # ì œëª© ì¶”ì¶œ
                        title = title_tag.text.strip()
                        
                    # ê²Œì‹œë¬¼ ID ì¶”ì¶œ
                        onclick = title_tag.find('a').get('onclick', '')
                        match = re.search(r"fn_detail\((\d+)\)", onclick)
                        if not match:
                            continue
                    
                        post_id = match.group(1)
                        post_url = f"https://www.msit.go.kr/bbs/view.do?sCode=user&mId=99&mPid=74&nttSeqNo={post_id}"
                    
                    # ë¶€ì„œ ì •ë³´ ì¶”ì¶œ
                        dept_tag = item.find('dd', {'id': lambda x: x and 'td_CHRG_DEPT_NM' in x})
                        dept = dept_tag.text.strip() if dept_tag else "ë¶€ì„œ ì •ë³´ ì—†ìŒ"
                    
                    # ê²Œì‹œë¬¼ ì •ë³´ êµ¬ì„±
                        post_info = {
                            'title': title,
                            'date': date_str,
                            'department': dept,
                            'post_id': post_id,
                            'url': post_url
                        }
                    
                    # ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                        all_posts.append(post_info)
                        
                    # í†µì‹  í†µê³„ ê²Œì‹œë¬¼ ë¶„ë¥˜
                        if self.is_telecom_stats_post(title):
                            telecom_posts.append(post_info)
                            logger.info(f"í†µì‹  í†µê³„ ê²Œì‹œë¬¼ ë°œê²¬: {title}")
                
                # ë‚ ì§œ ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ê²½ìš° ê²€ìƒ‰ ì¤‘ë‹¨
                    if not continue_search:
                        break
                
                # ë‹¤ìŒ í˜ì´ì§€ í™•ì¸ ë° ì´ë™
                    if self.has_next_page(driver):
                        if not self.go_to_next_page(driver):
                            break
                    else:
                        break
                    
                except Exception as e:
                    logger.error(f"í˜ì´ì§€ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    break
        
        # í†µì‹  í†µê³„ ê²Œì‹œë¬¼ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
            if not telecom_posts:
                logger.info(f"ìµœê·¼ {days_range}ì¼ ë‚´ í†µì‹  í†µê³„ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return
        
        # í†µì‹  í†µê³„ ê²Œì‹œë¬¼ ì²˜ë¦¬
            data_updates = []
            
            for post in telecom_posts:
                try:
                # íŒŒì¼ ì •ë³´ ì¶”ì¶œ
                    file_info = self.extract_file_info(driver, post)
                
                    if not file_info:
                        logger.warning(f"íŒŒì¼ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ: {post['title']}")
                        continue
                
                # ì‹œìŠ¤í…œ ì ê²€ ì¤‘ì´ë©´ ê±´ë„ˆë›°ê¸°
                    if file_info.get('system_maintenance'):
                        logger.warning("ì‹œìŠ¤í…œ ì ê²€ìœ¼ë¡œ ì¸í•´ ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue
                
                # ë¬¸ì„œ ë·°ì–´ ì ‘ê·¼ ë° ë°ì´í„° ì¶”ì¶œ
                    if 'atch_file_no' in file_info and 'file_ord' in file_info:
                        sheets_data = self.access_document_viewer(
                            driver, 
                            file_info['atch_file_no'], 
                            file_info['file_ord']
                        )
                    
                        if sheets_data:
                            # Google Sheets ì—…ë°ì´íŠ¸
                            if gs_client:
                                update_data = {
                                    'sheets': sheets_data,
                                    'post_info': post
                                }
                            
                                success = self.update_google_sheets(gs_client, update_data)
                                if success:
                                    logger.info(f"ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì—…ë°ì´íŠ¸ ì„±ê³µ: {post['title']}")
                                    data_updates.append(update_data)
                                else:
                                    logger.warning(f"ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {post['title']}")
                        else:
                        # ëŒ€ì²´ ë°ì´í„° ìƒì„±
                            placeholder_df = self.create_placeholder_dataframe(post)
                            if not placeholder_df.empty and gs_client:
                                update_data = {
                                    'dataframe': placeholder_df,
                                    'post_info': post
                                }
                            
                                success = self.update_google_sheets(gs_client, update_data)
                                if success:
                                    logger.info(f"ëŒ€ì²´ ë°ì´í„°ë¡œ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì—…ë°ì´íŠ¸: {post['title']}")
                                    data_updates.append(update_data)
                
                # ê²Œì‹œë¬¼ ë‚´ìš©ìœ¼ë¡œ ì²˜ë¦¬
                    elif 'content' in file_info:
                        # ê²Œì‹œë¬¼ ë‚´ìš©ì—ì„œ ë°ì´í„° ì¶”ì¶œ ì‹œë„í•  ìˆ˜ ìˆìŒ
                        # ì—¬ê¸°ì—ì„œëŠ” ë‹¨ìˆœíˆ í”Œë ˆì´ìŠ¤í™€ë” ë°ì´í„° ìƒì„±
                        placeholder_df = self.create_placeholder_dataframe(post)
                        if not placeholder_df.empty and gs_client:
                            update_data = {
                                'dataframe': placeholder_df,
                                'post_info': post
                            }
                        
                            success = self.update_google_sheets(gs_client, update_data)
                            if success:
                                logger.info(f"ê²Œì‹œë¬¼ ë‚´ìš© ê¸°ë°˜ ë°ì´í„°ë¡œ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì—…ë°ì´íŠ¸: {post['title']}")
                                data_updates.append(update_data)
                
                except Exception as e:
                    logger.error(f"ê²Œì‹œë¬¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
            # í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì „ì†¡
            if all_posts or data_updates:
                await self.send_telegram_message(all_posts, data_updates)
        
        except Exception as e:
            logger.error(f"ëª¨ë‹ˆí„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
            try:
            # ì˜¤ë¥˜ ì•Œë¦¼ ì „ì†¡
                error_post = {
                    'title': f"ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {str(e)}",
                    'date': datetime.now().strftime('%Y. %m. %d'),
                    'department': 'System Error'
                }
                await self.send_telegram_message([error_post])
            except:
                pass
            
        finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            if driver:
                driver.quit()
                logger.info("WebDriver ì¢…ë£Œ")
                
def setup_gspread_client(self):
    """Google Sheets í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    if not self.gspread_creds:
        return None
        
    try:
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ìê²© ì¦ëª… íŒŒì‹±
        creds_dict = json.loads(self.gspread_creds)
        
        # ì„ì‹œ íŒŒì¼ì— ìê²© ì¦ëª… ì €ì¥
        temp_creds_path = self.temp_dir / "temp_creds.json"
        with open(temp_creds_path, 'w') as f:
            json.dump(creds_dict, f)
        
        # gspread í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
        credentials = ServiceAccountCredentials.from_json_keyfile_name(str(temp_creds_path), scope)
        client = gspread.authorize(credentials)
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.unlink(temp_creds_path)
        
        return client
    except Exception as e:
        logger.error(f"Google Sheets í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def determine_report_type(self, title):
    """ê²Œì‹œë¬¼ ì œëª©ì—ì„œ ë³´ê³ ì„œ ìœ í˜• ê²°ì •"""
    for report_type in self.report_types:
        if report_type in title:
            return report_type
    return "ê¸°íƒ€ í†µì‹  í†µê³„"

def is_in_date_range(self, date_str, days=4):
    """ê²Œì‹œë¬¼ ë‚ ì§œê°€ ì§€ì •ëœ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸"""
    try:
        post_date = self.parse_date(date_str)
        cutoff_date = (datetime.now() - timedelta(days=days)).date()
        
        logger.info(f"ê²Œì‹œë¬¼ ë‚ ì§œ í™•ì¸: {post_date} vs {cutoff_date} ({days}ì¼ ì „)")
        return post_date >= cutoff_date
    except Exception as e:
        logger.error(f"ë‚ ì§œ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return True  # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ì ìœ¼ë¡œ í¬í•¨

def has_next_page(self, driver):
    """ë‹¤ìŒ í˜ì´ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸"""
    try:
        current_page = int(driver.find_element(By.CSS_SELECTOR, "a.page-link[aria-current='page']").text)
        next_page_link = driver.find_elements(By.CSS_SELECTOR, f"a.page-link[href*='pageIndex={current_page + 1}']")
        return len(next_page_link) > 0
    except Exception as e:
        logger.error(f"ë‹¤ìŒ í˜ì´ì§€ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False

def go_to_next_page(self, driver):
    """ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™"""
    try:
        current_page = int(driver.find_element(By.CSS_SELECTOR, "a.page-link[aria-current='page']").text)
        next_page = driver.find_element(By.CSS_SELECTOR, f"a.page-link[href*='pageIndex={current_page + 1}']")
        next_page.click()
        WebDriverWait(driver, 10).until(
            EC.staleness_of(driver.find_element(By.CSS_SELECTOR, "div.board_list"))
        )
        return True
    except Exception as e:
        logger.error(f"ë‹¤ìŒ í˜ì´ì§€ ì´ë™ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False

def create_placeholder_dataframe(self, post_info):
    """ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë°ì´í„°í”„ë ˆì„ ìƒì„±"""
    # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
    date_match = re.search(r'\((\d{4})ë…„\s+(\d{1,2})ì›”ë§\s+ê¸°ì¤€\)', post_info['title'])
    if date_match:
        year = date_match.group(1)
        month = date_match.group(2)
        
        report_type = self.determine_report_type(post_info['title'])
        
        # ê¸°ë³¸ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        df = pd.DataFrame({
            'êµ¬ë¶„': [f'{month}ì›” {report_type}'],
            'ê°’': ['ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'],
            'ë¹„ê³ ': [post_info['title']]
        })
        
        return df
    
    return pd.DataFrame()

async def main():
    days_range = int(os.environ.get('DAYS_RANGE', '4'))
    
    try:
        monitor = MSITMonitor()
        await monitor.run_monitor(days_range=days_range)
    except Exception as e:
        logging.error(f"ê¸°ë³¸ í•¨ìˆ˜ ì˜¤ë¥˜: {str(e)}", exc_info=True)

if __name__ == "__main__":
    asyncio.run(main())
