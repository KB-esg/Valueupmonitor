import asyncio
from playwright.async_api import async_playwright
import pandas as pd
import gspread
from google.oauth2.service_account import Credentials
from datetime import datetime
import json
import os
import requests
import time
import re
from PIL import Image
import pytesseract
import numpy as np

class ESGFundScraper:
    def __init__(self):
        self.base_url = "https://www.fundguide.net/hkcenter/esg"
        self.telegram_bot_token = os.environ.get('TELCO_NEWS_TOKEN')
        self.telegram_chat_id = os.environ.get('TELCO_NEWS_TESTER')
        
    async def fetch_tab_data(self, page, tab_value, tab_name):
        """íŠ¹ì • íƒ­ì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        print(f"Fetching data for {tab_name}...")
        
        # íƒ­ í´ë¦­
        await page.click(f'button[value="{tab_value}"]')
        await page.wait_for_timeout(3000)  # ë°ì´í„° ë¡œë”© ëŒ€ê¸°
        
        # ë°ì´í„° ì¶”ì¶œ
        data = {
            'tab_name': tab_name,
            'top_funds': await self.parse_top_funds(page),
            'new_funds': await self.parse_new_funds(page),
            'chart_data': await self.parse_chart_data_with_hover_ocr(page, tab_name)
        }
        
        return data
    
    async def parse_chart_data_with_hover_ocr(self, page, tab_name):
        """ë§ˆìš°ìŠ¤ í˜¸ë²„ + ìŠ¤í¬ë¦°ìƒ· + OCRì„ í†µí•œ ì°¨íŠ¸ ë°ì´í„° ì¶”ì¶œ"""
        chart_data = {
            'dates': [],
            'setup_amounts': [],
            'returns': []
        }
        
        try:
            # ì°¨íŠ¸ ì˜ì—­ ì°¾ê¸°
            chart_element = await page.query_selector('#lineAreaZone')
            if not chart_element:
                print("Chart element not found")
                return chart_data
            
            # ì°¨íŠ¸ ì˜ì—­ì˜ í¬ê¸°ì™€ ìœ„ì¹˜ ê°€ì ¸ì˜¤ê¸°
            box = await chart_element.bounding_box()
            if not box:
                print("Could not get chart bounding box")
                return chart_data
            
            print(f"Chart area: x={box['x']}, y={box['y']}, width={box['width']}, height={box['height']}")
            
            # ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ë””ë ‰í† ë¦¬
            screenshot_dir = 'chart_screenshots'
            if not os.path.exists(screenshot_dir):
                os.makedirs(screenshot_dir)
            
            # ì°¨íŠ¸ì˜ ì¤‘ê°„ ë†’ì´ (Yì¶•)
            hover_y = box['y'] + box['height'] / 2
            
            # Xì¶•ì„ ë”°ë¼ ì´ë™í•˜ë©° ë°ì´í„° ìˆ˜ì§‘
            num_points = 15  # ì°¨íŠ¸ë¥¼ 15ê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ í˜¸ë²„
            step = box['width'] / num_points
            
            collected_data = []
            
            for i in range(num_points):
                hover_x = box['x'] + (i * step) + 20  # ì™¼ìª½ ì—¬ë°± ê³ ë ¤
                
                # ë§ˆìš°ìŠ¤ë¥¼ í•´ë‹¹ ìœ„ì¹˜ë¡œ ì´ë™
                await page.mouse.move(hover_x, hover_y)
                await page.wait_for_timeout(500)  # íˆ´íŒì´ ë‚˜íƒ€ë‚  ì‹œê°„ ëŒ€ê¸°
                
                # í˜„ì¬ í™”ë©´ ìŠ¤í¬ë¦°ìƒ·
                screenshot_path = f'{screenshot_dir}/{tab_name}_hover_{i}.png'
                await page.screenshot(path=screenshot_path)
                
                # PILë¡œ ì´ë¯¸ì§€ ì—´ê¸°
                img = Image.open(screenshot_path)
                
                # íˆ´íŒì´ ë‚˜íƒ€ë‚  ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì˜ì—­ í™•ëŒ€
                # ë§ˆìš°ìŠ¤ ìœ„ì¹˜ ì£¼ë³€ ì˜ì—­ì„ í¬ë¡­
                tooltip_area = img.crop((
                    max(0, int(hover_x - 150)),
                    max(0, int(hover_y - 100)),
                    min(img.width, int(hover_x + 150)),
                    min(img.height, int(hover_y + 100))
                ))
                
                # íˆ´íŒ ì˜ì—­ ì €ì¥ (ë””ë²„ê¹…ìš©)
                tooltip_path = f'{screenshot_dir}/{tab_name}_tooltip_{i}.png'
                tooltip_area.save(tooltip_path)
                
                # OCR ìˆ˜í–‰
                custom_config = r'--oem 3 --psm 6'
                tooltip_text = pytesseract.image_to_string(tooltip_area, lang='kor+eng', config=custom_config)
                
                if tooltip_text.strip():
                    print(f"Point {i} OCR result: {tooltip_text.strip()}")
                    
                    # ë°ì´í„° íŒŒì‹±
                    data_point = {}
                    lines = tooltip_text.strip().split('\n')
                    
                    for line in lines:
                        # ë‚ ì§œ íŒ¨í„´ (YYYY.MM.DD)
                        date_match = re.search(r'(\d{4}[.\s]+\d{1,2}[.\s]+\d{1,2})', line)
                        if date_match:
                            # ê³µë°± ì œê±°í•˜ê³  ì ìœ¼ë¡œ í†µì¼
                            date_str = re.sub(r'\s+', '', date_match.group(1))
                            date_str = re.sub(r'\.+', '.', date_str)
                            if len(date_str.split('.')) == 3:
                                data_point['date'] = date_str
                        
                        # ì„¤ì •ì•¡ íŒ¨í„´ (ìˆ«ì,ìˆ«ì ì–µì›)
                        amount_match = re.search(r'([\d,]+\.?\d*)\s*ì–µì›', line)
                        if amount_match:
                            value = amount_match.group(1).replace(',', '').replace(' ', '')
                            try:
                                data_point['setup_amount'] = float(value)
                            except:
                                pass
                        
                        # ìˆ˜ìµë¥  íŒ¨í„´ (ìˆ«ì%)
                        if 'ìˆ˜ìµë¥ ' in line or '%' in line:
                            rate_match = re.search(r'([-+]?\d+\.?\d*)\s*%', line)
                            if rate_match:
                                try:
                                    data_point['return_rate'] = float(rate_match.group(1))
                                except:
                                    pass
                    
                    if data_point and 'date' in data_point:
                        # ì¤‘ë³µ ì œê±°
                        if data_point['date'] not in [d.get('date') for d in collected_data]:
                            collected_data.append(data_point)
                            print(f"Collected data point: {data_point}")
                
                # ìŠ¤í¬ë¦°ìƒ· íŒŒì¼ ì‚­ì œ (ê³µê°„ ì ˆì•½)
                try:
                    os.remove(screenshot_path)
                except:
                    pass
            
            # ìˆ˜ì§‘ëœ ë°ì´í„° ì •ë¦¬
            if collected_data:
                # ë‚ ì§œìˆœ ì •ë ¬
                collected_data.sort(key=lambda x: x.get('date', ''))
                
                for data in collected_data:
                    if 'date' in data:
                        chart_data['dates'].append(data['date'])
                    if 'setup_amount' in data:
                        chart_data['setup_amounts'].append(data['setup_amount'])
                    else:
                        chart_data['setup_amounts'].append(None)
                    if 'return_rate' in data:
                        chart_data['returns'].append(data['return_rate'])
                    else:
                        chart_data['returns'].append(None)
                
                print(f"Total collected {len(collected_data)} data points through hover + OCR")
            
            # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° Xì¶•ì—ì„œ ë‚ ì§œë§Œì´ë¼ë„ ì¶”ì¶œ
            if not chart_data['dates']:
                print("Hover + OCR failed, extracting dates from X-axis")
                
                # ì „ì²´ ì°¨íŠ¸ ì˜ì—­ OCR
                full_chart_img = img.crop((
                    int(box['x']),
                    int(box['y']),
                    int(box['x'] + box['width']),
                    int(box['y'] + box['height'] + 50)
                ))
                
                chart_text = pytesseract.image_to_string(full_chart_img, lang='kor+eng', config=custom_config)
                
                # Xì¶• ë‚ ì§œ ì¶”ì¶œ
                date_pattern = r'(\d{4}[.\s]+\d{1,2}[.\s]+\d{1,2})'
                dates = re.findall(date_pattern, chart_text)
                
                for date_str in dates:
                    # ê³µë°± ì œê±°í•˜ê³  ì ìœ¼ë¡œ í†µì¼
                    clean_date = re.sub(r'\s+', '', date_str)
                    clean_date = re.sub(r'\.+', '.', clean_date)
                    if len(clean_date.split('.')) == 3 and clean_date not in chart_data['dates']:
                        chart_data['dates'].append(clean_date)
                
                if chart_data['dates']:
                    print(f"Extracted {len(chart_data['dates'])} dates from full chart OCR")
                    chart_data['setup_amounts'] = [None] * len(chart_data['dates'])
                    chart_data['returns'] = [None] * len(chart_data['dates'])
            
        except Exception as e:
            print(f"Error in hover + OCR data collection: {e}")
            import traceback
            traceback.print_exc()
        
        return chart_data
    
    async def parse_top_funds(self, page):
        """Top í€ë“œ ë°ì´í„° íŒŒì‹±"""
        top_funds_data = {
            'return_top': [],
            'growth_top': []
        }
        
        # ìˆ˜ìµë¥  TOP 5
        return_funds = await page.query_selector_all('#topFundZone td:nth-child(1) li')
        return_rates = await page.query_selector_all('#topFundZone td:nth-child(2) li')
        
        for i in range(len(return_funds)):
            fund_elem = return_funds[i]
            rate_elem = return_rates[i] if i < len(return_rates) else None
            
            rank = await fund_elem.query_selector('i')
            rank_text = await rank.inner_text() if rank else ''
            
            fund_link = await fund_elem.query_selector('a')
            if fund_link:
                fund_name = await fund_link.inner_text()
                fund_code = await fund_link.get_attribute('data-fund_cd')
                rate_text = await rate_elem.inner_text() if rate_elem else ''
                
                top_funds_data['return_top'].append({
                    'rank': rank_text,
                    'fund_name': fund_name.strip(),
                    'fund_code': fund_code or '',
                    'return_rate': rate_text.strip()
                })
        
        # ì„¤ì •ì•¡ì¦ê°€ TOP 5
        growth_funds = await page.query_selector_all('#topFundZone td:nth-child(3) li')
        growth_amounts = await page.query_selector_all('#topFundZone td:nth-child(4) li')
        
        for i in range(len(growth_funds)):
            fund_elem = growth_funds[i]
            amount_elem = growth_amounts[i] if i < len(growth_amounts) else None
            
            rank = await fund_elem.query_selector('i')
            rank_text = await rank.inner_text() if rank else ''
            
            fund_link = await fund_elem.query_selector('a')
            if fund_link:
                fund_name = await fund_link.inner_text()
                fund_code = await fund_link.get_attribute('data-fund_cd')
                amount_text = await amount_elem.inner_text() if amount_elem else ''
                
                top_funds_data['growth_top'].append({
                    'rank': rank_text,
                    'fund_name': fund_name.strip(),
                    'fund_code': fund_code or '',
                    'growth_amount': amount_text.strip()
                })
        
        return top_funds_data
    
    async def parse_new_funds(self, page):
        """ì‹ ê·œ í€ë“œ ë°ì´í„° íŒŒì‹±"""
        new_funds_data = []
        
        # ì‹ ê·œ í€ë“œê°€ ì—†ëŠ”ì§€ í™•ì¸
        no_data = await page.query_selector('#newFundZone .nodata')
        if no_data:
            return new_funds_data
        
        # ì‹ ê·œ í€ë“œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        rows = await page.query_selector_all('#newFundZone tr')
        for row in rows:
            cols = await row.query_selector_all('td')
            if len(cols) >= 3:
                fund_name = await cols[0].inner_text()
                company = await cols[1].inner_text()
                setup_date = await cols[2].inner_text()
                
                new_funds_data.append({
                    'fund_name': fund_name.strip(),
                    'company': company.strip(),
                    'setup_date': setup_date.strip()
                })
        
        return new_funds_data
    
    async def parse_chart_data_from_svg(self, page):
        """SVG ì°¨íŠ¸ì—ì„œ ë‚ ì§œ ì¶”ì¶œ (Xì¶• ë ˆì´ë¸”)"""
        chart_data = {
            'dates': [],
            'setup_amounts': [],
            'returns': []
        }
        
        try:
            # Xì¶• ë ˆì´ë¸”ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
            x_axis_texts = await page.query_selector_all('.highcharts-xaxis-labels text')
            for text_elem in x_axis_texts:
                date_text = await text_elem.inner_text()
                if date_text and '.' in date_text:  # ë‚ ì§œ í˜•ì‹ í™•ì¸
                    chart_data['dates'].append(date_text.strip())
            
            # Yì¶• ê°’ë“¤ ì¶”ì¶œ (ì°¸ê³ ìš©)
            y_axis_texts = await page.query_selector_all('.highcharts-yaxis-labels text')
            setup_amounts_range = []
            returns_range = []
            
            for i, text_elem in enumerate(y_axis_texts):
                value_text = await text_elem.inner_text()
                if value_text:
                    # ì²« ë²ˆì§¸ Yì¶•ì€ ì„¤ì •ì•¡, ë‘ ë²ˆì§¸ Yì¶•ì€ ìˆ˜ìµë¥ 
                    value = value_text.replace(',', '').replace('%', '')
                    try:
                        if i < 7:  # ì²« ë²ˆì§¸ Yì¶• (ì„¤ì •ì•¡)
                            setup_amounts_range.append(float(value))
                        else:  # ë‘ ë²ˆì§¸ Yì¶• (ìˆ˜ìµë¥ )
                            returns_range.append(float(value))
                    except:
                        pass
            
            print(f"Found {len(chart_data['dates'])} dates from chart")
            print(f"Date range: {chart_data['dates'][0] if chart_data['dates'] else 'N/A'} ~ "
                  f"{chart_data['dates'][-1] if chart_data['dates'] else 'N/A'}")
            
            # ì°¨íŠ¸ì˜ path ìš”ì†Œì—ì„œ ì‹¤ì œ ë°ì´í„° í¬ì¸íŠ¸ ì¶”ì • (ë³µì¡í•œ ì‘ì—…)
            # í˜„ì¬ëŠ” ë‚ ì§œë§Œ ì¶”ì¶œ
            
        except Exception as e:
            print(f"Error parsing SVG chart: {e}")
        
        return chart_data
    
    async def scrape_all_tabs(self):
        """ëª¨ë“  íƒ­ì˜ ë°ì´í„° ìˆ˜ì§‘"""
        all_data = {}
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            
            try:
                # í˜ì´ì§€ ë¡œë“œ
                await page.goto(self.base_url, wait_until='networkidle')
                await page.wait_for_timeout(3000)
                
                # ê° íƒ­ ë°ì´í„° ìˆ˜ì§‘
                tabs = [
                    ('T0370', 'SRI'),
                    ('T0371', 'ESG_ì£¼ì‹'),
                    ('T0373', 'ESG_ì±„ê¶Œ')
                ]
                
                for tab_value, tab_name in tabs:
                    data = await self.fetch_tab_data(page, tab_value, tab_name)
                    all_data[tab_name] = data
                    await page.wait_for_timeout(1000)  # íƒ­ ê°„ ëŒ€ê¸°
                
            except Exception as e:
                print(f"Error during scraping: {e}")
                await self.send_telegram_message(f"âŒ ESG í€ë“œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                raise
            finally:
                await browser.close()
        
        return all_data
    
    def to_dataframes(self, all_data):
        """ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜"""
        dfs = {}
        collection_date = datetime.now().strftime('%Y-%m-%d')
        collection_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        for tab_name, tab_data in all_data.items():
            # ìˆ˜ìµë¥  TOP 5
            if tab_data['top_funds']['return_top']:
                df_key = f'{tab_name}_return_top'
                df = pd.DataFrame(tab_data['top_funds']['return_top'])
                df['tab_type'] = tab_name
                df['collection_date'] = collection_date
                df['collection_time'] = collection_time
                dfs[df_key] = df
            
            # ì„¤ì •ì•¡ì¦ê°€ TOP 5
            if tab_data['top_funds']['growth_top']:
                df_key = f'{tab_name}_growth_top'
                df = pd.DataFrame(tab_data['top_funds']['growth_top'])
                df['tab_type'] = tab_name
                df['collection_date'] = collection_date
                df['collection_time'] = collection_time
                dfs[df_key] = df
            
            # ì‹ ê·œ í€ë“œ
            if tab_data['new_funds']:
                df_key = f'{tab_name}_new_funds'
                df = pd.DataFrame(tab_data['new_funds'])
                df['tab_type'] = tab_name
                df['collection_date'] = collection_date
                df['collection_time'] = collection_time
                dfs[df_key] = df
            
            # ì¼ë³„ ì°¨íŠ¸ ë°ì´í„° (ì„¤ì •ì•¡/ìˆ˜ìµë¥ )
            if tab_data.get('chart_data') and tab_data['chart_data'].get('dates'):
                df_key = f'{tab_name}_daily_chart'
                
                # ë°ì´í„° ê¸¸ì´ ë§ì¶”ê¸°
                dates = tab_data['chart_data']['dates']
                setup_amounts = tab_data['chart_data']['setup_amounts']
                returns = tab_data['chart_data']['returns']
                
                # ê°€ì¥ ì§§ì€ ê¸¸ì´ì— ë§ì¶”ê¸°
                min_length = min(len(dates), len(setup_amounts) if setup_amounts else 0, len(returns) if returns else 0)
                
                if min_length > 0:
                    chart_df = pd.DataFrame({
                        'date': dates[:min_length],
                        'setup_amount': setup_amounts[:min_length] if setup_amounts else [None] * min_length,
                        'return_rate': returns[:min_length] if returns else [None] * min_length
                    })
                    chart_df['tab_type'] = tab_name
                    chart_df['collection_time'] = collection_time
                    dfs[df_key] = chart_df
                    print(f"Created chart dataframe for {tab_name} with {min_length} rows")
        
        return dfs
    
    def save_to_sheets(self, dfs):
        """Google Sheetsì— ë°ì´í„° ì €ì¥"""
        # ì„œë¹„ìŠ¤ ê³„ì • ì¸ì¦
        scope = ['https://spreadsheets.google.com/feeds',
                 'https://www.googleapis.com/auth/drive']
        
        creds_json = os.environ.get('GOOGLE_SERVICE')
        if not creds_json:
            print("No Google Sheets credentials found")
            print("Looking for GOOGLE_SERVICE environment variable")
            print("Available environment variables:", list(os.environ.keys()))
            return []
        
        try:
            creds_dict = json.loads(creds_json)
            # ì„œë¹„ìŠ¤ ê³„ì • ì´ë©”ì¼ ì£¼ì†Œ ì¶œë ¥
            service_account_email = creds_dict.get('client_email', 'Unknown')
            print(f"Using service account: {service_account_email}")
            print(f"Please make sure this email has edit access to your Google Sheets")
            
            creds = Credentials.from_service_account_info(creds_dict, scopes=scope)
            client = gspread.authorize(creds)
            
            sheet_id = os.environ.get('KRFUND_SPREADSHEET_ID')
            if not sheet_id:
                print("No Google Sheet ID found")
                print("Available environment variables:", list(os.environ.keys()))
                return []
                
            spreadsheet = client.open_by_key(sheet_id)
            
            # ì‹œíŠ¸ ì´ë¦„ ë§¤í•‘
            sheet_mapping = {
                'SRI_return_top': 'SRI_ìˆ˜ìµë¥ TOP5',
                'SRI_growth_top': 'SRI_ì„¤ì •ì•¡ì¦ê°€TOP5',
                'SRI_new_funds': 'SRI_ì‹ ê·œí€ë“œ',
                'SRI_daily_chart': 'SRI_ì¼ë³„ì°¨íŠ¸',
                'ESG_ì£¼ì‹_return_top': 'ESGì£¼ì‹_ìˆ˜ìµë¥ TOP5',
                'ESG_ì£¼ì‹_growth_top': 'ESGì£¼ì‹_ì„¤ì •ì•¡ì¦ê°€TOP5',
                'ESG_ì£¼ì‹_new_funds': 'ESGì£¼ì‹_ì‹ ê·œí€ë“œ',
                'ESG_ì£¼ì‹_daily_chart': 'ESGì£¼ì‹_ì¼ë³„ì°¨íŠ¸',
                'ESG_ì±„ê¶Œ_return_top': 'ESGì±„ê¶Œ_ìˆ˜ìµë¥ TOP5',
                'ESG_ì±„ê¶Œ_growth_top': 'ESGì±„ê¶Œ_ì„¤ì •ì•¡ì¦ê°€TOP5',
                'ESG_ì±„ê¶Œ_new_funds': 'ESGì±„ê¶Œ_ì‹ ê·œí€ë“œ',
                'ESG_ì±„ê¶Œ_daily_chart': 'ESGì±„ê¶Œ_ì¼ë³„ì°¨íŠ¸'
            }
            
            updated_sheets = []
            
            for df_key, df in dfs.items():
                sheet_name = sheet_mapping.get(df_key, df_key)
                
                try:
                    # ì‹œíŠ¸ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
                    try:
                        worksheet = spreadsheet.worksheet(sheet_name)
                    except:
                        worksheet = spreadsheet.add_worksheet(title=sheet_name, rows=1000, cols=20)
                    
                    # ê¸°ì¡´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                    existing_data = worksheet.get_all_records()
                    
                    if existing_data:
                        existing_df = pd.DataFrame(existing_data)
                        combined_df = pd.concat([existing_df, df], ignore_index=True)
                    else:
                        combined_df = df
                    
                    # ë°ì´í„° ì“°ê¸°
                    worksheet.clear()
                    worksheet.update([combined_df.columns.values.tolist()] + combined_df.values.tolist())
                    
                    updated_sheets.append(sheet_name)
                    print(f"Successfully updated {sheet_name}")
                    
                except Exception as e:
                    print(f"Error updating {sheet_name}: {e}")
                    
            return updated_sheets
            
        except Exception as e:
            print(f"Error in save_to_sheets: {e}")
            return []
    
    def save_backup(self, dfs):
        """ë¡œì»¬ ë°±ì—… ì €ì¥"""
        backup_dir = 'data_backup'
        if not os.path.exists(backup_dir):
            os.makedirs(backup_dir)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        saved_files = []
        
        for key, df in dfs.items():
            filename = f'{backup_dir}/esg_fund_{key}_{timestamp}.csv'
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            saved_files.append(filename)
        
        return saved_files
    
    def send_telegram_message(self, message):
        """Telegram ë©”ì‹œì§€ ì „ì†¡"""
        if not self.telegram_bot_token or not self.telegram_chat_id:
            print("Telegram credentials not found")
            return
        
        try:
            url = f"https://api.telegram.org/bot{self.telegram_bot_token}/sendMessage"
            data = {
                "chat_id": self.telegram_chat_id,
                "text": message,
                "parse_mode": "Markdown"
            }
            response = requests.post(url, data=data)
            response.raise_for_status()
        except Exception as e:
            print(f"Error sending Telegram message: {e}")
    
    async def run(self):
        """ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        start_time = time.time()
        print(f"Starting ESG Fund data collection at {datetime.now()}")
        
        try:
            # 1. ëª¨ë“  íƒ­ ë°ì´í„° ìˆ˜ì§‘
            all_data = await self.scrape_all_tabs()
            
            # 2. DataFrame ë³€í™˜
            dfs = self.to_dataframes(all_data)
            
            # 3. ë°ì´í„° í†µê³„
            total_records = sum(len(df) for df in dfs.values())
            
            # 4. Google Sheets ì €ì¥
            updated_sheets = self.save_to_sheets(dfs)
            
            # 5. ë¡œì»¬ ë°±ì—…
            saved_files = self.save_backup(dfs)
            
            # 6. ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
            execution_time = round(time.time() - start_time, 2)
            
            # 7. ì„±ê³µ ë©”ì‹œì§€ ì „ì†¡
            sheets_count = len(updated_sheets) if isinstance(updated_sheets, list) else 0
            message = f"""âœ… *ESG í€ë“œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ*

ğŸ“… ìˆ˜ì§‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ğŸ“Š ìˆ˜ì§‘ ë°ì´í„°: {total_records}ê°œ ë ˆì½”ë“œ
ğŸ“ ì—…ë°ì´íŠ¸ ì‹œíŠ¸: {sheets_count}ê°œ
ğŸ’¾ ë°±ì—… íŒŒì¼: {len(saved_files)}ê°œ
â±ï¸ ì‹¤í–‰ ì‹œê°„: {execution_time}ì´ˆ

*ìˆ˜ì§‘ í•­ëª©:*
- SRI í€ë“œ
- ESG ì£¼ì‹í˜• í€ë“œ
- ESG ì±„ê¶Œí˜• í€ë“œ

ê° í•­ëª©ë³„ ìˆ˜ìµë¥  TOP5, ì„¤ì •ì•¡ì¦ê°€ TOP5, ì‹ ê·œí€ë“œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ"""
            
            self.send_telegram_message(message)
            print("Data collection completed successfully")
            
        except Exception as e:
            error_message = f"""âŒ *ESG í€ë“œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨*

ğŸ“… ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ğŸš« ì˜¤ë¥˜: {str(e)}

ê´€ë¦¬ìì—ê²Œ í™•ì¸ì„ ìš”ì²­í•˜ì„¸ìš”."""
            
            self.send_telegram_message(error_message)
            print(f"Data collection failed: {e}")
            raise

if __name__ == "__main__":
    scraper = ESGFundScraper()
    asyncio.run(scraper.run())
