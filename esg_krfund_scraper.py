import asyncio
from playwright.async_api import async_playwright
import pandas as pd
import gspread
from google.oauth2.service_account import Credentials
from datetime import datetime
import json
import os
import requests
import time

class ESGFundScraper:
    def __init__(self):
        self.base_url = "https://www.fundguide.net/hkcenter/esg"
        self.telegram_bot_token = os.environ.get('TELCO_NEWS_TOKEN')
        self.telegram_chat_id = os.environ.get('TELCO_NEWS_TESTER')
        
    async def fetch_tab_data(self, page, tab_value, tab_name):
        """íŠ¹ì • íƒ­ì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        print(f"Fetching data for {tab_name}...")
        
        # íƒ­ í´ë¦­
        await page.click(f'button[value="{tab_value}"]')
        await page.wait_for_timeout(2000)  # ë°ì´í„° ë¡œë”© ëŒ€ê¸°
        
        # ë°ì´í„° ì¶”ì¶œ
        data = {
            'tab_name': tab_name,
            'top_funds': await self.parse_top_funds(page),
            'new_funds': await self.parse_new_funds(page),
            'chart_info': await self.parse_chart_info(page)
        }
        
        return data
    
    async def parse_top_funds(self, page):
        """Top í€ë“œ ë°ì´í„° íŒŒì‹±"""
        top_funds_data = {
            'return_top': [],
            'growth_top': []
        }
        
        # ìˆ˜ìµë¥  TOP 5
        return_funds = await page.query_selector_all('#topFundZone td:nth-child(1) li')
        return_rates = await page.query_selector_all('#topFundZone td:nth-child(2) li')
        
        for i in range(len(return_funds)):
            fund_elem = return_funds[i]
            rate_elem = return_rates[i] if i < len(return_rates) else None
            
            rank = await fund_elem.query_selector('i')
            rank_text = await rank.inner_text() if rank else ''
            
            fund_link = await fund_elem.query_selector('a')
            if fund_link:
                fund_name = await fund_link.inner_text()
                fund_code = await fund_link.get_attribute('data-fund_cd')
                rate_text = await rate_elem.inner_text() if rate_elem else ''
                
                top_funds_data['return_top'].append({
                    'rank': rank_text,
                    'fund_name': fund_name.strip(),
                    'fund_code': fund_code or '',
                    'return_rate': rate_text.strip()
                })
        
        # ì„¤ì •ì•¡ì¦ê°€ TOP 5
        growth_funds = await page.query_selector_all('#topFundZone td:nth-child(3) li')
        growth_amounts = await page.query_selector_all('#topFundZone td:nth-child(4) li')
        
        for i in range(len(growth_funds)):
            fund_elem = growth_funds[i]
            amount_elem = growth_amounts[i] if i < len(growth_amounts) else None
            
            rank = await fund_elem.query_selector('i')
            rank_text = await rank.inner_text() if rank else ''
            
            fund_link = await fund_elem.query_selector('a')
            if fund_link:
                fund_name = await fund_link.inner_text()
                fund_code = await fund_link.get_attribute('data-fund_cd')
                amount_text = await amount_elem.inner_text() if amount_elem else ''
                
                top_funds_data['growth_top'].append({
                    'rank': rank_text,
                    'fund_name': fund_name.strip(),
                    'fund_code': fund_code or '',
                    'growth_amount': amount_text.strip()
                })
        
        return top_funds_data
    
    async def parse_new_funds(self, page):
        """ì‹ ê·œ í€ë“œ ë°ì´í„° íŒŒì‹±"""
        new_funds_data = []
        
        # ì‹ ê·œ í€ë“œê°€ ì—†ëŠ”ì§€ í™•ì¸
        no_data = await page.query_selector('#newFundZone .nodata')
        if no_data:
            return new_funds_data
        
        # ì‹ ê·œ í€ë“œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        rows = await page.query_selector_all('#newFundZone tr')
        for row in rows:
            cols = await row.query_selector_all('td')
            if len(cols) >= 3:
                fund_name = await cols[0].inner_text()
                company = await cols[1].inner_text()
                setup_date = await cols[2].inner_text()
                
                new_funds_data.append({
                    'fund_name': fund_name.strip(),
                    'company': company.strip(),
                    'setup_date': setup_date.strip()
                })
        
        return new_funds_data
    
    async def parse_chart_info(self, page):
        """ì°¨íŠ¸ ì •ë³´ íŒŒì‹± (ì„¤ì •ì•¡, ìˆ˜ìµë¥  ë“±)"""
        chart_info = {}
        
        # ì°¨íŠ¸ ì œëª© ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        period_elem = await page.query_selector('#txtTopFund')
        if period_elem:
            chart_info['period'] = await period_elem.inner_text()
        
        return chart_info
    
    async def scrape_all_tabs(self):
        """ëª¨ë“  íƒ­ì˜ ë°ì´í„° ìˆ˜ì§‘"""
        all_data = {}
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            
            try:
                # í˜ì´ì§€ ë¡œë“œ
                await page.goto(self.base_url, wait_until='networkidle')
                await page.wait_for_timeout(3000)
                
                # ê° íƒ­ ë°ì´í„° ìˆ˜ì§‘
                tabs = [
                    ('T0370', 'SRI'),
                    ('T0371', 'ESG_ì£¼ì‹'),
                    ('T0373', 'ESG_ì±„ê¶Œ')
                ]
                
                for tab_value, tab_name in tabs:
                    data = await self.fetch_tab_data(page, tab_value, tab_name)
                    all_data[tab_name] = data
                    await page.wait_for_timeout(1000)  # íƒ­ ê°„ ëŒ€ê¸°
                
            except Exception as e:
                print(f"Error during scraping: {e}")
                await self.send_telegram_message(f"âŒ ESG í€ë“œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                raise
            finally:
                await browser.close()
        
        return all_data
    
    def to_dataframes(self, all_data):
        """ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜"""
        dfs = {}
        collection_date = datetime.now().strftime('%Y-%m-%d')
        collection_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        for tab_name, tab_data in all_data.items():
            # ìˆ˜ìµë¥  TOP 5
            if tab_data['top_funds']['return_top']:
                df_key = f'{tab_name}_return_top'
                df = pd.DataFrame(tab_data['top_funds']['return_top'])
                df['tab_type'] = tab_name
                df['collection_date'] = collection_date
                df['collection_time'] = collection_time
                dfs[df_key] = df
            
            # ì„¤ì •ì•¡ì¦ê°€ TOP 5
            if tab_data['top_funds']['growth_top']:
                df_key = f'{tab_name}_growth_top'
                df = pd.DataFrame(tab_data['top_funds']['growth_top'])
                df['tab_type'] = tab_name
                df['collection_date'] = collection_date
                df['collection_time'] = collection_time
                dfs[df_key] = df
            
            # ì‹ ê·œ í€ë“œ
            if tab_data['new_funds']:
                df_key = f'{tab_name}_new_funds'
                df = pd.DataFrame(tab_data['new_funds'])
                df['tab_type'] = tab_name
                df['collection_date'] = collection_date
                df['collection_time'] = collection_time
                dfs[df_key] = df
        
        return dfs
    
    def save_to_sheets(self, dfs):
        """Google Sheetsì— ë°ì´í„° ì €ì¥"""
        # ì„œë¹„ìŠ¤ ê³„ì • ì¸ì¦
        scope = ['https://spreadsheets.google.com/feeds',
                 'https://www.googleapis.com/auth/drive']
        
        creds_json = os.environ.get('MSIT_GSPREAD_REF')
        if not creds_json:
            print("No Google Sheets credentials found")
            print("Available environment variables:", list(os.environ.keys()))
            return []
        
        try:
            creds_dict = json.loads(creds_json)
            # ì„œë¹„ìŠ¤ ê³„ì • ì´ë©”ì¼ ì£¼ì†Œ ì¶œë ¥
            service_account_email = creds_dict.get('client_email', 'Unknown')
            print(f"Using service account: {service_account_email}")
            print(f"Please make sure this email has edit access to your Google Sheets")
            
            creds = Credentials.from_service_account_info(creds_dict, scopes=scope)
            client = gspread.authorize(creds)
            
            sheet_id = os.environ.get('KRFUND_SPREADSHEET_ID')
            if not sheet_id:
                print("No Google Sheet ID found")
                print("Available environment variables:", list(os.environ.keys()))
                return []
                
            spreadsheet = client.open_by_key(sheet_id)
            
            # ì‹œíŠ¸ ì´ë¦„ ë§¤í•‘
            sheet_mapping = {
                'SRI_return_top': 'SRI_ìˆ˜ìµë¥ TOP5',
                'SRI_growth_top': 'SRI_ì„¤ì •ì•¡ì¦ê°€TOP5',
                'SRI_new_funds': 'SRI_ì‹ ê·œí€ë“œ',
                'ESG_ì£¼ì‹_return_top': 'ESGì£¼ì‹_ìˆ˜ìµë¥ TOP5',
                'ESG_ì£¼ì‹_growth_top': 'ESGì£¼ì‹_ì„¤ì •ì•¡ì¦ê°€TOP5',
                'ESG_ì£¼ì‹_new_funds': 'ESGì£¼ì‹_ì‹ ê·œí€ë“œ',
                'ESG_ì±„ê¶Œ_return_top': 'ESGì±„ê¶Œ_ìˆ˜ìµë¥ TOP5',
                'ESG_ì±„ê¶Œ_growth_top': 'ESGì±„ê¶Œ_ì„¤ì •ì•¡ì¦ê°€TOP5',
                'ESG_ì±„ê¶Œ_new_funds': 'ESGì±„ê¶Œ_ì‹ ê·œí€ë“œ'
            }
            
            updated_sheets = []
            
            for df_key, df in dfs.items():
                sheet_name = sheet_mapping.get(df_key, df_key)
                
                try:
                    # ì‹œíŠ¸ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
                    try:
                        worksheet = spreadsheet.worksheet(sheet_name)
                    except:
                        worksheet = spreadsheet.add_worksheet(title=sheet_name, rows=1000, cols=20)
                    
                    # ê¸°ì¡´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                    existing_data = worksheet.get_all_records()
                    
                    if existing_data:
                        existing_df = pd.DataFrame(existing_data)
                        combined_df = pd.concat([existing_df, df], ignore_index=True)
                    else:
                        combined_df = df
                    
                    # ë°ì´í„° ì“°ê¸°
                    worksheet.clear()
                    worksheet.update([combined_df.columns.values.tolist()] + combined_df.values.tolist())
                    
                    updated_sheets.append(sheet_name)
                    print(f"Successfully updated {sheet_name}")
                    
                except Exception as e:
                    print(f"Error updating {sheet_name}: {e}")
                    
            return updated_sheets
            
        except Exception as e:
            print(f"Error in save_to_sheets: {e}")
            return []
    
    def save_backup(self, dfs):
        """ë¡œì»¬ ë°±ì—… ì €ì¥"""
        backup_dir = 'data_backup'
        if not os.path.exists(backup_dir):
            os.makedirs(backup_dir)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        saved_files = []
        
        for key, df in dfs.items():
            filename = f'{backup_dir}/esg_fund_{key}_{timestamp}.csv'
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            saved_files.append(filename)
        
        return saved_files
    
    def send_telegram_message(self, message):
        """Telegram ë©”ì‹œì§€ ì „ì†¡"""
        if not self.telegram_bot_token or not self.telegram_chat_id:
            print("Telegram credentials not found")
            return
        
        try:
            url = f"https://api.telegram.org/bot{self.telegram_bot_token}/sendMessage"
            data = {
                "chat_id": self.telegram_chat_id,
                "text": message,
                "parse_mode": "Markdown"
            }
            response = requests.post(url, data=data)
            response.raise_for_status()
        except Exception as e:
            print(f"Error sending Telegram message: {e}")
    
    async def run(self):
        """ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        start_time = time.time()
        print(f"Starting ESG Fund data collection at {datetime.now()}")
        
        try:
            # 1. ëª¨ë“  íƒ­ ë°ì´í„° ìˆ˜ì§‘
            all_data = await self.scrape_all_tabs()
            
            # 2. DataFrame ë³€í™˜
            dfs = self.to_dataframes(all_data)
            
            # 3. ë°ì´í„° í†µê³„
            total_records = sum(len(df) for df in dfs.values())
            
            # 4. Google Sheets ì €ì¥
            updated_sheets = self.save_to_sheets(dfs)
            
            # 5. ë¡œì»¬ ë°±ì—…
            saved_files = self.save_backup(dfs)
            
            # 6. ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
            execution_time = round(time.time() - start_time, 2)
            
            # 7. ì„±ê³µ ë©”ì‹œì§€ ì „ì†¡
            sheets_count = len(updated_sheets) if isinstance(updated_sheets, list) else 0
            message = f"""âœ… *ESG í€ë“œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ*

ğŸ“… ìˆ˜ì§‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ğŸ“Š ìˆ˜ì§‘ ë°ì´í„°: {total_records}ê°œ ë ˆì½”ë“œ
ğŸ“ ì—…ë°ì´íŠ¸ ì‹œíŠ¸: {sheets_count}ê°œ
ğŸ’¾ ë°±ì—… íŒŒì¼: {len(saved_files)}ê°œ
â±ï¸ ì‹¤í–‰ ì‹œê°„: {execution_time}ì´ˆ

*ìˆ˜ì§‘ í•­ëª©:*
- SRI í€ë“œ
- ESG ì£¼ì‹í˜• í€ë“œ
- ESG ì±„ê¶Œí˜• í€ë“œ

ê° í•­ëª©ë³„ ìˆ˜ìµë¥  TOP5, ì„¤ì •ì•¡ì¦ê°€ TOP5, ì‹ ê·œí€ë“œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ"""
            
            self.send_telegram_message(message)
            print("Data collection completed successfully")
            
        except Exception as e:
            error_message = f"""âŒ *ESG í€ë“œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨*

ğŸ“… ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ğŸš« ì˜¤ë¥˜: {str(e)}

ê´€ë¦¬ìì—ê²Œ í™•ì¸ì„ ìš”ì²­í•˜ì„¸ìš”."""
            
            self.send_telegram_message(error_message)
            print(f"Data collection failed: {e}")
            raise

if __name__ == "__main__":
    scraper = ESGFundScraper()
    asyncio.run(scraper.run())
