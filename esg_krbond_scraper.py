import requests
import pandas as pd
from datetime import datetime, timedelta
import gspread
from google.oauth2.service_account import Credentials
import json
import os
import sys
import time
from dateutil.relativedelta import relativedelta
from tqdm import tqdm

def get_monthly_dates(start_date, end_date):
    """ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ ì‚¬ì´ì˜ ë§¤ì›” 1ì¼ ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    dates = []
    current = datetime.strptime(start_date, '%Y%m%d')
    end = datetime.strptime(end_date, '%Y%m%d')
    
    # ì‹œì‘ì›”ì˜ 1ì¼ë¶€í„° ì‹œì‘
    current = current.replace(day=1)
    
    while current <= end:
        dates.append(current.strftime('%Y%m%d'))
        # ë‹¤ìŒ ë‹¬ë¡œ ì´ë™
        current = current + relativedelta(months=1)
    
    return dates

def scrape_krx_esg_bonds_by_date(query_date):
    """íŠ¹ì • ë‚ ì§œì˜ KRX ESG ì±„ê¶Œ í˜„í™© ë°ì´í„°ë¥¼ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤."""
    
    # ìš”ì²­ URLê³¼ í—¤ë” ì„¤ì •
    url = "https://esgbond.krx.co.kr/contents/99/SRI99000001.jspx"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'application/json, text/javascript, */*; q=0.01',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
        'Origin': 'https://esgbond.krx.co.kr',
        'Referer': 'https://esgbond.krx.co.kr/contents/02/02010000/SRI02010000.jsp',
        'X-Requested-With': 'XMLHttpRequest'
    }
    
    # POST ë°ì´í„° ì„¤ì •
    data = {
        'sri_bnd_tp_cd': 'ALL',  # ì „ì²´ ì±„ê¶Œì¢…ë¥˜
        'isu_cdnm': 'ì „ì²´',      # ì „ì²´ ë°œí–‰ê¸°ê´€
        'isur_cd': '',
        'isu_cd': '',
        'iss_inst_nm': '',
        'isu_srt_cd': '',
        'isu_nm': '',
        'bnd_tp_cd': 'ALL',      # ì „ì²´ ì±„ê¶Œìœ í˜•
        'schdate': query_date,   # ì¡°íšŒì¼ì
        'pagePath': '/contents/02/02010000/SRI02010000.jsp',
        'code': '02/02010000/sri02010000_02',
        'pageFirstCall': 'Y'
    }
    
    try:
        # POST ìš”ì²­
        response = requests.post(url, headers=headers, data=data, timeout=30)
        response.raise_for_status()
        
        # JSON ì‘ë‹µ íŒŒì‹±
        json_data = response.json()
        
        # ë°ì´í„° ì¶”ì¶œ
        if 'block1' in json_data:
            bonds_data = json_data['block1']
        else:
            keys = list(json_data.keys())
            if keys:
                bonds_data = json_data[keys[0]]
            else:
                return pd.DataFrame()
        
        if not bonds_data:
            return pd.DataFrame()
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        df = pd.DataFrame(bonds_data)
        
        # ì±„ê¶Œì¢…ë¥˜ ì¶”ì¶œ í•¨ìˆ˜
        def extract_bond_type(name):
            if pd.isna(name):
                return ''
            
            name_str = str(name)
            if '(ë…¹)' in name_str:
                return 'ë…¹ìƒ‰ì±„ê¶Œ'
            elif '(ì‚¬)' in name_str:
                return 'ì‚¬íšŒì ì±„ê¶Œ'
            elif '(ì§€)' in name_str:
                return 'ì§€ì†ê°€ëŠ¥ì±„ê¶Œ'
            elif '(ì—°)' in name_str:
                return 'ì§€ì†ê°€ëŠ¥ì—°ê³„ì±„ê¶Œ'
            else:
                return ''
        
        # ì±„ê¶Œì¢…ë¥˜ ì»¬ëŸ¼ ì¶”ê°€
        df['ì±„ê¶Œì¢…ë¥˜'] = df['isu_nm'].apply(extract_bond_type)
        
        # ì¡°íšŒì¼ì ì¶”ê°€
        df['ì¡°íšŒì¼ì'] = query_date
        
        # ìˆ˜ì§‘ì¼ì‹œ ì¶”ê°€
        df['ìˆ˜ì§‘ì¼ì‹œ'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # ë°ì´í„°êµ¬ë¶„ ì¶”ê°€
        df['ë°ì´í„°êµ¬ë¶„'] = 'êµ­ë‚´'
        
        # ì»¬ëŸ¼ëª… í•œê¸€ë¡œ ë³€ê²½
        column_mapping = {
            'com_abbrv': 'ë°œí–‰ê¸°ê´€',
            'isu_cd': 'í‘œì¤€ì½”ë“œ',
            'isu_nm': 'ì¢…ëª©ëª…',
            'lst_dt': 'ìƒì¥ì¼',
            'iss_dt': 'ë°œí–‰ì¼',
            'dis_dt': 'ìƒí™˜ì¼',
            'curr_iso_cd': 'í‘œë©´ì´ììœ¨',
            'iss_amt': 'ë°œí–‰ê¸ˆì•¡(ë°±ë§Œ)',
            'lst_amt': 'ìƒì¥ê¸ˆì•¡(ë°±ë§Œ)',
            'bnd_tp_nm': 'ì±„ê¶Œìœ í˜•'
        }
        
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ê³  ì´ë¦„ ë³€ê²½
        columns_to_keep = list(column_mapping.keys()) + ['ì±„ê¶Œì¢…ë¥˜', 'ì¡°íšŒì¼ì', 'ìˆ˜ì§‘ì¼ì‹œ', 'ë°ì´í„°êµ¬ë¶„']
        df = df[df.columns.intersection(columns_to_keep)]
        df.rename(columns=column_mapping, inplace=True)
        
        # ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ë ¬
        desired_order = ['ì¡°íšŒì¼ì', 'ìˆ˜ì§‘ì¼ì‹œ', 'ë°ì´í„°êµ¬ë¶„', 'ë°œí–‰ê¸°ê´€', 'í‘œì¤€ì½”ë“œ', 'ì¢…ëª©ëª…', 
                        'ì±„ê¶Œì¢…ë¥˜', 'ìƒì¥ì¼', 'ë°œí–‰ì¼', 'ìƒí™˜ì¼', 'í‘œë©´ì´ììœ¨', 
                        'ë°œí–‰ê¸ˆì•¡(ë°±ë§Œ)', 'ìƒì¥ê¸ˆì•¡(ë°±ë§Œ)', 'ì±„ê¶Œìœ í˜•']
        
        final_columns = [col for col in desired_order if col in df.columns]
        df = df[final_columns]
        
        # ë‚ ì§œ í˜•ì‹ í†µì¼ (YYYY/MM/DD -> YYYY-MM-DD)
        date_columns = ['ìƒì¥ì¼', 'ë°œí–‰ì¼', 'ìƒí™˜ì¼']
        for col in date_columns:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce').dt.strftime('%Y-%m-%d')
        
        # ìˆ«ì ì»¬ëŸ¼ ì •ë¦¬
        numeric_columns = ['í‘œë©´ì´ììœ¨', 'ë°œí–‰ê¸ˆì•¡(ë°±ë§Œ)', 'ìƒì¥ê¸ˆì•¡(ë°±ë§Œ)']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce')
        
        return df
        
    except Exception as e:
        print(f"    â†’ {query_date}: ì˜¤ë¥˜ ë°œìƒ - {e}")
        return pd.DataFrame()

def scrape_krx_overseas_esg_bonds():
    """í•œêµ­ê¸°ì—… í•´ì™¸ë¬¼ ESGì±„ê¶Œ ë°ì´í„°ë¥¼ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤."""
    
    # ìš”ì²­ URLê³¼ í—¤ë” ì„¤ì •
    url = "https://esgbond.krx.co.kr/contents/99/SRI99000001.jspx"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'application/json, text/javascript, */*; q=0.01',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
        'Origin': 'https://esgbond.krx.co.kr',
        'Referer': 'https://esgbond.krx.co.kr/contents/02/02030000/SRI02030000.jsp',
        'X-Requested-With': 'XMLHttpRequest'
    }
    
    # POST ë°ì´í„° ì„¤ì • (í•´ì™¸ë¬¼ ì±„ê¶Œìš©)
    data = {
        'code': '02/02030000/sri02030000_01',
        'pagePath': '/contents/02/02030000/SRI02030000.jsp'
    }
    
    try:
        # POST ìš”ì²­
        response = requests.post(url, headers=headers, data=data, timeout=30)
        response.raise_for_status()
        
        # JSON ì‘ë‹µ íŒŒì‹±
        json_data = response.json()
        
        # ë°ì´í„° ì¶”ì¶œ
        if 'block1' in json_data:
            bonds_data = json_data['block1']
        else:
            keys = list(json_data.keys())
            if keys:
                bonds_data = json_data[keys[0]]
            else:
                return pd.DataFrame()
        
        if not bonds_data:
            return pd.DataFrame()
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        df = pd.DataFrame(bonds_data)
        
        # ì¡°íšŒì¼ì ì¶”ê°€ (ì˜¤ëŠ˜ ë‚ ì§œ)
        df['ì¡°íšŒì¼ì'] = datetime.now().strftime('%Y%m%d')
        
        # ìˆ˜ì§‘ì¼ì‹œ ì¶”ê°€
        df['ìˆ˜ì§‘ì¼ì‹œ'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # ë°ì´í„°êµ¬ë¶„ ì¶”ê°€
        df['ë°ì´í„°êµ¬ë¶„'] = 'í•´ì™¸ë¬¼'
        
        # ì±„ê¶Œì¢…ë¥˜ ì¶”ì¶œ
        df['ì±„ê¶Œì¢…ë¥˜'] = df['isu_nm'].apply(lambda x: x if pd.notna(x) else '')
        
        # ì»¬ëŸ¼ëª… í•œê¸€ë¡œ ë³€ê²½
        column_mapping = {
            'isur_nm': 'ë°œí–‰ê¸°ê´€',
            'isu_nm': 'ì±„ê¶Œìœ í˜•',
            'usr_defin_nm1': 'ë°œí–‰ê¸ˆì•¡',
            'isu_dd': 'ë°œí–‰ì—°ì›”',
            'usr_defin_nm2': 'ë§Œê¸°ì—°ì›”',
            'usr_defin_nm3': 'ê¸°ê°„',
            'usr_defin_nm4': 'ë°œí–‰ê¸ˆë¦¬',
            'usr_defin_nm5': 'í‘œë©´ê¸ˆë¦¬',
            'misc_info': 'ì£¼ê´€ì‚¬'
        }
        
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ê³  ì´ë¦„ ë³€ê²½
        columns_to_keep = list(column_mapping.keys()) + ['ì±„ê¶Œì¢…ë¥˜', 'ì¡°íšŒì¼ì', 'ìˆ˜ì§‘ì¼ì‹œ', 'ë°ì´í„°êµ¬ë¶„']
        df = df[df.columns.intersection(columns_to_keep)]
        df.rename(columns=column_mapping, inplace=True)
        
        # ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ë ¬
        desired_order = ['ì¡°íšŒì¼ì', 'ìˆ˜ì§‘ì¼ì‹œ', 'ë°ì´í„°êµ¬ë¶„', 'ë°œí–‰ê¸°ê´€', 'ì±„ê¶Œìœ í˜•', 
                        'ì±„ê¶Œì¢…ë¥˜', 'ë°œí–‰ì—°ì›”', 'ë§Œê¸°ì—°ì›”', 'ê¸°ê°„', 'ë°œí–‰ê¸ˆì•¡', 
                        'ë°œí–‰ê¸ˆë¦¬', 'í‘œë©´ê¸ˆë¦¬', 'ì£¼ê´€ì‚¬']
        
        final_columns = [col for col in desired_order if col in df.columns]
        df = df[final_columns]
        
        return df
        
    except Exception as e:
        print(f"í•´ì™¸ë¬¼ ì±„ê¶Œ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame()

def update_google_sheets(domestic_df, overseas_df, spreadsheet_id, credentials_json):
    """Google Sheetsë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    
    try:
        # ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ íŒŒì‹±
        creds_info = json.loads(credentials_json)
        
        # ì„œë¹„ìŠ¤ ê³„ì • ì´ë©”ì¼ ì¶œë ¥
        service_account_email = creds_info.get('client_email', 'Unknown')
        print(f"\nì„œë¹„ìŠ¤ ê³„ì • ì´ë©”ì¼: {service_account_email}")
        print(f"ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ID: {spreadsheet_id}")
        
        # ì„œë¹„ìŠ¤ ê³„ì • ì¸ì¦
        credentials = Credentials.from_service_account_info(
            creds_info,
            scopes=['https://www.googleapis.com/auth/spreadsheets']
        )
        
        # gspread í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        gc = gspread.authorize(credentials)
        
        # ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì—´ê¸°
        try:
            spreadsheet = gc.open_by_key(spreadsheet_id)
            print(f"ìŠ¤í”„ë ˆë“œì‹œíŠ¸ì— ì„±ê³µì ìœ¼ë¡œ ì ‘ê·¼í–ˆìŠµë‹ˆë‹¤.")
        except gspread.exceptions.APIError as e:
            print(f"\nê¶Œí•œ ì˜¤ë¥˜: Google Sheetsì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"í•´ê²° ë°©ë²•:")
            print(f"1. Google Sheets ì—´ê¸°: https://docs.google.com/spreadsheets/d/{spreadsheet_id}")
            print(f"2. ê³µìœ  ë²„íŠ¼ í´ë¦­")
            print(f"3. '{service_account_email}' ì´ë©”ì¼ ì¶”ê°€")
            print(f"4. 'í¸ì§‘ì' ê¶Œí•œ ë¶€ì—¬")
            print(f"5. ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”\n")
            raise
        
        # 1. êµ­ë‚´ ëˆ„ì  ë°ì´í„° ì›Œí¬ì‹œíŠ¸
        try:
            domestic_ws = spreadsheet.worksheet("êµ­ë‚´_ëˆ„ì ë°ì´í„°")
        except:
            domestic_ws = spreadsheet.add_worksheet(title="êµ­ë‚´_ëˆ„ì ë°ì´í„°", rows=100000, cols=20)
        
        # ê¸°ì¡´ êµ­ë‚´ ë°ì´í„° ì²˜ë¦¬
        combined_domestic_df = update_worksheet_data(domestic_ws, domestic_df, "êµ­ë‚´")
        
        # 2. êµ­ë‚´ ìµœì‹  í˜„í™© ì›Œí¬ì‹œíŠ¸
        try:
            domestic_current_ws = spreadsheet.worksheet("êµ­ë‚´_ìµœì‹ í˜„í™©")
        except:
            domestic_current_ws = spreadsheet.add_worksheet(title="êµ­ë‚´_ìµœì‹ í˜„í™©", rows=5000, cols=20)
        
        # ê°€ì¥ ìµœê·¼ ì¡°íšŒì¼ìì˜ êµ­ë‚´ ë°ì´í„°ë§Œ ì¶”ì¶œ
        if not combined_domestic_df.empty:
            latest_date = combined_domestic_df['ì¡°íšŒì¼ì'].max()
            latest_domestic_df = combined_domestic_df[combined_domestic_df['ì¡°íšŒì¼ì'] == latest_date].copy()
            
            print(f"\nêµ­ë‚´ ìµœì‹  í˜„í™© ì—…ë°ì´íŠ¸ ì¤‘ (ì¡°íšŒì¼ì: {latest_date})")
            update_worksheet_simple(domestic_current_ws, latest_domestic_df, force_update=True)
        
        # 3. í•´ì™¸ë¬¼ ëˆ„ì  ë°ì´í„° ì›Œí¬ì‹œíŠ¸
        try:
            overseas_cumulative_ws = spreadsheet.worksheet("í•´ì™¸ë¬¼_ëˆ„ì ë°ì´í„°")
        except:
            overseas_cumulative_ws = spreadsheet.add_worksheet(title="í•´ì™¸ë¬¼_ëˆ„ì ë°ì´í„°", rows=50000, cols=20)
        
        # í•´ì™¸ë¬¼ ëˆ„ì  ë°ì´í„° ì²˜ë¦¬
        combined_overseas_df = update_overseas_cumulative_data(overseas_cumulative_ws, overseas_df)
        
        # 4. í•´ì™¸ë¬¼ ìµœì‹  í˜„í™© ì›Œí¬ì‹œíŠ¸
        try:
            overseas_current_ws = spreadsheet.worksheet("í•´ì™¸ë¬¼_ìµœì‹ í˜„í™©")
        except:
            overseas_current_ws = spreadsheet.add_worksheet(title="í•´ì™¸ë¬¼_ìµœì‹ í˜„í™©", rows=5000, cols=20)
        
        # í•´ì™¸ë¬¼ ìµœì‹  ë°ì´í„° ì—…ë°ì´íŠ¸
        if not overseas_df.empty:
            print(f"\ní•´ì™¸ë¬¼ ìµœì‹  í˜„í™© ì—…ë°ì´íŠ¸ ì¤‘...")
            update_worksheet_simple(overseas_current_ws, overseas_df, force_update=True)
        
        # 5. ìš”ì•½ ì •ë³´ ì—…ë°ì´íŠ¸
        try:
            summary_ws = spreadsheet.worksheet("ìš”ì•½")
        except:
            summary_ws = spreadsheet.add_worksheet(title="ìš”ì•½", rows=100, cols=10)
        
        update_summary_sheet(summary_ws, combined_domestic_df, combined_overseas_df)
        
        print(f"\nGoogle Sheets ì—…ë°ì´íŠ¸ ì™„ë£Œ:")
        print(f"- êµ­ë‚´ ëˆ„ì  ë°ì´í„°: {len(combined_domestic_df)}ê°œ í–‰")
        print(f"- í•´ì™¸ë¬¼ ëˆ„ì  ë°ì´í„°: {len(combined_overseas_df)}ê°œ í–‰")
        print(f"- í•´ì™¸ë¬¼ ìµœì‹  í˜„í™©: {len(overseas_df)}ê°œ í–‰")
        
    except Exception as e:
        print(f"Google Sheets ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

def update_worksheet_data(worksheet, new_df, data_type):
    """ì›Œí¬ì‹œíŠ¸ì— ëˆ„ì  ë°ì´í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    
    # ê¸°ì¡´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    print(f"\n{data_type} ê¸°ì¡´ ëˆ„ì  ë°ì´í„° í™•ì¸ ì¤‘...")
    existing_data = worksheet.get_all_values()
    
    if existing_data and len(existing_data) > 1:
        # ê¸°ì¡´ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        existing_df = pd.DataFrame(existing_data[1:], columns=existing_data[0])
        print(f"ê¸°ì¡´ {data_type} ëˆ„ì  ë°ì´í„°: {len(existing_df)}ê°œ")
        
        # ë°ì´í„° íƒ€ì… ë§ì¶”ê¸° (ë¬¸ìì—´ë¡œ í†µì¼)
        for col in existing_df.columns:
            existing_df[col] = existing_df[col].astype(str)
        for col in new_df.columns:
            new_df[col] = new_df[col].astype(str)
        
        # êµ­ë‚´ ì±„ê¶Œì˜ ê²½ìš° í‘œì¤€ì½”ë“œì™€ ì¢…ëª©ëª…ìœ¼ë¡œ ê³ ìœ  í‚¤ ìƒì„±
        if 'í‘œì¤€ì½”ë“œ' in new_df.columns and 'ì¢…ëª©ëª…' in new_df.columns:
            # ê¸°ì¡´ ë°ì´í„°ì˜ ê³ ìœ  í‚¤ ì§‘í•© ìƒì„±
            existing_keys = set()
            for _, row in existing_df.iterrows():
                key = f"{row.get('í‘œì¤€ì½”ë“œ', '')}_{row.get('ì¢…ëª©ëª…', '')}"
                existing_keys.add(key)
            
            # ìƒˆ ë°ì´í„°ì—ì„œ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê²ƒë§Œ í•„í„°ë§
            new_rows = []
            duplicates = 0
            for _, row in new_df.iterrows():
                key = f"{row.get('í‘œì¤€ì½”ë“œ', '')}_{row.get('ì¢…ëª©ëª…', '')}"
                if key not in existing_keys:
                    new_rows.append(row)
                else:
                    duplicates += 1
            
            if duplicates > 0:
                print(f"  â†’ ì¤‘ë³µ ì œê±°: {duplicates}ê°œ (ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì±„ê¶Œ)")
            
            if new_rows:
                new_unique_df = pd.DataFrame(new_rows)
                print(f"  â†’ ì‹ ê·œ ì¶”ê°€: {len(new_unique_df)}ê°œ")
                combined_df = pd.concat([existing_df, new_unique_df], ignore_index=True)
                
                # ì •ë ¬
                if 'ì¡°íšŒì¼ì' in combined_df.columns:
                    # ì¡°íšŒì¼ìë¥¼ datetimeìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì •ë ¬
                    combined_df['ì¡°íšŒì¼ì_temp'] = pd.to_datetime(combined_df['ì¡°íšŒì¼ì'], format='%Y%m%d', errors='coerce')
                    combined_df = combined_df.sort_values(['ì¡°íšŒì¼ì_temp', 'ë°œí–‰ê¸°ê´€'], ascending=[False, True])
                    combined_df = combined_df.drop(columns=['ì¡°íšŒì¼ì_temp'])
                else:
                    combined_df = combined_df.sort_values(['ë°œí–‰ê¸°ê´€'])
                
                print(f"ìµœì¢… {data_type} ëˆ„ì  ë°ì´í„°: {len(combined_df)}ê°œ")
                
                # ì›Œí¬ì‹œíŠ¸ ì—…ë°ì´íŠ¸ (ë³€ê²½ì‚¬í•­ì´ ìˆì„ ë•Œë§Œ)
                update_worksheet_simple(worksheet, combined_df)
            else:
                print(f"  â†’ ì‹ ê·œ ì±„ê¶Œ ì—†ìŒ (ì—…ë°ì´íŠ¸ ìƒëµ)")
                combined_df = existing_df
        else:
            # í•´ì™¸ë¬¼ì˜ ê²½ìš° ë°œí–‰ê¸°ê´€ê³¼ ì±„ê¶Œìœ í˜•ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
            combined_df = pd.concat([existing_df, new_df], ignore_index=True)
            combined_df = combined_df.drop_duplicates(
                subset=['ë°œí–‰ê¸°ê´€', 'ì±„ê¶Œìœ í˜•', 'ì¡°íšŒì¼ì'], 
                keep='first'  # ê¸°ì¡´ ë°ì´í„° ìš°ì„ 
            )
            
            # ì •ë ¬
            combined_df = combined_df.sort_values(['ë°œí–‰ê¸°ê´€'])
            
            print(f"ìµœì¢… {data_type} ëˆ„ì  ë°ì´í„°: {len(combined_df)}ê°œ")
            
            # ì›Œí¬ì‹œíŠ¸ ì—…ë°ì´íŠ¸
            update_worksheet_simple(worksheet, combined_df)
    else:
        combined_df = new_df
        print(f"ê¸°ì¡´ {data_type} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆ ë°ì´í„°ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        # ì›Œí¬ì‹œíŠ¸ ì—…ë°ì´íŠ¸
        update_worksheet_simple(worksheet, combined_df)
    
    return combined_df

def update_overseas_cumulative_data(worksheet, new_df):
    """í•´ì™¸ë¬¼ ì±„ê¶Œì˜ ëˆ„ì  ë°ì´í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    
    # ê¸°ì¡´ ëˆ„ì  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    print(f"\ní•´ì™¸ë¬¼ ê¸°ì¡´ ëˆ„ì  ë°ì´í„° í™•ì¸ ì¤‘...")
    existing_data = worksheet.get_all_values()
    
    if existing_data and len(existing_data) > 1:
        # ê¸°ì¡´ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        existing_df = pd.DataFrame(existing_data[1:], columns=existing_data[0])
        print(f"ê¸°ì¡´ í•´ì™¸ë¬¼ ëˆ„ì  ë°ì´í„°: {len(existing_df)}ê°œ")
        
        # ë°ì´í„° íƒ€ì… ë§ì¶”ê¸° (ë¬¸ìì—´ë¡œ í†µì¼)
        for col in existing_df.columns:
            existing_df[col] = existing_df[col].astype(str)
        for col in new_df.columns:
            new_df[col] = new_df[col].astype(str)
        
        # ê³ ìœ  í‚¤ ìƒì„± (ë°œí–‰ê¸°ê´€ + ì±„ê¶Œìœ í˜• + ë°œí–‰ê¸ˆì•¡ + ë°œí–‰ì—°ì›”)
        existing_df['unique_key'] = (existing_df['ë°œí–‰ê¸°ê´€'].astype(str) + '_' + 
                                    existing_df['ì±„ê¶Œìœ í˜•'].astype(str) + '_' + 
                                    existing_df['ë°œí–‰ê¸ˆì•¡'].astype(str) + '_' + 
                                    existing_df['ë°œí–‰ì—°ì›”'].astype(str))
        
        new_df['unique_key'] = (new_df['ë°œí–‰ê¸°ê´€'].astype(str) + '_' + 
                              new_df['ì±„ê¶Œìœ í˜•'].astype(str) + '_' + 
                              new_df['ë°œí–‰ê¸ˆì•¡'].astype(str) + '_' + 
                              new_df['ë°œí–‰ì—°ì›”'].astype(str))
        
        # ê¸°ì¡´ ì±„ê¶Œ ì¤‘ í™œì„± ìƒíƒœì¸ ê²ƒë“¤ì˜ í‚¤ ì§‘í•©
        existing_active_keys = set(existing_df[existing_df.get('ìƒíƒœ', 'í™œì„±') == 'í™œì„±']['unique_key'])
        
        # í˜„ì¬ ë°ì´í„°ì˜ í‚¤ ì§‘í•©
        current_keys = set(new_df['unique_key'])
        
        # ìƒˆë¡œìš´ ì±„ê¶Œ (ê¸°ì¡´ì— ì—†ë˜ ê²ƒ)
        new_keys = current_keys - set(existing_df['unique_key'])
        
        # ì‚¬ë¼ì§„ ì±„ê¶Œ (ê¸°ì¡´ í™œì„± ì¤‘ í˜„ì¬ ì—†ëŠ” ê²ƒ)
        disappeared_keys = existing_active_keys - current_keys
        
        print(f"  â†’ ì‹ ê·œ ì±„ê¶Œ: {len(new_keys)}ê°œ")
        print(f"  â†’ ë§Œê¸°/ìƒí™˜ ì¶”ì •: {len(disappeared_keys)}ê°œ")
        
        # ë³€ê²½ì‚¬í•­ì´ ìˆëŠ” ê²½ìš°ë§Œ ì²˜ë¦¬
        if len(new_keys) > 0 or len(disappeared_keys) > 0:
            # ê²°ê³¼ DataFrame êµ¬ì„±
            result_rows = []
            
            # 1. ê¸°ì¡´ ë°ì´í„° ì²˜ë¦¬
            for _, row in existing_df.iterrows():
                if row['unique_key'] in disappeared_keys:
                    # ì‚¬ë¼ì§„ ì±„ê¶Œì€ ìƒíƒœë¥¼ 'ë§Œê¸°/ìƒí™˜'ìœ¼ë¡œ ë³€ê²½
                    row['ìƒíƒœ'] = 'ë§Œê¸°/ìƒí™˜'
                    row['ìµœì¢…í™•ì¸ì¼'] = new_df['ì¡°íšŒì¼ì'].iloc[0] if not new_df.empty else datetime.now().strftime('%Y%m%d')
                elif row['unique_key'] in current_keys:
                    # ì—¬ì „íˆ ì¡´ì¬í•˜ëŠ” ì±„ê¶Œì€ ìµœì‹  ì •ë³´ë¡œ ì—…ë°ì´íŠ¸
                    new_row = new_df[new_df['unique_key'] == row['unique_key']].iloc[0]
                    row = new_row.copy()
                    row['ìƒíƒœ'] = 'í™œì„±'
                # ì´ë¯¸ ë§Œê¸°/ìƒí™˜ ìƒíƒœì¸ ì±„ê¶Œì€ ê·¸ëŒ€ë¡œ ìœ ì§€
                result_rows.append(row)
            
            # 2. ì‹ ê·œ ì±„ê¶Œ ì¶”ê°€
            for key in new_keys:
                new_row = new_df[new_df['unique_key'] == key].iloc[0].copy()
                new_row['ìƒíƒœ'] = 'í™œì„±'
                result_rows.append(new_row)
            
            # DataFrame ìƒì„± ë° ì •ë¦¬
            combined_df = pd.DataFrame(result_rows)
            
            # unique_key ì»¬ëŸ¼ ì œê±°
            combined_df = combined_df.drop(columns=['unique_key'])
            
            # ì •ë ¬ (ìƒíƒœë³„, ë°œí–‰ì—°ì›” ì—­ìˆœ)
            combined_df['ë°œí–‰ì—°ì›”_sort'] = pd.to_datetime(
                combined_df['ë°œí–‰ì—°ì›”'].astype(str).str[:4] + '-' + 
                combined_df['ë°œí–‰ì—°ì›”'].astype(str).str[5:7] + '-01',
                errors='coerce'
            )
            
            combined_df = combined_df.sort_values(
                ['ìƒíƒœ', 'ë°œí–‰ì—°ì›”_sort', 'ë°œí–‰ê¸°ê´€'], 
                ascending=[True, False, True]
            )
            
            combined_df = combined_df.drop(columns=['ë°œí–‰ì—°ì›”_sort'])
            
            print(f"ìµœì¢… í•´ì™¸ë¬¼ ëˆ„ì  ë°ì´í„°: {len(combined_df)}ê°œ (í™œì„±: {len(combined_df[combined_df['ìƒíƒœ'] == 'í™œì„±'])}ê°œ)")
            
            # ì›Œí¬ì‹œíŠ¸ ì—…ë°ì´íŠ¸ (ë³€ê²½ì‚¬í•­ì´ ìˆì„ ë•Œë§Œ)
            update_worksheet_simple(worksheet, combined_df)
        else:
            print(f"  â†’ ë³€ê²½ì‚¬í•­ ì—†ìŒ (ì—…ë°ì´íŠ¸ ìƒëµ)")
            combined_df = existing_df
            combined_df = combined_df.drop(columns=['unique_key'])
            
    else:
        combined_df = new_df.copy()
        combined_df['ìƒíƒœ'] = 'í™œì„±'
        print(f"ê¸°ì¡´ í•´ì™¸ë¬¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆ ë°ì´í„°ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        # ì›Œí¬ì‹œíŠ¸ ì—…ë°ì´íŠ¸
        update_worksheet_simple(worksheet, combined_df)
    
    return combined_df

def update_worksheet_simple(worksheet, df, force_update=False):
    """ì›Œí¬ì‹œíŠ¸ì— ë°ì´í„°ë¥¼ ê°„ë‹¨íˆ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    
    # force_updateê°€ Falseì´ê³  ë°ì´í„°ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒ
    if not force_update and df.empty:
        print("  â†’ ì—…ë°ì´íŠ¸í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    worksheet.clear()
    
    if df.empty:
        worksheet.update([['ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤']], 'A1')
        return
    
    # í—¤ë” ì¶”ê°€
    headers = df.columns.tolist()
    worksheet.update([headers], 'A1')
    
    # ë°ì´í„° ì¶”ê°€ (ë°°ì¹˜ ì²˜ë¦¬)
    df = df.fillna('')
    values = df.astype(str).values.tolist()
    
    batch_size = 500
    total_rows = len(values)
    
    # tqdmìœ¼ë¡œ ì—…ë¡œë“œ ì§„í–‰ ìƒí™© í‘œì‹œ
    with tqdm(total=total_rows, desc="Google Sheets ì—…ë¡œë“œ") as pbar:
        for i in range(0, total_rows, batch_size):
            batch_end = min(i + batch_size, total_rows)
            batch_data = values[i:batch_end]
            
            start_row = i + 2
            end_row = batch_end + 1
            
            num_cols = len(headers)
            end_col = chr(ord('A') + num_cols - 1) if num_cols <= 26 else 'Z'
            range_str = f'A{start_row}:{end_col}{end_row}'
            
            try:
                worksheet.update(batch_data, range_str)
                pbar.update(batch_end - i)
                time.sleep(1)
            except Exception as e:
                print(f"\në°°ì¹˜ ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
                time.sleep(2)
                try:
                    worksheet.update(batch_data, range_str)
                    pbar.update(batch_end - i)
                except:
                    print(f"ì¬ì‹œë„ ì‹¤íŒ¨. ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                    continue

def update_summary_sheet(summary_ws, domestic_df, overseas_df):
    """ìš”ì•½ ì •ë³´ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    
    # êµ­ë‚´ ìµœì‹  ë°ì´í„°
    if not domestic_df.empty:
        latest_date = domestic_df['ì¡°íšŒì¼ì'].max()
        latest_domestic_df = domestic_df[domestic_df['ì¡°íšŒì¼ì'] == latest_date]
    else:
        latest_domestic_df = pd.DataFrame()
    
    # í•´ì™¸ë¬¼ í™œì„± ì±„ê¶Œ ìˆ˜
    active_overseas_count = len(overseas_df[overseas_df['ìƒíƒœ'] == 'í™œì„±']) if 'ìƒíƒœ' in overseas_df.columns else len(overseas_df)
    expired_overseas_count = len(overseas_df[overseas_df['ìƒíƒœ'] == 'ë§Œê¸°/ìƒí™˜']) if 'ìƒíƒœ' in overseas_df.columns else 0
    
    summary_data = [
        ["ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸", datetime.now().strftime('%Y-%m-%d %H:%M:%S')],
        [""],
        ["êµ­ë‚´ ESG ì±„ê¶Œ", ""],
        ["ì´ ëˆ„ì  ë°ì´í„°", str(len(domestic_df))],
        ["ê³ ìœ  ì±„ê¶Œ ìˆ˜", str(domestic_df['í‘œì¤€ì½”ë“œ'].nunique()) if not domestic_df.empty else "0"],
        [""],
        ["ì±„ê¶Œì¢…ë¥˜ë³„ í˜„í™© (ìµœì‹ )", "ê°œìˆ˜"],
        ["ë…¹ìƒ‰ì±„ê¶Œ", str(len(latest_domestic_df[latest_domestic_df['ì±„ê¶Œì¢…ë¥˜'] == 'ë…¹ìƒ‰ì±„ê¶Œ']))],
        ["ì‚¬íšŒì ì±„ê¶Œ", str(len(latest_domestic_df[latest_domestic_df['ì±„ê¶Œì¢…ë¥˜'] == 'ì‚¬íšŒì ì±„ê¶Œ']))],
        ["ì§€ì†ê°€ëŠ¥ì±„ê¶Œ", str(len(latest_domestic_df[latest_domestic_df['ì±„ê¶Œì¢…ë¥˜'] == 'ì§€ì†ê°€ëŠ¥ì±„ê¶Œ']))],
        ["ì§€ì†ê°€ëŠ¥ì—°ê³„ì±„ê¶Œ", str(len(latest_domestic_df[latest_domestic_df['ì±„ê¶Œì¢…ë¥˜'] == 'ì§€ì†ê°€ëŠ¥ì—°ê³„ì±„ê¶Œ']))],
        [""],
        ["í•´ì™¸ë¬¼ ESG ì±„ê¶Œ", ""],
        ["ì´ ëˆ„ì  ì±„ê¶Œ", str(len(overseas_df))],
        ["í™œì„± ì±„ê¶Œ", str(active_overseas_count)],
        ["ë§Œê¸°/ìƒí™˜ ì±„ê¶Œ", str(expired_overseas_count)],
        [""],
        ["í•´ì™¸ë¬¼ ì±„ê¶Œìœ í˜•ë³„ í˜„í™© (í™œì„±)", "ê°œìˆ˜"]
    ]
    
    # í•´ì™¸ë¬¼ í™œì„± ì±„ê¶Œìœ í˜•ë³„ í˜„í™©
    if not overseas_df.empty and 'ìƒíƒœ' in overseas_df.columns:
        active_overseas_df = overseas_df[overseas_df['ìƒíƒœ'] == 'í™œì„±']
        if not active_overseas_df.empty:
            overseas_type_counts = active_overseas_df['ì±„ê¶Œìœ í˜•'].value_counts()
            for bond_type, count in overseas_type_counts.items():
                summary_data.append([bond_type, str(count)])
    elif not overseas_df.empty:
        overseas_type_counts = overseas_df['ì±„ê¶Œìœ í˜•'].value_counts()
        for bond_type, count in overseas_type_counts.items():
            summary_data.append([bond_type, str(count)])
    
    summary_ws.clear()
    summary_ws.update(summary_data, 'A1')

def send_telegram_notification(domestic_df, overseas_df):
    """ESG ì±„ê¶Œ ì •ë³´ë¥¼ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡í•©ë‹ˆë‹¤."""
    
    bot_token = os.environ.get('TELCO_NEWS_TOKEN')
    chat_id = os.environ.get('TELCO_NEWS_TESTER')
    
    if not bot_token or not chat_id:
        print("í…”ë ˆê·¸ë¨ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    try:
        # ìµœê·¼ ì¼ì£¼ì¼ ë‚´ ìƒì¥í•œ êµ­ë‚´ ì±„ê¶Œ
        today = datetime.now()
        week_ago = today - timedelta(days=7)
        
        message = f"ğŸ“Š KRX ESG ì±„ê¶Œ ì—…ë°ì´íŠ¸ ì™„ë£Œ!\n\n"
        
        # êµ­ë‚´ ì±„ê¶Œ ì •ë³´
        if not domestic_df.empty:
            domestic_df['ìƒì¥ì¼_dt'] = pd.to_datetime(domestic_df['ìƒì¥ì¼'], errors='coerce')
            
            recent_domestic = domestic_df[
                (domestic_df['ìƒì¥ì¼_dt'] >= week_ago) & 
                (domestic_df['ìƒì¥ì¼_dt'] <= today)
            ].copy()
            
            recent_domestic = recent_domestic.sort_values('ìƒì¥ì¼_dt', ascending=False)
            
            if len(recent_domestic) > 0:
                message += f"ğŸ‡°ğŸ‡· êµ­ë‚´ ESG ì±„ê¶Œ - ìµœê·¼ ì¼ì£¼ì¼ ì‹ ê·œ ìƒì¥ ({len(recent_domestic)}ê°œ)\n"
                
                # ì±„ê¶Œì¢…ë¥˜ë³„ ì§‘ê³„
                bond_type_counts = recent_domestic['ì±„ê¶Œì¢…ë¥˜'].value_counts()
                
                for bond_type, count in bond_type_counts.items():
                    if bond_type == 'ë…¹ìƒ‰ì±„ê¶Œ':
                        emoji = 'ğŸŒ±'
                    elif bond_type == 'ì‚¬íšŒì ì±„ê¶Œ':
                        emoji = 'ğŸ¤'
                    elif bond_type == 'ì§€ì†ê°€ëŠ¥ì±„ê¶Œ':
                        emoji = 'â™»ï¸'
                    elif bond_type == 'ì§€ì†ê°€ëŠ¥ì—°ê³„ì±„ê¶Œ':
                        emoji = 'ğŸ”—'
                    else:
                        emoji = 'ğŸ“Œ'
                    message += f"{emoji} {bond_type}: {count}ê°œ\n"
                
                # ë°œí–‰ê¸°ê´€ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í‘œì‹œ (ìµœëŒ€ 10ê°œ ê¸°ê´€)
                message += "\në°œí–‰ê¸°ê´€ë³„ ë‚´ì—­:\n"
                
                # ë°œí–‰ê¸°ê´€ë³„ë¡œ ê·¸ë£¹í™”
                issuer_groups = recent_domestic.groupby('ë°œí–‰ê¸°ê´€').agg({
                    'ì±„ê¶Œì¢…ë¥˜': 'first',
                    'ë°œí–‰ê¸ˆì•¡(ë°±ë§Œ)': 'sum',
                    'ìƒì¥ì¼': 'count'
                }).reset_index()
                
                issuer_groups.columns = ['ë°œí–‰ê¸°ê´€', 'ì±„ê¶Œì¢…ë¥˜', 'ì´ë°œí–‰ê¸ˆì•¡', 'ì±„ê¶Œìˆ˜']
                issuer_groups = issuer_groups.sort_values('ì´ë°œí–‰ê¸ˆì•¡', ascending=False)
                
                for idx, row in issuer_groups.head(10).iterrows():
                    bond_type_emoji = {
                        'ë…¹ìƒ‰ì±„ê¶Œ': 'ğŸŒ±',
                        'ì‚¬íšŒì ì±„ê¶Œ': 'ğŸ¤',
                        'ì§€ì†ê°€ëŠ¥ì±„ê¶Œ': 'â™»ï¸',
                        'ì§€ì†ê°€ëŠ¥ì—°ê³„ì±„ê¶Œ': 'ğŸ”—'
                    }.get(row['ì±„ê¶Œì¢…ë¥˜'], 'ğŸ“Œ')
                    
                    if row['ì±„ê¶Œìˆ˜'] > 1:
                        message += f"â€¢ {row['ë°œí–‰ê¸°ê´€']} - {row['ì±„ê¶Œìˆ˜']}ê°œ ì±„ê¶Œ, ì´ {row['ì´ë°œí–‰ê¸ˆì•¡']:,.0f}ë°±ë§Œì›\n"
                    else:
                        message += f"â€¢ {row['ë°œí–‰ê¸°ê´€']} - {row['ì´ë°œí–‰ê¸ˆì•¡']:,.0f}ë°±ë§Œì›\n"
                
                if len(issuer_groups) > 10:
                    message += f"... ì™¸ {len(issuer_groups) - 10}ê°œ ê¸°ê´€\n"
            else:
                message += f"ğŸ‡°ğŸ‡· êµ­ë‚´: ìµœê·¼ ì¼ì£¼ì¼ ì‹ ê·œ ìƒì¥ ì—†ìŒ\n"
        
        # í•´ì™¸ë¬¼ ì±„ê¶Œ ì •ë³´
        if not overseas_df.empty:
            message += f"\nğŸŒ í•´ì™¸ë¬¼ ESG ì±„ê¶Œ í˜„í™©\n"
            
            # í™œì„±/ë§Œê¸° êµ¬ë¶„
            if 'ìƒíƒœ' in overseas_df.columns:
                active_count = len(overseas_df[overseas_df['ìƒíƒœ'] == 'í™œì„±'])
                expired_count = len(overseas_df[overseas_df['ìƒíƒœ'] == 'ë§Œê¸°/ìƒí™˜'])
                message += f"â€¢ í™œì„± ì±„ê¶Œ: {active_count}ê°œ\n"
                message += f"â€¢ ë§Œê¸°/ìƒí™˜: {expired_count}ê°œ\n"
                
                # ìµœê·¼ ë°œí–‰ ì±„ê¶Œ (í™œì„± ì±„ê¶Œ ì¤‘ì—ì„œ)
                active_df = overseas_df[overseas_df['ìƒíƒœ'] == 'í™œì„±'].copy()
            else:
                active_df = overseas_df.copy()
                message += f"â€¢ ì´ {len(overseas_df)}ê°œ ì±„ê¶Œ\n"
            
            # ìµœê·¼ ë°œí–‰ ì±„ê¶Œ (ë°œí–‰ì—°ì›” ê¸°ì¤€)
            if not active_df.empty:
                active_df['ë°œí–‰ì—°ì›”_dt'] = pd.to_datetime(
                    active_df['ë°œí–‰ì—°ì›”'].astype(str).str[:4] + '-' + 
                    active_df['ë°œí–‰ì—°ì›”'].astype(str).str[5:7] + '-01',
                    errors='coerce'
                )
                
                # ìµœê·¼ 6ê°œì›” ì´ë‚´ ë°œí–‰
                six_months_ago = today - timedelta(days=180)
                recent_overseas = active_df[active_df['ë°œí–‰ì—°ì›”_dt'] >= six_months_ago]
                
                if len(recent_overseas) > 0:
                    message += f"â€¢ ìµœê·¼ 6ê°œì›” ë°œí–‰: {len(recent_overseas)}ê°œ\n"
                    
                    # ìµœê·¼ ë°œí–‰ ë°œí–‰ê¸°ê´€ë³„ë¡œ í‘œì‹œ (ìµœëŒ€ 5ê°œ)
                    recent_overseas_sorted = recent_overseas.sort_values('ë°œí–‰ì—°ì›”_dt', ascending=False)
                    message += "\nìµœê·¼ ë°œí–‰ ê¸°ê´€:\n"
                    
                    # ë°œí–‰ê¸°ê´€ë³„ë¡œ ê·¸ë£¹í™”
                    recent_issuers = recent_overseas_sorted.groupby('ë°œí–‰ê¸°ê´€').agg({
                        'ì±„ê¶Œìœ í˜•': lambda x: ', '.join(x.unique()),
                        'ë°œí–‰ê¸ˆì•¡': lambda x: ', '.join(x),
                        'ë°œí–‰ì—°ì›”': 'count'
                    }).reset_index()
                    
                    recent_issuers.columns = ['ë°œí–‰ê¸°ê´€', 'ì±„ê¶Œìœ í˜•', 'ë°œí–‰ê¸ˆì•¡', 'ê±´ìˆ˜']
                    
                    for idx, row in recent_issuers.head(5).iterrows():
                        if row['ê±´ìˆ˜'] > 1:
                            message += f"  - {row['ë°œí–‰ê¸°ê´€']} ({row['ê±´ìˆ˜']}ê±´)\n"
                        else:
                            message += f"  - {row['ë°œí–‰ê¸°ê´€']} {row['ì±„ê¶Œìœ í˜•']} ({row['ë°œí–‰ê¸ˆì•¡']})\n"
                    
                    if len(recent_issuers) > 5:
                        message += f"  ... ì™¸ {len(recent_issuers) - 5}ê°œ ê¸°ê´€\n"
                
                # ìƒˆë¡œ ì¶”ê°€ëœ ì±„ê¶Œ í™•ì¸ (ì´ì „ ìˆ˜ì§‘ ëŒ€ë¹„)
                new_bonds = active_df[active_df['ì¡°íšŒì¼ì'] == active_df['ì¡°íšŒì¼ì'].max()]
                if len(new_bonds) > 0 and len(active_df) > len(new_bonds):
                    message += f"\nğŸ†• ì‹ ê·œ ì¶”ê°€: {len(new_bonds)}ê°œ\n"
        
        # ì „ì²´ í†µê³„
        message += f"\nğŸ“ˆ ì „ì²´ í˜„í™©:\n"
        message += f"â€¢ êµ­ë‚´ ESG ì±„ê¶Œ: {domestic_df['í‘œì¤€ì½”ë“œ'].nunique():,}ê°œ\n"
        
        # êµ­ë‚´ ë°œí–‰ê¸°ê´€ ìˆ˜
        if not domestic_df.empty:
            domestic_issuers = domestic_df['ë°œí–‰ê¸°ê´€'].nunique()
            message += f"  - ë°œí–‰ê¸°ê´€: {domestic_issuers}ê°œ\n"
        
        message += f"â€¢ í•´ì™¸ë¬¼ ESG ì±„ê¶Œ: {len(overseas_df)}ê°œ"
        if 'ìƒíƒœ' in overseas_df.columns:
            active_count = len(overseas_df[overseas_df['ìƒíƒœ'] == 'í™œì„±'])
            message += f" (í™œì„±: {active_count}ê°œ)"
        
        # í•´ì™¸ë¬¼ ë°œí–‰ê¸°ê´€ ìˆ˜
        if not overseas_df.empty:
            overseas_issuers = overseas_df['ë°œí–‰ê¸°ê´€'].nunique()
            message += f"\n  - ë°œí–‰ê¸°ê´€: {overseas_issuers}ê°œ"
        
        # í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡
        url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
        data = {
            'chat_id': chat_id,
            'text': message,
            'parse_mode': 'HTML'
        }
        
        response = requests.post(url, data=data)
        
        if response.status_code == 200:
            print("\nâœ… í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì „ì†¡ ì„±ê³µ!")
        else:
            print(f"\nâŒ í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {response.status_code}")
            
    except Exception as e:
        print(f"\ní…”ë ˆê·¸ë¨ ì•Œë¦¼ ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def main():
    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    if 'GITHUB_ACTIONS' in os.environ:
        # GitHub Actions í™˜ê²½
        spreadsheet_id = os.environ.get('KRDEBT_SPREADSHEET_ID')
        credentials_json = os.environ.get('GOOGLE_CREDENTIALS_JSON')
        
        # ë‚ ì§œ ë²”ìœ„ í™•ì¸ (í™˜ê²½ ë³€ìˆ˜ë¡œ ì „ë‹¬)
        start_date = os.environ.get('START_DATE')
        end_date = os.environ.get('END_DATE')
        
        if not spreadsheet_id or not credentials_json:
            print("í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            sys.exit(1)
    else:
        # ë¡œì»¬ í…ŒìŠ¤íŠ¸ í™˜ê²½
        print("ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘...")
        spreadsheet_id = input("ìŠ¤í”„ë ˆë“œì‹œíŠ¸ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
        credentials_path = input("ì¸ì¦ JSON íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
        
        with open(credentials_path, 'r') as f:
            credentials_json = f.read()
        
        start_date = input("ì‹œì‘ì¼ìë¥¼ ì…ë ¥í•˜ì„¸ìš” (YYYYMMDD, Enter=ì˜¤ëŠ˜): ")
        end_date = input("ì¢…ë£Œì¼ìë¥¼ ì…ë ¥í•˜ì„¸ìš” (YYYYMMDD, Enter=ì˜¤ëŠ˜): ")
    
    print("\nKRX ESG ì±„ê¶Œ ë°ì´í„° ìŠ¤í¬ë˜í•‘ ì‹œì‘...")
    
    # 1. êµ­ë‚´ ESG ì±„ê¶Œ ë°ì´í„° ìˆ˜ì§‘
    # ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    if start_date and end_date:
        print(f"\n[êµ­ë‚´ ì±„ê¶Œ] ë‚ ì§œ ë²”ìœ„: {start_date} ~ {end_date}")
        dates_list = get_monthly_dates(start_date, end_date)
        print(f"ì¡°íšŒí•  ë‚ ì§œ ({len(dates_list)}ê°œ): {', '.join(dates_list)}")
    else:
        # ë‚ ì§œ ë²”ìœ„ê°€ ì—†ìœ¼ë©´ ì˜¤ëŠ˜ ë‚ ì§œë§Œ
        today = datetime.now().strftime('%Y%m%d')
        dates_list = [today]
        print(f"\n[êµ­ë‚´ ì±„ê¶Œ] ë‹¨ì¼ ë‚ ì§œ ì¡°íšŒ: {today}")
    
    domestic_data = []
    
    # tqdmìœ¼ë¡œ ì§„í–‰ ìƒí™© í‘œì‹œ
    for date in tqdm(dates_list, desc="êµ­ë‚´ ì±„ê¶Œ ìˆ˜ì§‘ ì¤‘"):
        df = scrape_krx_esg_bonds_by_date(date)
        if not df.empty:
            domestic_data.append(df)
            tqdm.write(f"    â†’ {date}: {len(df)}ê°œ ì±„ê¶Œ ìˆ˜ì§‘ ì™„ë£Œ")
        else:
            tqdm.write(f"    â†’ {date}: ë°ì´í„° ì—†ìŒ")
        
        # API ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
        time.sleep(2)
    
    if domestic_data:
        # ëª¨ë“  êµ­ë‚´ ë°ì´í„° ë³‘í•©
        domestic_df = pd.concat(domestic_data, ignore_index=True)
        print(f"\n[êµ­ë‚´ ì±„ê¶Œ] ì´ ìˆ˜ì§‘ëœ ë°ì´í„°: {len(domestic_df)}ê°œ")
    else:
        print("\n[êµ­ë‚´ ì±„ê¶Œ] ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        domestic_df = pd.DataFrame()
    
    # 2. í•´ì™¸ë¬¼ ESG ì±„ê¶Œ ë°ì´í„° ìˆ˜ì§‘
    print("\n[í•´ì™¸ë¬¼ ì±„ê¶Œ] ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    overseas_df = scrape_krx_overseas_esg_bonds()
    
    if not overseas_df.empty:
        print(f"[í•´ì™¸ë¬¼ ì±„ê¶Œ] ìˆ˜ì§‘ ì™„ë£Œ: {len(overseas_df)}ê°œ")
    else:
        print("[í•´ì™¸ë¬¼ ì±„ê¶Œ] ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # ë°ì´í„°ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ì¢…ë£Œ
    if domestic_df.empty and overseas_df.empty:
        print("\nìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)
    
    # Google Sheets ì—…ë°ì´íŠ¸
    print("\nGoogle Sheets ì—…ë°ì´íŠ¸ ì¤‘...")
    update_google_sheets(domestic_df, overseas_df, spreadsheet_id, credentials_json)
    
    # í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì „ì†¡ (GitHub Actions í™˜ê²½ì—ì„œë§Œ)
    if 'GITHUB_ACTIONS' in os.environ:
        send_telegram_notification(domestic_df, overseas_df)
    
    print("\nâœ… ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
