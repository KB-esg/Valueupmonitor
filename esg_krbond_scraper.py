import requests
import pandas as pd
from datetime import datetime, timedelta
import gspread
from google.oauth2.service_account import Credentials
import json
import os
import sys
import time
from dateutil.relativedelta import relativedelta
from tqdm import tqdm

def get_monthly_dates(start_date, end_date):
    """ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ ì‚¬ì´ì˜ ë§¤ì›” 1ì¼ ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    dates = []
    current = datetime.strptime(start_date, '%Y%m%d')
    end = datetime.strptime(end_date, '%Y%m%d')
    
    # ì‹œì‘ì›”ì˜ 1ì¼ë¶€í„° ì‹œì‘
    current = current.replace(day=1)
    
    while current <= end:
        dates.append(current.strftime('%Y%m%d'))
        # ë‹¤ìŒ ë‹¬ë¡œ ì´ë™
        current = current + relativedelta(months=1)
    
    return dates

def scrape_krx_esg_bonds_by_date(query_date):
    """íŠ¹ì • ë‚ ì§œì˜ KRX ESG ì±„ê¶Œ í˜„í™© ë°ì´í„°ë¥¼ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤."""
    
    # ìš”ì²­ URLê³¼ í—¤ë” ì„¤ì •
    url = "https://esgbond.krx.co.kr/contents/99/SRI99000001.jspx"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'application/json, text/javascript, */*; q=0.01',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
        'Origin': 'https://esgbond.krx.co.kr',
        'Referer': 'https://esgbond.krx.co.kr/contents/02/02010000/SRI02010000.jsp',
        'X-Requested-With': 'XMLHttpRequest'
    }
    
    # POST ë°ì´í„° ì„¤ì •
    data = {
        'sri_bnd_tp_cd': 'ALL',  # ì „ì²´ ì±„ê¶Œì¢…ë¥˜
        'isu_cdnm': 'ì „ì²´',      # ì „ì²´ ë°œí–‰ê¸°ê´€
        'isur_cd': '',
        'isu_cd': '',
        'iss_inst_nm': '',
        'isu_srt_cd': '',
        'isu_nm': '',
        'bnd_tp_cd': 'ALL',      # ì „ì²´ ì±„ê¶Œìœ í˜•
        'schdate': query_date,   # ì¡°íšŒì¼ì
        'pagePath': '/contents/02/02010000/SRI02010000.jsp',
        'code': '02/02010000/sri02010000_02',
        'pageFirstCall': 'Y'
    }
    
    try:
        # POST ìš”ì²­
        response = requests.post(url, headers=headers, data=data, timeout=30)
        response.raise_for_status()
        
        # JSON ì‘ë‹µ íŒŒì‹±
        json_data = response.json()
        
        # ë°ì´í„° ì¶”ì¶œ
        if 'block1' in json_data:
            bonds_data = json_data['block1']
        else:
            keys = list(json_data.keys())
            if keys:
                bonds_data = json_data[keys[0]]
            else:
                return pd.DataFrame()
        
        if not bonds_data:
            return pd.DataFrame()
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        df = pd.DataFrame(bonds_data)
        
        # ì±„ê¶Œì¢…ë¥˜ ì¶”ì¶œ í•¨ìˆ˜
        def extract_bond_type(name):
            if pd.isna(name):
                return ''
            
            name_str = str(name)
            if '(ë…¹)' in name_str:
                return 'ë…¹ìƒ‰ì±„ê¶Œ'
            elif '(ì‚¬)' in name_str:
                return 'ì‚¬íšŒì ì±„ê¶Œ'
            elif '(ì§€)' in name_str:
                return 'ì§€ì†ê°€ëŠ¥ì±„ê¶Œ'
            elif '(ì—°)' in name_str:
                return 'ì§€ì†ê°€ëŠ¥ì—°ê³„ì±„ê¶Œ'
            else:
                return ''
        
        # ì±„ê¶Œì¢…ë¥˜ ì»¬ëŸ¼ ì¶”ê°€
        df['ì±„ê¶Œì¢…ë¥˜'] = df['isu_nm'].apply(extract_bond_type)
        
        # ì¡°íšŒì¼ì ì¶”ê°€
        df['ì¡°íšŒì¼ì'] = query_date
        
        # ìˆ˜ì§‘ì¼ì‹œ ì¶”ê°€
        df['ìˆ˜ì§‘ì¼ì‹œ'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # ì»¬ëŸ¼ëª… í•œê¸€ë¡œ ë³€ê²½
        column_mapping = {
            'com_abbrv': 'ë°œí–‰ê¸°ê´€',
            'isu_cd': 'í‘œì¤€ì½”ë“œ',
            'isu_nm': 'ì¢…ëª©ëª…',
            'lst_dt': 'ìƒì¥ì¼',
            'iss_dt': 'ë°œí–‰ì¼',
            'dis_dt': 'ìƒí™˜ì¼',
            'curr_iso_cd': 'í‘œë©´ì´ììœ¨',
            'iss_amt': 'ë°œí–‰ê¸ˆì•¡(ë°±ë§Œ)',
            'lst_amt': 'ìƒì¥ê¸ˆì•¡(ë°±ë§Œ)',
            'bnd_tp_nm': 'ì±„ê¶Œìœ í˜•'
        }
        
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ê³  ì´ë¦„ ë³€ê²½
        columns_to_keep = list(column_mapping.keys()) + ['ì±„ê¶Œì¢…ë¥˜', 'ì¡°íšŒì¼ì', 'ìˆ˜ì§‘ì¼ì‹œ']
        df = df[df.columns.intersection(columns_to_keep)]
        df.rename(columns=column_mapping, inplace=True)
        
        # ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ë ¬
        desired_order = ['ì¡°íšŒì¼ì', 'ìˆ˜ì§‘ì¼ì‹œ', 'ë°œí–‰ê¸°ê´€', 'í‘œì¤€ì½”ë“œ', 'ì¢…ëª©ëª…', 
                        'ì±„ê¶Œì¢…ë¥˜', 'ìƒì¥ì¼', 'ë°œí–‰ì¼', 'ìƒí™˜ì¼', 'í‘œë©´ì´ììœ¨', 
                        'ë°œí–‰ê¸ˆì•¡(ë°±ë§Œ)', 'ìƒì¥ê¸ˆì•¡(ë°±ë§Œ)', 'ì±„ê¶Œìœ í˜•']
        
        final_columns = [col for col in desired_order if col in df.columns]
        df = df[final_columns]
        
        # ë‚ ì§œ í˜•ì‹ í†µì¼ (YYYY/MM/DD -> YYYY-MM-DD)
        date_columns = ['ìƒì¥ì¼', 'ë°œí–‰ì¼', 'ìƒí™˜ì¼']
        for col in date_columns:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce').dt.strftime('%Y-%m-%d')
        
        # ìˆ«ì ì»¬ëŸ¼ ì •ë¦¬
        numeric_columns = ['í‘œë©´ì´ììœ¨', 'ë°œí–‰ê¸ˆì•¡(ë°±ë§Œ)', 'ìƒì¥ê¸ˆì•¡(ë°±ë§Œ)']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce')
        
        return df
        
    except Exception as e:
        print(f"    â†’ {query_date}: ì˜¤ë¥˜ ë°œìƒ - {e}")
        return pd.DataFrame()

def update_google_sheets(all_data_df, spreadsheet_id, credentials_json):
    """Google Sheetsë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    
    try:
        # ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ íŒŒì‹±
        creds_info = json.loads(credentials_json)
        
        # ì„œë¹„ìŠ¤ ê³„ì • ì´ë©”ì¼ ì¶œë ¥
        service_account_email = creds_info.get('client_email', 'Unknown')
        print(f"\nì„œë¹„ìŠ¤ ê³„ì • ì´ë©”ì¼: {service_account_email}")
        print(f"ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ID: {spreadsheet_id}")
        
        # ì„œë¹„ìŠ¤ ê³„ì • ì¸ì¦
        credentials = Credentials.from_service_account_info(
            creds_info,
            scopes=['https://www.googleapis.com/auth/spreadsheets']
        )
        
        # gspread í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        gc = gspread.authorize(credentials)
        
        # ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì—´ê¸°
        try:
            spreadsheet = gc.open_by_key(spreadsheet_id)
            print(f"ìŠ¤í”„ë ˆë“œì‹œíŠ¸ì— ì„±ê³µì ìœ¼ë¡œ ì ‘ê·¼í–ˆìŠµë‹ˆë‹¤.")
        except gspread.exceptions.APIError as e:
            print(f"\nê¶Œí•œ ì˜¤ë¥˜: Google Sheetsì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"í•´ê²° ë°©ë²•:")
            print(f"1. Google Sheets ì—´ê¸°: https://docs.google.com/spreadsheets/d/{spreadsheet_id}")
            print(f"2. ê³µìœ  ë²„íŠ¼ í´ë¦­")
            print(f"3. '{service_account_email}' ì´ë©”ì¼ ì¶”ê°€")
            print(f"4. 'í¸ì§‘ì' ê¶Œí•œ ë¶€ì—¬")
            print(f"5. ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”\n")
            raise
        
        # ëˆ„ì  ë°ì´í„° ì›Œí¬ì‹œíŠ¸
        try:
            cumulative_ws = spreadsheet.worksheet("ëˆ„ì ë°ì´í„°")
        except:
            cumulative_ws = spreadsheet.add_worksheet(title="ëˆ„ì ë°ì´í„°", rows=100000, cols=20)
        
        # ê¸°ì¡´ ëˆ„ì  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        print("\nê¸°ì¡´ ëˆ„ì  ë°ì´í„° í™•ì¸ ì¤‘...")
        existing_data = cumulative_ws.get_all_values()
        
        if existing_data and len(existing_data) > 1:
            # ê¸°ì¡´ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
            existing_df = pd.DataFrame(existing_data[1:], columns=existing_data[0])
            print(f"ê¸°ì¡´ ëˆ„ì  ë°ì´í„°: {len(existing_df)}ê°œ")
            
            # ìƒˆ ë°ì´í„°ì™€ ë³‘í•© (í‘œì¤€ì½”ë“œì™€ ì¡°íšŒì¼ì ê¸°ì¤€ ì¤‘ë³µ ì œê±°)
            combined_df = pd.concat([all_data_df, existing_df], ignore_index=True)
            
            # ì¤‘ë³µ ì œê±°
            combined_df = combined_df.drop_duplicates(
                subset=['í‘œì¤€ì½”ë“œ', 'ì¡°íšŒì¼ì'], 
                keep='first'
            )
            
            # ì •ë ¬
            combined_df = combined_df.sort_values(['ì¡°íšŒì¼ì', 'í‘œì¤€ì½”ë“œ'])
            
            print(f"ì¤‘ë³µ ì œê±° í›„ ì´ ë°ì´í„°: {len(combined_df)}ê°œ")
        else:
            combined_df = all_data_df
            print("ê¸°ì¡´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆ ë°ì´í„°ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        # ëˆ„ì  ë°ì´í„° ì—…ë°ì´íŠ¸ (tqdmìœ¼ë¡œ ì§„í–‰ ìƒí™© í‘œì‹œ)
        print("\nëˆ„ì  ë°ì´í„° ì—…ë°ì´íŠ¸ ì¤‘...")
        cumulative_ws.clear()
        
        # í—¤ë” ì¶”ê°€
        headers = combined_df.columns.tolist()
        cumulative_ws.update('A1', [headers])
        
        # ë°ì´í„° ì¶”ê°€ (ë°°ì¹˜ ì²˜ë¦¬)
        if not combined_df.empty:
            combined_df = combined_df.fillna('')
            values = combined_df.astype(str).values.tolist()
            
            batch_size = 500
            total_rows = len(values)
            
            # tqdmìœ¼ë¡œ ì—…ë¡œë“œ ì§„í–‰ ìƒí™© í‘œì‹œ
            with tqdm(total=total_rows, desc="Google Sheets ì—…ë¡œë“œ") as pbar:
                for i in range(0, total_rows, batch_size):
                    batch_end = min(i + batch_size, total_rows)
                    batch_data = values[i:batch_end]
                    
                    start_row = i + 2
                    end_row = batch_end + 1
                    
                    range_str = f'A{start_row}:M{end_row}'
                    
                    try:
                        cumulative_ws.update(range_str, batch_data)
                        pbar.update(batch_end - i)
                        time.sleep(1)
                    except Exception as e:
                        print(f"\në°°ì¹˜ ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
                        time.sleep(2)
                        try:
                            cumulative_ws.update(range_str, batch_data)
                            pbar.update(batch_end - i)
                        except:
                            print(f"ì¬ì‹œë„ ì‹¤íŒ¨. ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                            continue
        
        # ìµœì‹  í˜„í™© ì›Œí¬ì‹œíŠ¸ ì—…ë°ì´íŠ¸
        try:
            current_ws = spreadsheet.worksheet("ìµœì‹ í˜„í™©")
        except:
            current_ws = spreadsheet.add_worksheet(title="ìµœì‹ í˜„í™©", rows=5000, cols=20)
        
        # ê°€ì¥ ìµœê·¼ ì¡°íšŒì¼ìì˜ ë°ì´í„°ë§Œ ì¶”ì¶œ
        latest_date = combined_df['ì¡°íšŒì¼ì'].max()
        latest_df = combined_df[combined_df['ì¡°íšŒì¼ì'] == latest_date].copy()
        
        print(f"\nìµœì‹  í˜„í™© ì—…ë°ì´íŠ¸ ì¤‘ (ì¡°íšŒì¼ì: {latest_date})")
        current_ws.clear()
        current_ws.update('A1', [headers])
        
        if not latest_df.empty:
            latest_values = latest_df.astype(str).values.tolist()
            
            # í•œ ë²ˆì— ì—…ë°ì´íŠ¸ (ìµœì‹  í˜„í™©ì€ ë³´í†µ ì ìŒ)
            batch_size = 500
            for i in range(0, len(latest_values), batch_size):
                batch_end = min(i + batch_size, len(latest_values))
                batch_data = latest_values[i:batch_end]
                
                start_row = i + 2
                end_row = batch_end + 1
                
                range_str = f'A{start_row}:M{end_row}'
                current_ws.update(range_str, batch_data)
                time.sleep(1)
        
        # ìš”ì•½ ì •ë³´ ì—…ë°ì´íŠ¸
        try:
            summary_ws = spreadsheet.worksheet("ìš”ì•½")
        except:
            summary_ws = spreadsheet.add_worksheet(title="ìš”ì•½", rows=100, cols=10)
        
        # ì¡°íšŒì¼ìë³„ ì±„ê¶Œ ìˆ˜ ê³„ì‚°
        date_summary = combined_df.groupby('ì¡°íšŒì¼ì').size().reset_index(name='ì±„ê¶Œìˆ˜')
        
        summary_data = [
            ["ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸", datetime.now().strftime('%Y-%m-%d %H:%M:%S')],
            ["ì´ ëˆ„ì  ë°ì´í„°", str(len(combined_df))],
            ["ê³ ìœ  ì±„ê¶Œ ìˆ˜", str(combined_df['í‘œì¤€ì½”ë“œ'].nunique())],
            [""],
            ["ì¡°íšŒì¼ìë³„ í˜„í™©", ""]
        ]
        
        # ì¡°íšŒì¼ìë³„ í˜„í™© ì¶”ê°€
        for _, row in date_summary.iterrows():
            summary_data.append([row['ì¡°íšŒì¼ì'], str(row['ì±„ê¶Œìˆ˜'])])
        
        summary_data.extend([
            [""],
            ["ìµœì‹  ì±„ê¶Œì¢…ë¥˜ë³„ í˜„í™©", "ê°œìˆ˜"],
            ["ë…¹ìƒ‰ì±„ê¶Œ", str(len(latest_df[latest_df['ì±„ê¶Œì¢…ë¥˜'] == 'ë…¹ìƒ‰ì±„ê¶Œ']))],
            ["ì‚¬íšŒì ì±„ê¶Œ", str(len(latest_df[latest_df['ì±„ê¶Œì¢…ë¥˜'] == 'ì‚¬íšŒì ì±„ê¶Œ']))],
            ["ì§€ì†ê°€ëŠ¥ì±„ê¶Œ", str(len(latest_df[latest_df['ì±„ê¶Œì¢…ë¥˜'] == 'ì§€ì†ê°€ëŠ¥ì±„ê¶Œ']))],
            ["ì§€ì†ê°€ëŠ¥ì—°ê³„ì±„ê¶Œ", str(len(latest_df[latest_df['ì±„ê¶Œì¢…ë¥˜'] == 'ì§€ì†ê°€ëŠ¥ì—°ê³„ì±„ê¶Œ']))]
        ])
        
        summary_ws.clear()
        summary_ws.update('A1', summary_data)
        
        # ì„œì‹ ì„¤ì •
        try:
            cumulative_ws.format('A1:M1', {
                'backgroundColor': {'red': 0.2, 'green': 0.2, 'blue': 0.2},
                'textFormat': {'foregroundColor': {'red': 1, 'green': 1, 'blue': 1}, 'bold': True},
                'horizontalAlignment': 'CENTER'
            })
            
            current_ws.format('A1:M1', {
                'backgroundColor': {'red': 0.2, 'green': 0.4, 'blue': 0.2},
                'textFormat': {'foregroundColor': {'red': 1, 'green': 1, 'blue': 1}, 'bold': True},
                'horizontalAlignment': 'CENTER'
            })
        except:
            print("ì„œì‹ ì„¤ì • ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ê³„ì†)")
        
        print(f"\nGoogle Sheets ì—…ë°ì´íŠ¸ ì™„ë£Œ:")
        print(f"- ëˆ„ì  ë°ì´í„°: {len(combined_df)}ê°œ í–‰")
        print(f"- ìµœì‹  í˜„í™©: {len(latest_df)}ê°œ í–‰")
        print(f"- ê³ ìœ  ì±„ê¶Œ: {combined_df['í‘œì¤€ì½”ë“œ'].nunique()}ê°œ")
        
    except Exception as e:
        print(f"Google Sheets ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

def send_telegram_notification(all_data_df):
    """ìµœê·¼ ì¼ì£¼ì¼ ë‚´ ìƒì¥í•œ ESG ì±„ê¶Œ ì •ë³´ë¥¼ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡í•©ë‹ˆë‹¤."""
    
    bot_token = os.environ.get('TELCO_NEWS_TOKEN')
    chat_id = os.environ.get('TELCO_NEWS_TESTER')
    
    if not bot_token or not chat_id:
        print("í…”ë ˆê·¸ë¨ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    try:
        # ìµœì‹  ë°ì´í„°ì—ì„œ ìƒì¥ì¼ ê¸°ì¤€ìœ¼ë¡œ ìµœê·¼ ì¼ì£¼ì¼ ë°ì´í„° í•„í„°ë§
        today = datetime.now()
        week_ago = today - timedelta(days=7)
        
        # ìƒì¥ì¼ì„ datetimeìœ¼ë¡œ ë³€í™˜
        all_data_df['ìƒì¥ì¼_dt'] = pd.to_datetime(all_data_df['ìƒì¥ì¼'], errors='coerce')
        
        # ìµœê·¼ ì¼ì£¼ì¼ ë‚´ ìƒì¥ëœ ì±„ê¶Œ í•„í„°ë§
        recent_bonds = all_data_df[
            (all_data_df['ìƒì¥ì¼_dt'] >= week_ago) & 
            (all_data_df['ìƒì¥ì¼_dt'] <= today)
        ].copy()
        
        # ìƒì¥ì¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        recent_bonds = recent_bonds.sort_values('ìƒì¥ì¼_dt', ascending=False)
        
        # ë©”ì‹œì§€ ì‘ì„±
        if len(recent_bonds) > 0:
            message = f"ğŸ“Š KRX ESG ì±„ê¶Œ ì—…ë°ì´íŠ¸ ì™„ë£Œ!\n\n"
            message += f"ğŸ—“ï¸ ìµœê·¼ ì¼ì£¼ì¼ ì‹ ê·œ ìƒì¥ ESG ì±„ê¶Œ ({len(recent_bonds)}ê°œ)\n"
            message += f"({week_ago.strftime('%Y-%m-%d')} ~ {today.strftime('%Y-%m-%d')})\n\n"
            
            # ì±„ê¶Œì¢…ë¥˜ë³„ ì§‘ê³„
            bond_type_counts = recent_bonds['ì±„ê¶Œì¢…ë¥˜'].value_counts()
            
            for bond_type, count in bond_type_counts.items():
                if bond_type == 'ë…¹ìƒ‰ì±„ê¶Œ':
                    emoji = 'ğŸŒ±'
                elif bond_type == 'ì‚¬íšŒì ì±„ê¶Œ':
                    emoji = 'ğŸ¤'
                elif bond_type == 'ì§€ì†ê°€ëŠ¥ì±„ê¶Œ':
                    emoji = 'â™»ï¸'
                elif bond_type == 'ì§€ì†ê°€ëŠ¥ì—°ê³„ì±„ê¶Œ':
                    emoji = 'ğŸ”—'
                else:
                    emoji = 'ğŸ“Œ'
                message += f"{emoji} {bond_type}: {count}ê°œ\n"
            
            message += "\nğŸ“‹ ìƒì„¸ ë‚´ì—­:\n"
            
            # ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ í‘œì‹œ
            for idx, row in recent_bonds.head(10).iterrows():
                bond_type_emoji = {
                    'ë…¹ìƒ‰ì±„ê¶Œ': 'ğŸŒ±',
                    'ì‚¬íšŒì ì±„ê¶Œ': 'ğŸ¤',
                    'ì§€ì†ê°€ëŠ¥ì±„ê¶Œ': 'â™»ï¸',
                    'ì§€ì†ê°€ëŠ¥ì—°ê³„ì±„ê¶Œ': 'ğŸ”—'
                }.get(row['ì±„ê¶Œì¢…ë¥˜'], 'ğŸ“Œ')
                
                message += f"\n{bond_type_emoji} [{row['ìƒì¥ì¼']}]\n"
                message += f"â€¢ ë°œí–‰ê¸°ê´€: {row['ë°œí–‰ê¸°ê´€']}\n"
                message += f"â€¢ ì¢…ëª©ëª…: {row['ì¢…ëª©ëª…']}\n"
                message += f"â€¢ ë°œí–‰ê¸ˆì•¡: {row['ë°œí–‰ê¸ˆì•¡(ë°±ë§Œ)']:,.0f}ë°±ë§Œì›\n"
                
            if len(recent_bonds) > 10:
                message += f"\n... ì™¸ {len(recent_bonds) - 10}ê°œ"
                
        else:
            message = f"ğŸ“Š KRX ESG ì±„ê¶Œ ì—…ë°ì´íŠ¸ ì™„ë£Œ!\n\n"
            message += f"ğŸ—“ï¸ ìµœê·¼ ì¼ì£¼ì¼({week_ago.strftime('%Y-%m-%d')} ~ {today.strftime('%Y-%m-%d')}) ë™ì•ˆ\n"
            message += f"ì‹ ê·œ ìƒì¥ëœ ESG ì±„ê¶Œì´ ì—†ìŠµë‹ˆë‹¤."
        
        # ì „ì²´ í†µê³„ ì¶”ê°€
        total_bonds = len(all_data_df)
        unique_bonds = all_data_df['í‘œì¤€ì½”ë“œ'].nunique()
        
        message += f"\n\nğŸ“ˆ ì „ì²´ ESG ì±„ê¶Œ í˜„í™©:\n"
        message += f"â€¢ ì´ ë°ì´í„°: {total_bonds:,}ê°œ\n"
        message += f"â€¢ ê³ ìœ  ì±„ê¶Œ: {unique_bonds:,}ê°œ\n"
        
        # ì±„ê¶Œì¢…ë¥˜ë³„ ì „ì²´ í˜„í™©
        total_type_counts = all_data_df[all_data_df['ì¡°íšŒì¼ì'] == all_data_df['ì¡°íšŒì¼ì'].max()]['ì±„ê¶Œì¢…ë¥˜'].value_counts()
        message += "\nì±„ê¶Œì¢…ë¥˜ë³„ í˜„í™©:\n"
        for bond_type, count in total_type_counts.items():
            if bond_type:  # ë¹ˆ ê°’ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
                message += f"â€¢ {bond_type}: {count}ê°œ\n"
        
        # í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡
        url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
        data = {
            'chat_id': chat_id,
            'text': message,
            'parse_mode': 'HTML'
        }
        
        response = requests.post(url, data=data)
        
        if response.status_code == 200:
            print("\nâœ… í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì „ì†¡ ì„±ê³µ!")
        else:
            print(f"\nâŒ í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {response.status_code}")
            
    except Exception as e:
        print(f"\ní…”ë ˆê·¸ë¨ ì•Œë¦¼ ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def main():
    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    if 'GITHUB_ACTIONS' in os.environ:
        # GitHub Actions í™˜ê²½
        spreadsheet_id = os.environ.get('KRDEBT_SPREADSHEET_ID')
        credentials_json = os.environ.get('GOOGLE_CREDENTIALS_JSON')
        
        # ë‚ ì§œ ë²”ìœ„ í™•ì¸ (í™˜ê²½ ë³€ìˆ˜ë¡œ ì „ë‹¬)
        start_date = os.environ.get('START_DATE')
        end_date = os.environ.get('END_DATE')
        
        if not spreadsheet_id or not credentials_json:
            print("í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            sys.exit(1)
    else:
        # ë¡œì»¬ í…ŒìŠ¤íŠ¸ í™˜ê²½
        print("ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘...")
        spreadsheet_id = input("ìŠ¤í”„ë ˆë“œì‹œíŠ¸ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
        credentials_path = input("ì¸ì¦ JSON íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
        
        with open(credentials_path, 'r') as f:
            credentials_json = f.read()
        
        start_date = input("ì‹œì‘ì¼ìë¥¼ ì…ë ¥í•˜ì„¸ìš” (YYYYMMDD, Enter=ì˜¤ëŠ˜): ")
        end_date = input("ì¢…ë£Œì¼ìë¥¼ ì…ë ¥í•˜ì„¸ìš” (YYYYMMDD, Enter=ì˜¤ëŠ˜): ")
    
    print("\nKRX ESG ì±„ê¶Œ ë°ì´í„° ìŠ¤í¬ë˜í•‘ ì‹œì‘...")
    
    # ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    if start_date and end_date:
        print(f"ë‚ ì§œ ë²”ìœ„: {start_date} ~ {end_date}")
        dates_list = get_monthly_dates(start_date, end_date)
        print(f"ì¡°íšŒí•  ë‚ ì§œ ({len(dates_list)}ê°œ): {', '.join(dates_list)}")
    else:
        # ë‚ ì§œ ë²”ìœ„ê°€ ì—†ìœ¼ë©´ ì˜¤ëŠ˜ ë‚ ì§œë§Œ
        today = datetime.now().strftime('%Y%m%d')
        dates_list = [today]
        print(f"ë‹¨ì¼ ë‚ ì§œ ì¡°íšŒ: {today}")
    
    all_data = []
    
    # tqdmìœ¼ë¡œ ì§„í–‰ ìƒí™© í‘œì‹œ
    for date in tqdm(dates_list, desc="ë°ì´í„° ìˆ˜ì§‘ ì¤‘"):
        df = scrape_krx_esg_bonds_by_date(date)
        if not df.empty:
            all_data.append(df)
            tqdm.write(f"    â†’ {date}: {len(df)}ê°œ ì±„ê¶Œ ìˆ˜ì§‘ ì™„ë£Œ")
        else:
            tqdm.write(f"    â†’ {date}: ë°ì´í„° ì—†ìŒ")
        
        # API ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
        time.sleep(2)
    
    if all_data:
        # ëª¨ë“  ë°ì´í„° ë³‘í•©
        all_data_df = pd.concat(all_data, ignore_index=True)
        print(f"\nì´ ìˆ˜ì§‘ëœ ë°ì´í„°: {len(all_data_df)}ê°œ")
        
        # ì¡°íšŒì¼ìë³„ ìˆ˜ì§‘ í˜„í™©
        print("\nì¡°íšŒì¼ìë³„ ìˆ˜ì§‘ í˜„í™©:")
        date_counts = all_data_df['ì¡°íšŒì¼ì'].value_counts().sort_index()
        for date, count in date_counts.items():
            print(f"  - {date}: {count}ê°œ")
    else:
        print("\nìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)
    
    # Google Sheets ì—…ë°ì´íŠ¸
    print("\nGoogle Sheets ì—…ë°ì´íŠ¸ ì¤‘...")
    update_google_sheets(all_data_df, spreadsheet_id, credentials_json)
    
    # í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì „ì†¡ (GitHub Actions í™˜ê²½ì—ì„œë§Œ)
    if 'GITHUB_ACTIONS' in os.environ:
        send_telegram_notification(all_data_df)
    
    print("\nâœ… ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
